{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3d39ab",
   "metadata": {
    "id": "3d3d39ab"
   },
   "source": [
    "# Ingestion\n",
    "This notebook parses data from XXX using Google Vertex AI Generative AI.  It then uses Generative AI to create Neo4j Cypher queries which write the data to a Neo4j database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a9328-f17d-48ae-b72e-c333fb867eb0",
   "metadata": {
    "id": "f17a9328-f17d-48ae-b72e-c333fb867eb0"
   },
   "source": [
    "## Setup\n",
    "This notebook should be run within Vertex AI Workbench.  Be sure to select \"single user\" when starting a managed notebook to run this.  Otherwise the auth won't allow access to the preview.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb2fda-70c8-4733-ac6f-2678e5cbff47",
   "metadata": {
    "id": "87eb2fda-70c8-4733-ac6f-2678e5cbff47"
   },
   "source": [
    "First we need to install the preview libraries for Generative AI.  It's a new version of the AI platform library.  To access the bucket, your user account and project will need to be part of the preview.\n",
    "\n",
    "By default a Vertex AI Workbench Notebook uses a service account.  That account doesn't have access to the bucket where the preview binary is.  So, you'll need to auth.  To do so, open a terminal window in your managed notebook and run the command 'gcloud auth login'.  With that complete, you'll be able to install the preview library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c34bc50c-cb41-4775-9be5-c8b7bd74d1fc",
   "metadata": {
    "id": "c34bc50c-cb41-4775-9be5-c8b7bd74d1fc"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install google-cloud-aiplatform --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d388e",
   "metadata": {
    "id": "707d388e"
   },
   "source": [
    "Let us now fine-tune the `text-bison` model to help us extract entities & relationships better. The untuned model makes is good but we tune here to make it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad823ee-84ca-4de3-9b29-4b324ac5b9ae",
   "metadata": {
    "id": "4ad823ee-84ca-4de3-9b29-4b324ac5b9ae"
   },
   "outputs": [],
   "source": [
    "# Note, you will need to set these variables\n",
    "project_id = 'neo4jbusinessdev'\n",
    "location = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c5a70c-aee1-462b-953a-4c87a524a111",
   "metadata": {
    "id": "42c5a70c-aee1-462b-953a-4c87a524a111"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vertexai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/645488448.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mvertexai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvertexai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vertexai'"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=project_id, location=location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed392ee-66db-42ee-8c63-d4099b4fc36a",
   "metadata": {
    "id": "2ed392ee-66db-42ee-8c63-d4099b4fc36a"
   },
   "source": [
    "We will use a `jsonl` file in the following format to tune our Ingestion model. Our ingestion model will consume text and extract entities & relationships out of it. Each training example should be JSONL record with two keys, for example:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"input_text\": <input prompt>,\n",
    "  \"output_text\": <associated output>\n",
    "}\n",
    "```\n",
    "\n",
    "Model tuning will take a few minutes and consumes TPU resources. If the model is already tuned, you can skip the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "xUJ785QJbNZ-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUJ785QJbNZ-",
    "outputId": "2c936197-02f8-47c5-9e2f-c1f99b755a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524054242\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524054242')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/tune-large-model-20230524054242?project=803648085855\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524054242 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524054242 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524054242 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524054242 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524054242 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524054242 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524054242 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524054242 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524054242 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524054242\n",
      "Tuning has completed. Created Vertex Model: projects/803648085855/locations/us-central1/models/1971567285113978880\n"
     ]
    }
   ],
   "source": [
    "training_data = 'gs://gs_vertex_ai/entity-extraction-trng/entity-extraction-trng-1.jsonl'\n",
    "train_steps = 10\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "\n",
    "model.tune_model(\n",
    "  training_data=training_data,\n",
    "  train_steps=train_steps,\n",
    "  tuning_job_location=\"europe-west4\",\n",
    "  tuned_model_location=\"us-central1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "LXK2Bxm_yLmc",
   "metadata": {
    "id": "LXK2Bxm_yLmc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['projects/803648085855/locations/us-central1/models/1971567285113978880', 'projects/803648085855/locations/us-central1/models/3124488789720825856', 'projects/803648085855/locations/us-central1/models/2671877027170091008', 'projects/803648085855/locations/us-central1/models/7947351409425383424']\n"
     ]
    }
   ],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "entity_extraction_tuned_model = model.list_tuned_model_names()\n",
    "print(entity_extraction_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db2e49",
   "metadata": {
    "id": "48db2e49"
   },
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10113a08",
   "metadata": {
    "id": "10113a08"
   },
   "source": [
    "Now, let's define a function that can help clean the input data. The data refers to some figures like scanned images. We don't have them and so we will remove any such references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e430a536",
   "metadata": {
    "id": "e430a536"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  clean = \"\\n\".join([row for row in text.split(\"\\n\")])\n",
    "  clean = re.sub(r'\\(fig[^)]*\\)', '', clean, flags=re.IGNORECASE)\n",
    "  return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72960046",
   "metadata": {
    "id": "72960046"
   },
   "source": [
    "Let's take this case sheet and extract entities and relations using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43bb3c",
   "metadata": {
    "id": "4e43bb3c"
   },
   "source": [
    "## Prompt Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938880e",
   "metadata": {
    "id": "3938880e"
   },
   "source": [
    "**⚠️** You need to duplicate `config.env.example` file in the left and rename as `config.env`. Edit the values in this file and provide the values for API keys and Neo4j credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ad98c",
   "metadata": {
    "id": "605ad98c"
   },
   "source": [
    "This is a helper function to talk to the LLM with our prompt and text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1148d87e",
   "metadata": {
    "id": "1148d87e"
   },
   "outputs": [],
   "source": [
    "def run_text_model(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    prompt: str,\n",
    "    location: str = \"us-central1\",\n",
    "    tuned_model_name: str = \"\",\n",
    "    ) :\n",
    "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "      model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7781a12b",
   "metadata": {
    "id": "7781a12b"
   },
   "source": [
    "This is a simple prompt to start with. If the processing is very complex, you can also chain the prompts as and when required. I am going to use a single prompt here that will extract the text strictly as per the Entities and Relationships defined. This is a simplification. \n",
    "In the real scenario, especially with medical records, you have to leverage on Domain experts to define the Ontology systematically and capture the important information. You should also be mindful of following the relevant regulations around handling health records,\n",
    "\n",
    "Instead of one single large model, you can also consider chaining a number of smaller ones as per your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4f9a9",
   "metadata": {
    "id": "d2b4f9a9"
   },
   "source": [
    "Let's run our completion task with our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "dcbfd725",
   "metadata": {
    "id": "dcbfd725"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from string import Template\n",
    "from vertexai.preview.language_models import InputOutputTextPair, ChatModel\n",
    "import json \n",
    "\n",
    "def extract_entities_relationships(prompt, tuned_model_name=''):\n",
    "    try:\n",
    "        res = run_text_model(project_id, \"text-bison@001\", 0, 1024, 0.8, 40, prompt, location, tuned_model_name)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "7921fa85",
   "metadata": {
    "id": "7921fa85"
   },
   "outputs": [],
   "source": [
    "prompt=\"\"\"From the Case sheet for a patient below, extract the following Entities & relationships described in the mentioned format \n",
    "1. First, look for these Entity types in the text and generate as comma-separated format similar to entity type.\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. Do not create new entity types that aren't mentioned below. Document must be summarized and stored inside Case entity under `summary` property. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Types:\n",
    "    label:'Case',id:string //Case\n",
    "    label:'Person',id:string,age:string,location:string,gender:string //Patient mentioned in the case\n",
    "    label:'Symptom',id:string,description:string //Symptom Entity; `id` property is the name of the symptom, in lowercase & camel-case & should always start with an alphabet\n",
    "    label:'Disease',id:string,name:string //Disease diagnosed now or previously as per the Case sheet; `id` property is the name of the disease, in lowercase & camel-case & should always start with an alphabet\n",
    "    label:'BodySystem',id:string,name:string //Body Part affected. Eg: Chest, Lungs; id property is the name of the part, in lowercase & camel-case & should always start with an alphabet\n",
    "    label:'Diagnosis',id:string,name:string,description:string,when:string //Diagnostic procedure conducted; `id` property is the summary of the Diagnosis, in lowercase & camel-case & should always start with an alphabet\n",
    "    label:'Biological',id:string,name:string,description:string //Results identified from Diagnosis; `id` property is the summary of the Biological, in lowercase & camel-case & should always start with an alphabet\n",
    "2. Next generate each relationships as triples of head, relationship and tail. To refer the head and tail entity, use their respective `id` property. Relationship property should be mentioned within brackets as comma-separated. They should follow these relationship types below. You will have to generate as many relationships as needed as defined below:\n",
    "    Relationship types:\n",
    "    case|FOR|person\n",
    "    person|HAS_SYMPTOM{when:string,frequency:string,span:string}|symptom //the properties inside HAS_SYMPTOM gets populated from the Case sheet\n",
    "    person|HAS_DISEASE{when:string}|disease //the properties inside HAS_DISEASE gets populated from the Case sheet\n",
    "    symptom|SEEN_ON|chest\n",
    "    disease|AFFECTS|heart\n",
    "    person|HAS_DIAGNOSIS|diagnosis\n",
    "    diagnosis|SHOWED|biological\n",
    "3. Summary & description properties inside entities should be the text summary\n",
    "\n",
    "Output Format (Follow Strictly):\n",
    "{\n",
    "    \"entities\": [{\"label\":\"Case\",\"id\":\"case1\"}, {\"label\":\"Person\",\"id\":\"person1\",\"age\":20,\"gender\":\"female\"}],\n",
    "    \"relationships\": [\"disease|AFFECTS|heart\", \"case1|FOR|person1\", \"person|HAS_DISEASE{when:'2020'}|disease\"]\n",
    "}\n",
    "\n",
    "Question: Now, extract entities & relationships as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6JKeHYgb4op-",
   "metadata": {
    "id": "6JKeHYgb4op-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [{'label': 'Case', 'id': 'case1'},\n",
       "  {'label': 'Person', 'id': 'person1', 'age': '49', 'gender': 'female'},\n",
       "  {'label': 'Disease', 'id': 'disease1', 'name': 'Hepatocellular carcinoma'},\n",
       "  {'label': 'Symptom', 'id': 'symptom1', 'description': 'upper quadrant pain'},\n",
       "  {'label': 'Body', 'id': 'body1', 'name': 'chest'},\n",
       "  {'label': 'Diagnosis',\n",
       "   'id': 'diagnosis1',\n",
       "   'name': 'TACE',\n",
       "   'description': 'transarterial chemoembolization',\n",
       "   'when': '2014-10-04'},\n",
       "  {'label': 'Biological',\n",
       "   'id': 'biological1',\n",
       "   'name': 'oil-like materials',\n",
       "   'description': 'iodinated oil-like materials'}],\n",
       " 'relationships': ['case1|FOR|person1',\n",
       "  \"person1|HAS_SYMPTOM{when:'2014-10-04'}|symptom1\",\n",
       "  'disease1|AFFECTS|body1',\n",
       "  \"disease1|HAS_DISEASE_STAGE{stage:'0'}|disease1\",\n",
       "  'diagnosis1|HAS_BIOLOGICAL|biological1',\n",
       "  'diagnosis1|HAS_PATIENT|person1']}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que = \"\"\"A 49-year-old woman was admitted to the Department of Radiology of the Second Affiliated Hospital of Zhejiang University in October 2004 with right upper quadrant pain and weight loss.\n",
    "She was a hepatitis B virus carrier.\n",
    "Her α-fetoprotein level was 1185.3 ng/mL.\n",
    "Ultrasonography and computed tomography (CT) revealed a 10-cm mass in the posterior segments of the right liver lobe.\n",
    "A 1.5-cm mass was also found in the left lateral segment.\n",
    "These clinical signs indicated that the patient had inoperable HCC and Child-Pugh class A cirrhosis.\n",
    "TACE was offered to the patient.\n",
    "Angiogram demonstrated no obvious hepatic arterio-venous shunt, but multiple smaller masses in both lobes of the liver.\n",
    "An emulsion of oxaliplatin, pirarubicin, hydroxycamptothecin and lipiodol were prepared, 35 mL and 3 mL of the mixture were administered intra-arterially to the right and left hepatic artery, respectively.\n",
    "The patient experienced right upper quadrant pain after TACE and had an uneventful recovery.\n",
    "One month later, a second TACE procedure was performed via the right hepatic artery and 40 mL of the mixture was administered.\n",
    "On the next day, she experienced sudden acute dyspnoea and the peripheral oxygen saturation decreased to 90%.\n",
    "The chest X-ray showed some increased reticular shadows in the left lung, especially in the lower zones, and a chest CT scan revealed multiple iodized oil-like high-density materials in parenchyma of the lung (Figure ​1).\n",
    "After 10 mg dexamethasone i.v. and other supportive therapies were administered, the respiratory symptom was attenuated.\n",
    "Two days later, the patient suffered from a serious headache and transient consciousness loss, accompanying nausea and vomiting followed by confusion, lower extremity weakness.\n",
    "Non-contrast enhanced CT scanning showed multiple disseminated hyper-intense lesions in the brain, consistent with deposition of iodized oil (Figure ​2).\n",
    "One week later, her respiratory and neurologic symptoms disappeared completely, and she was discharged.The patient also consequently completed the other three TACE procedures, during which no similar symptoms occurred.\n",
    "\n",
    "\"\"\"\n",
    "_prompt = Template(prompt).substitute(ctext=clean_text(que))\n",
    "entity_extraction_tuned_model = 'projects/803648085855/locations/us-central1/models/1971567285113978880'\n",
    "result = extract_entities_relationships(_prompt, entity_extraction_tuned_model)\n",
    "\n",
    "result = result.split('Answer:\\n ')[1]\n",
    "result = json.loads(result.replace(\"\\'\", \"'\"))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb2a60",
   "metadata": {
    "id": "8feb2a60"
   },
   "source": [
    "## Neo4j Cypher Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96efc5",
   "metadata": {
    "id": "0b96efc5"
   },
   "source": [
    "The entities and relationships we got from the LLM have to be transformed to Cypher so we can write them into Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "084047d0",
   "metadata": {
    "id": "084047d0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_prop_str(prop_dict, _id):\n",
    "    s = []\n",
    "    for key, val in prop_dict.items():\n",
    "      if key != 'label' and key != 'id':\n",
    "         s.append(_id+\".\"+key+' = \"'+str(val).replace('\\\"', '\"').replace('\"', '\\\"')+'\"') \n",
    "    return ' ON CREATE SET ' + ','.join(s)\n",
    "\n",
    "def get_cypher_compliant_var(_id):\n",
    "    return \"_\"+ re.sub(r'[\\W_]', '', _id)\n",
    "\n",
    "def generate_cypher(in_json):\n",
    "    e_map = {}\n",
    "    e_stmt = []\n",
    "    r_stmt = []\n",
    "    e_stmt_tpl = Template(\"($id:$label{id:'$key'})\")\n",
    "    r_stmt_tpl = Template(\"\"\"\n",
    "      MATCH $src\n",
    "      MATCH $tgt\n",
    "      MERGE ($src_id)-[:$rel]->($tgt_id)\n",
    "    \"\"\")\n",
    "    for obj in in_json:\n",
    "      for j in obj['entities']:\n",
    "          props = ''\n",
    "          label = j['label']\n",
    "          id = j['id']\n",
    "          if label == 'Case':\n",
    "                id = 'c'+str(time.time_ns())\n",
    "          elif label == 'Person':\n",
    "                id = 'p'+str(time.time_ns())\n",
    "          varname = get_cypher_compliant_var(j['id'])\n",
    "          stmt = e_stmt_tpl.substitute(id=varname, label=label, key=id)\n",
    "          e_map[varname] = stmt\n",
    "          e_stmt.append('MERGE '+ stmt + get_prop_str(j, varname))\n",
    "\n",
    "      for st in obj['relationships']:\n",
    "          rels = st.split(\"|\")\n",
    "          src_id = get_cypher_compliant_var(rels[0].strip())\n",
    "          rel = rels[1].strip()\n",
    "          tgt_id = get_cypher_compliant_var(rels[2].strip())\n",
    "          stmt = r_stmt_tpl.substitute(\n",
    "              src_id=src_id, tgt_id=tgt_id, src=e_map[src_id], tgt=e_map[tgt_id], rel=rel)\n",
    "          \n",
    "          r_stmt.append(stmt)\n",
    "\n",
    "    return e_stmt, r_stmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ec143b14",
   "metadata": {
    "id": "ec143b14",
    "outputId": "6d2c5379-3ed9-4be4-ff94-433c21fd1275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"MERGE (_case1:Case{id:'c1684918150024394055'}) ON CREATE SET \", 'MERGE (_person1:Person{id:\\'p1684918150024425570\\'}) ON CREATE SET _person1.age = \"49\",_person1.gender = \"female\"', 'MERGE (_disease1:Disease{id:\\'disease1\\'}) ON CREATE SET _disease1.name = \"Hepatocellular carcinoma\"', 'MERGE (_symptom1:Symptom{id:\\'symptom1\\'}) ON CREATE SET _symptom1.description = \"upper quadrant pain\"', 'MERGE (_body1:Body{id:\\'body1\\'}) ON CREATE SET _body1.name = \"chest\"', 'MERGE (_diagnosis1:Diagnosis{id:\\'diagnosis1\\'}) ON CREATE SET _diagnosis1.name = \"TACE\",_diagnosis1.description = \"transarterial chemoembolization\",_diagnosis1.when = \"2014-10-04\"', 'MERGE (_biological1:Biological{id:\\'biological1\\'}) ON CREATE SET _biological1.name = \"oil-like materials\",_biological1.description = \"iodinated oil-like materials\"'] [\"\\n      MATCH (_case1:Case{id:'c1684918150024394055'})\\n      MATCH (_person1:Person{id:'p1684918150024425570'})\\n      MERGE (_case1)-[:FOR]->(_person1)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1684918150024425570'})\\n      MATCH (_symptom1:Symptom{id:'symptom1'})\\n      MERGE (_person1)-[:HAS_SYMPTOM{when:'2014-10-04'}]->(_symptom1)\\n    \", \"\\n      MATCH (_disease1:Disease{id:'disease1'})\\n      MATCH (_body1:Body{id:'body1'})\\n      MERGE (_disease1)-[:AFFECTS]->(_body1)\\n    \", \"\\n      MATCH (_disease1:Disease{id:'disease1'})\\n      MATCH (_disease1:Disease{id:'disease1'})\\n      MERGE (_disease1)-[:HAS_DISEASE_STAGE{stage:'0'}]->(_disease1)\\n    \", \"\\n      MATCH (_diagnosis1:Diagnosis{id:'diagnosis1'})\\n      MATCH (_biological1:Biological{id:'biological1'})\\n      MERGE (_diagnosis1)-[:HAS_BIOLOGICAL]->(_biological1)\\n    \", \"\\n      MATCH (_diagnosis1:Diagnosis{id:'diagnosis1'})\\n      MATCH (_person1:Person{id:'p1684918150024425570'})\\n      MERGE (_diagnosis1)-[:HAS_PATIENT]->(_person1)\\n    \"]\n"
     ]
    }
   ],
   "source": [
    "ent_cyp, rel_cyp = generate_cypher([result])\n",
    "\n",
    "print(ent_cyp, rel_cyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c69170",
   "metadata": {
    "id": "54c69170",
    "tags": []
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f06013-a653-43cf-be7c-de2888e621f7",
   "metadata": {
    "id": "00f06013-a653-43cf-be7c-de2888e621f7"
   },
   "source": [
    "You will need a Neo4j AuraDS Pro instance.  You can deploy that on Google Cloud Marketplace [here](https://console.cloud.google.com/marketplace/product/endpoints/prod.n4gcp.neo4j.io).\n",
    "\n",
    "With that complete, you'll need to install the Neo4j library and set up your database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d36df-21e4-4bad-969e-23ed11762518",
   "metadata": {
    "id": "a44d36df-21e4-4bad-969e-23ed11762518"
   },
   "outputs": [],
   "source": [
    "%pip install graphdatascience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e621c199-533a-4503-baef-200c5adcd8ad",
   "metadata": {
    "id": "e621c199-533a-4503-baef-200c5adcd8ad"
   },
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecea5ff",
   "metadata": {
    "id": "0ecea5ff"
   },
   "outputs": [],
   "source": [
    "connectionUrl = input('NEO4J_CONN_URL')\n",
    "username = input('NEO4J_USER')\n",
    "password = input('NEO4J_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfa6e8",
   "metadata": {
    "id": "ddbfa6e8"
   },
   "outputs": [],
   "source": [
    "gds = GraphDataScience(connectionUrl, auth=(username, password))\n",
    "gds.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228a3a58",
   "metadata": {
    "id": "228a3a58"
   },
   "source": [
    "Before loading the data, create constraints as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66756bab",
   "metadata": {
    "id": "66756bab"
   },
   "outputs": [],
   "source": [
    "gds.run_cypher('CREATE CONSTRAINT unique_case_id IF NOT EXISTS FOR (n:Case) REQUIRE n.id IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_person_id IF NOT EXISTS FOR (n:Person) REQUIRE (n.id) IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_symptom_id IF NOT EXISTS FOR (n:Symptom) REQUIRE (n.id) IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_disease_id IF NOT EXISTS FOR (n:Disease) REQUIRE n.id IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_bodysys_id IF NOT EXISTS FOR (n:BodySystem) REQUIRE n.id IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_diag_id IF NOT EXISTS FOR (n:Diagnosis) REQUIRE n.id IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_biological_id IF NOT EXISTS FOR (n:Biological) REQUIRE n.id IS UNIQUE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971bf0b3",
   "metadata": {
    "id": "971bf0b3"
   },
   "source": [
    "Ingest the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367ece7",
   "metadata": {
    "id": "7367ece7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for e in ent_cyp:\n",
    "    gds.run_cypher(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f811933",
   "metadata": {
    "id": "0f811933"
   },
   "source": [
    "Ingest relationships now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff4ad1",
   "metadata": {
    "id": "d9ff4ad1"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for r in rel_cyp:\n",
    "    gds.run_cypher(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65581a1",
   "metadata": {
    "id": "c65581a1"
   },
   "source": [
    "This is a helper function to ingest all case sheets inside the `data/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707904c",
   "metadata": {
    "id": "b707904c"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def run_pipeline(count=191):\n",
    "    txt_files = glob.glob(\"data/case_sheets/*.txt\")[0:count]\n",
    "    print(f\"Running pipeline for {len(txt_files)} files\")\n",
    "    failed_files = process_pipeline(txt_files)\n",
    "    print(failed_files)\n",
    "    return failed_files\n",
    "\n",
    "def process_pipeline(files):\n",
    "    failed_files = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            with open(f, 'r') as file:\n",
    "                print(f\"  {f}: Reading File...\")\n",
    "                data = file.read().rstrip()\n",
    "                text = clean_text(data)\n",
    "                print(f\"    {f}: Extracting E & R\")\n",
    "                results = extract_entities_relationships(f, text)\n",
    "                print(f\"    {f}: Generating Cypher\")\n",
    "                ent_cyp, rel_cyp = generate_cypher(results)\n",
    "                print(f\"    {f}: Ingesting Entities\")\n",
    "                for e in ent_cyp:\n",
    "                    gds.run_cypher(e)\n",
    "                print(f\"    {f}: Ingesting Relationships\")\n",
    "                for r in rel_cyp:\n",
    "                    gds.run_cypher(r)\n",
    "                print(f\"    {f}: Processing DONE\")\n",
    "        except Exception as e:\n",
    "            print(f\"    {f}: Processing Failed with exception {e}\")\n",
    "            failed_files.append(f)\n",
    "    return failed_files\n",
    "            \n",
    "def extract_entities_relationships(f, text):\n",
    "    start = timer()\n",
    "    system = \"You are a helpful Medical Case Sheet expert who extracts relevant information and store them on a Neo4j Knowledge Graph\"\n",
    "    prompts = [prompt1]\n",
    "    all_cypher = \"\"\n",
    "    results = []\n",
    "    for p in prompts:\n",
    "      p = Template(p).substitute(ctext=text)\n",
    "      res = process_gpt(system, p)\n",
    "      results.append(json.loads(res))\n",
    "    end = timer()\n",
    "    elapsed = (end-start)\n",
    "    print(f\"    {f}: E & R took {elapsed}secs\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b86a7",
   "metadata": {
    "id": "bb4b86a7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "failed_files = run_pipeline(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e9c48",
   "metadata": {
    "id": "653e9c48"
   },
   "source": [
    "If processing failed for some files due to API Rate limit or some other error, you can retry as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26a851",
   "metadata": {
    "id": "4e26a851"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "failed_files = process_pipeline(failed_files)\n",
    "failed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77de32",
   "metadata": {
    "id": "4d77de32"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UrvbzyY_X8uP",
   "metadata": {
    "id": "UrvbzyY_X8uP"
   },
   "source": [
    "## Cypher Generation for Consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c06e57d-72a9-4b86-8b8d-119690895c02",
   "metadata": {},
   "source": [
    "### Tune the model to generate Cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6316e464-5dbf-475d-8507-de07ba0bc0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524094701\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524094701')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/tune-large-model-20230524094701?project=803648085855\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524094701 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524094701 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524094701 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524094701 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524094701 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524094701 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524094701 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524094701 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524094701 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230524094701\n",
      "Tuning has completed. Created Vertex Model: projects/803648085855/locations/us-central1/models/2255294061638320128\n"
     ]
    }
   ],
   "source": [
    "training_data = 'gs://gs_vertex_ai/eng2cypher/eng-to-cypher-trng-1.jsonl'\n",
    "train_steps = 10\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "\n",
    "model.tune_model(\n",
    "  training_data=training_data,\n",
    "  train_steps=train_steps,\n",
    "  tuning_job_location=\"europe-west4\",\n",
    "  tuned_model_location=\"us-central1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113928b-b83d-4f41-833b-a92e851cca3a",
   "metadata": {},
   "source": [
    "### Generate Cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "5e9f2895-c3c4-401c-a4d6-cf1b80a2050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_to_cypher(prompt, tuned_model_name=''):\n",
    "    try:\n",
    "        res = run_text_model(project_id, \"text-bison@001\", 0, 1024, 0.8, 40, prompt, location, tuned_model_name)\n",
    "        # res = json.loads(res.replace(\"\\'\", \"'\"))\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "EMZysiC9YsqC",
   "metadata": {
    "id": "EMZysiC9YsqC"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Context:\n",
    "You are an expert Neo4j Cypher translator who understands the question in english and convert it to Cypher based on the Neo4j Schema provided.\n",
    "Here are the instructions to follow:\n",
    "1. Use the Neo4j schema to generate cypher compatible ONLY for Neo4j Version 5\n",
    "2. Do not use EXISTS, SIZE keywords in the cypher.\n",
    "3. Use only Nodes and relationships mentioned in the schema while generating the response\n",
    "4. Reply ONLY in Cypher when it makes sense.\n",
    "5. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Heart Disease use `toLower(d.name) contains 'heart disease'`\n",
    "6. Patient node is synonymous to Person.\n",
    "Now, use this Neo4j schema and Reply ONLY in Cypher when it makes sense.\n",
    "Schema:\n",
    "Nodes:\n",
    "    label:'Case',id:string,summary:string //Case Node\n",
    "    label:'Person',id:string,age:string,location:string,gender:string //Patient Node\n",
    "    label:'Symptom',id:string,description:string //Symptom Node\n",
    "    label:'Disease',id:string,name:string //Disease Node\n",
    "    label:'BodySystem',id:string,name:string //Node for Body Part affected Eg: Heart, lungs\n",
    "    label:'Diagnosis',id:string,name:string,description:string,when:string //Diagnostic Node\n",
    "    label:'Biological',id:string,name:string,description:string //Node for Results identified from Diagnosis\n",
    "Relationships:\n",
    "    (:Case)-[:FOR]->(Person)\n",
    "    (:Person)-[:HAS_SYMPTOM{when:string,frequency:string,span:string}]->(Symptom)\n",
    "    (:Person)-[:HAS_DISEASE{when:string}]->(:Disease)\n",
    "    (:Symptom)-[:SEEN_ON]->(:BodySystem)\n",
    "    (:Disease)-[:AFFECTS]->(:BodySystem)\n",
    "    (:Person)-[:HAS_DIAGNOSIS]->(:Diagnosis)\n",
    "    (:Diagnosis)-[:SHOWED]->(:Biological)\n",
    "\n",
    "So, for this question: 'How many teenagers have cough related ailments?', you will answer : MATCH (p:Person)-[:HAS_SYMPTOM]->(s:Symptom) WHERE toInteger(p.age) > 12 AND toInteger(p.age) < 20 AND toLower(s.description) CONTAINS 'cough' RETURN COUNT(p) \n",
    "Because:\n",
    "1. As per schema definition of nodes & relationships above, Person node is related to Symptom node via HAS_SYMPTOM relationship.\n",
    "2. Person's `age` property is a string. So we need to convert to Integer using toInteger before comparing.\n",
    "3. Teenagers are aged 13 to 19. So we add age filter.\n",
    "4. Symptoms are stored in `description` property as per schema. And we are doing a case-insensitive match to get the cough symptom\n",
    "5. Finally, we return the number of patients who match the input criteria using COUNT function\n",
    "\n",
    "\n",
    "Ouput Format (Strict): //Only code as output. No other text\n",
    "MATCH (d:Disease) RETURN d.name as disease, SIZE([(d)-[]-(p:Person) | p]) AS affected_patients ORDER BY affected_patients DESC LIMIT 1\n",
    "\n",
    "Question:\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "que = 'How many teenagers have cough related ailments?'\n",
    "_prompt = Template(prompt).substitute(ctext=clean_text(que))\n",
    "\n",
    "response = english_to_cypher(_prompt, 'projects/803648085855/locations/us-central1/models/2255294061638320128')\n",
    "cypher = response.split('Answer:\\n ')[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "fad060d0-e38a-4b0e-996a-c7307d225d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MATCH (p:Person)-[:HAS_SYMPTOM]->(s:Symptom) WHERE toLower(s.description) CONTAINS 'cough' AND p.age > 1 AND p.age < 19 RETURN count(p)\""
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "lrVRKnn8e1Xn",
   "metadata": {
    "id": "lrVRKnn8e1Xn"
   },
   "outputs": [],
   "source": [
    "# To do - move libraries to where they are imported.  Remove unused libraries.\n",
    "\n",
    "#%pip install graphdatascience\n",
    "#%pip install python-dotenv\n",
    "#%pip install retry\n",
    "\n",
    "\n",
    "# To do - move libraries to where they are imported.  Remove unused libraries.\n",
    "\n",
    "#import os\n",
    "#from retry import retry\n",
    "#import re\n",
    "#from string import Template\n",
    "#import json \n",
    "#import ast\n",
    "#import time\n",
    "#import pandas as pd\n",
    "#from graphdatascience import GraphDataScience\n",
    "#import glob\n",
    "#from timeit import default_timer as timer\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "#from google.cloud import aiplatform\n",
    "#from google.cloud.aiplatform.private_preview.language_models import ChatModel, InputOutputTextPair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427eab0-f9f1-4886-b4d3-11844e42284f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
