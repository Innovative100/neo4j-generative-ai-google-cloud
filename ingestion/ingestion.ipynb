{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d3d39ab",
      "metadata": {
        "id": "3d3d39ab"
      },
      "source": [
        "# Ingestion\n",
        "\n",
        "\n",
        "This notebook parses data from XXX using Google Vertex AI Generative AI.  It then uses Generative AI to create Neo4j Cypher queries which write the data to a Neo4j database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/neo4j-partners/intelligent-app-google-generativeai-neo4j/blob/main/ingestion/ingestion.ipynb?token=GHSAT0AAAAAAB5TWSXKZODTVRMD2NB7DCQMZDNK3FA\" target=\"_blank\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "</td>\n",
        "</table>"
      ],
      "id": "JAPoU8Sm5E6e"
    },
    {
      "cell_type": "markdown",
      "id": "f17a9328-f17d-48ae-b72e-c333fb867eb0",
      "metadata": {
        "id": "f17a9328-f17d-48ae-b72e-c333fb867eb0"
      },
      "source": [
        "## Setup\n",
        "This notebook should be run within Colab.  You cannot currently use Google Vertex AI Workbench because of auth issues.  The \"run in colab\" button won't work because the repo is private."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87eb2fda-70c8-4733-ac6f-2678e5cbff47",
      "metadata": {
        "id": "87eb2fda-70c8-4733-ac6f-2678e5cbff47"
      },
      "source": [
        "First we need to install the preview libraries for Generative AI.  It's a new version of the AI platform library.  To access the bucket, your user account and project will need to be part of the preview.\n",
        "\n",
        "By default a Vertex AI Workbench Notebook uses a service account.  That account doesn't have access to the bucket where the preview binary is.  So, you'll need to auth.  To do so, open a terminal window in your managed notebook and run the command 'gcloud auth login'.  With that complete, you'll be able to install the preview library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15fffe56-f33e-45b0-9d63-a41d02b0b22e",
      "metadata": {
        "id": "15fffe56-f33e-45b0-9d63-a41d02b0b22e"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c1fec41-dc9e-4440-ae09-5e985bfc84f0",
      "metadata": {
        "id": "2c1fec41-dc9e-4440-ae09-5e985bfc84f0"
      },
      "outputs": [],
      "source": [
        "!gsutil cp gs://vertex_sdk_llm_private_releases/SDK/google_cloud_aiplatform-1.25.dev20230413+language.models-py2.py3-none-any.whl ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c34bc50c-cb41-4775-9be5-c8b7bd74d1fc",
      "metadata": {
        "id": "c34bc50c-cb41-4775-9be5-c8b7bd74d1fc"
      },
      "outputs": [],
      "source": [
        "%pip install --user ./google_cloud_aiplatform-1.25.dev20230413+language.models-py2.py3-none-any.whl \"shapely<2.0.0\" --force-reinstall"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0da0ada1-05da-4e85-af45-927b3e3001fd",
      "metadata": {
        "id": "0da0ada1-05da-4e85-af45-927b3e3001fd"
      },
      "source": [
        "Important -- You will now need to restart the kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "707d388e",
      "metadata": {
        "id": "707d388e"
      },
      "source": [
        "The training below shows how to instruction-tune a text-bison model. The chat-bison model which we are going to use in the ingestion process is currently not tunable. The below code is meant to show an example of fine-tuning.\n",
        "\n",
        "Clarification --- what is the tuning job doing if this isn't tunable?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad823ee-84ca-4de3-9b29-4b324ac5b9ae",
      "metadata": {
        "id": "4ad823ee-84ca-4de3-9b29-4b324ac5b9ae"
      },
      "outputs": [],
      "source": [
        "# Note, you will need to set these variables\n",
        "project_id = 'neo4jbusinessdev'\n",
        "location = 'us-central1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42c5a70c-aee1-462b-953a-4c87a524a111",
      "metadata": {
        "id": "42c5a70c-aee1-462b-953a-4c87a524a111"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform.private_preview.language_models import TextGenerationModel\n",
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ed392ee-66db-42ee-8c63-d4099b4fc36a",
      "metadata": {
        "id": "2ed392ee-66db-42ee-8c63-d4099b4fc36a"
      },
      "source": [
        "Question -- is eng2cypher a parsed copy of data.  How did it end up that way?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xUJ785QJbNZ-",
      "metadata": {
        "id": "xUJ785QJbNZ-",
        "outputId": "6d73da42-625b-41bc-e34b-7a9e26f18b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating PipelineJob\n",
            "PipelineJob created. Resource name: projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230523211814\n",
            "To use this PipelineJob in another session:\n",
            "pipeline_job = aiplatform.PipelineJob.get('projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230523211814')\n",
            "View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/tune-large-model-20230523211814?project=803648085855\n",
            "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230523211814 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230523211814 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230523211814 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230523211814 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230523211814 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_1/2507441956.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mtrain_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mtuning_job_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"europe-west4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mtuned_model_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"us-central1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/private_preview/language_models/language_models.py\u001b[0m in \u001b[0;36mtune_model\u001b[0;34m(self, training_data, train_steps, tuning_job_location, tuned_model_location)\u001b[0m\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mtuned_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;31m# The UXR study attendees preferred to tune model in place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuned_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/private_preview/language_models/language_models.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         upload_model_tasks = [\n\u001b[1;32m    682\u001b[0m             \u001b[0mtask_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;34m\"\"\"Wait for this PipelineJob to complete.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_latest_future\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m                 \u001b[0mlog_wait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_wait\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                 \u001b[0mprevious_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;31m# Error is only populated when the job state is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "training_data = 'gs://gs_vertex_ai/eng2cypher/eng2cypher.jsonl'\n",
        "train_steps = 10\n",
        "\n",
        "aiplatform.init(project=project_id, location=location)\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison-001\")\n",
        "\n",
        "model.tune_model(\n",
        "  training_data=training_data,\n",
        "  train_steps=train_steps,\n",
        "  tuning_job_location=\"europe-west4\",\n",
        "  tuned_model_location=\"us-central1\",\n",
        ")\n",
        "\n",
        "# Test the tuned model\n",
        "print(model.predict(\"Tell me some ideas combining VR and fitness:\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LXK2Bxm_yLmc",
      "metadata": {
        "id": "LXK2Bxm_yLmc",
        "outputId": "d85d87cd-1312-4e78-e627-679e672e6ae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['projects/803648085855/locations/us-central1/models/7947351409425383424']\n"
          ]
        }
      ],
      "source": [
        "from google.cloud.aiplatform.private_preview.language_models import TextGenerationModel\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=project_id, location=location)\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison-001\")\n",
        "tuned_model_names = model.list_tuned_model_names()\n",
        "print(tuned_model_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48db2e49",
      "metadata": {
        "id": "48db2e49"
      },
      "source": [
        "## Data Cleansing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10113a08",
      "metadata": {
        "id": "10113a08"
      },
      "source": [
        "Now, let's define a function that can help clean the input data. The data refers to some figures like scanned images. We don't have them and so we will remove any such references."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e430a536",
      "metadata": {
        "id": "e430a536"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  clean = \"\\n\".join([row for row in text.split(\"\\n\")])\n",
        "  clean = re.sub(r'\\(fig[^)]*\\)', '', clean, flags=re.IGNORECASE)\n",
        "  return clean"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72960046",
      "metadata": {
        "id": "72960046"
      },
      "source": [
        "Let's take this case sheet and extract entities and relations using LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9cc20d7",
      "metadata": {
        "id": "e9cc20d7"
      },
      "outputs": [],
      "source": [
        "sample_que = \"\"\"The patient was a 34-yr-old man who presented with complaints of fever and a chronic cough.\n",
        "He was a smoker and had a history of pulmonary tuberculosis that had been treated and cured.\n",
        "A computed tomographic (CT) scan revealed multiple tiny nodules in both lungs.\n",
        "A thoracoscopic lung biopsy was taken from the right upper lobe.\n",
        "The microscopic examination revealed a typical LCH.\n",
        "The tumor cells had vesicular and grooved nuclei, and they formed small aggregations around the bronchioles (Fig.1).\n",
        "The tumor cells were strongly positive for S-100 protein, vimentin, CD68 and CD1a.\n",
        "There were infiltrations of lymphocytes and eosinophils around the tumor cells.\n",
        "With performing additional radiologic examinations, no other organs were thought to be involved.\n",
        "He quit smoking, but he received no other specific treatment.\n",
        "He was well for the following one year.\n",
        "After this, a follow-up CT scan was performed and it showed a 4 cm-sized mass in the left lower lobe, in addition to the multiple tiny nodules in both lungs (Fig.2).\n",
        "A needle biopsy specimen revealed the possibility of a sarcoma; therefore, a lobectomy was performed.\n",
        "Grossly, a 4 cm-sized poorly-circumscribed lobulated gray-white mass was found (Fig.3), and there were a few small satellite nodules around the main mass.\n",
        "Microscopically, the tumor cells were aggregated in large sheets and they showed an infiltrative growth.\n",
        "The cytologic features of some of the tumor cells were similar to those seen in a typical LCH.\n",
        "However, many tumor cells showed overtly malignant cytologic features such as pleomorphic/hyperchromatic nuclei and prominent nucleoli (Fig.4), and multinucleated tumor giant cells were also found.\n",
        "There were numerous mitotic figures ranging from 30 to 60 per 10 high power fields, and some of them were abnormal.\n",
        "A few foci of typical LCH remained around the main tumor mass.\n",
        "Immunohistochemically, the tumor cells were strongly positive for S-100 protein (Fig.5) and vimentin; they were also positive for CD68 (Dako N1577, Clone KPI), and focally positive for CD1a (Fig.6), and they were negative for cytokeratin, epithelial membrane antigen, CD3, CD20 and HMB45.\n",
        "The ultrastructural analysis failed to demonstrate any Birbeck granules in the cytoplasm of the tumor cells.\n",
        "Now, at five months after lobectomy, the patient is doing well with no significant change in the radiologic findings.\n",
        "\"\"\"\n",
        "\n",
        "sample_ans = \"\"\"\n",
        "{'entities': [{'label': 'Case',\n",
        "    'id': 'case1',\n",
        "    'summary': '34-yr-old man with fever, chronic cough, history of pulmonary tuberculosis, LCH diagnosis, and sarcoma. Underwent lobectomy and is doing well.'},\n",
        "   {'label': 'Person',\n",
        "    'id': 'person1',\n",
        "    'age': '34',\n",
        "    'location': '',\n",
        "    'gender': 'male'},\n",
        "   {'label': 'Symptom', 'id': 'fever', 'description': 'Fever'},\n",
        "   {'label': 'Symptom', 'id': 'chronicCough', 'description': 'Chronic cough'},\n",
        "   {'label': 'Disease',\n",
        "    'id': 'pulmonaryTuberculosis',\n",
        "    'name': 'Pulmonary Tuberculosis'},\n",
        "   {'label': 'Disease',\n",
        "    'id': 'langerhansCellHistiocytosis',\n",
        "    'name': 'Langerhans Cell Histiocytosis'},\n",
        "   {'label': 'Disease', 'id': 'sarcoma', 'name': 'Sarcoma'},\n",
        "   {'label': 'BodySystem', 'id': 'lungs', 'name': 'Lungs'},\n",
        "   {'label': 'BodySystem', 'id': 'heart', 'name': 'Heart'},\n",
        "   {'label': 'Diagnosis',\n",
        "    'id': 'ctScan',\n",
        "    'name': 'CT Scan',\n",
        "    'description': 'Computed Tomographic (CT) scan',\n",
        "    'when': 'initial'},\n",
        "   {'label': 'Diagnosis',\n",
        "    'id': 'thoracoscopicLungBiopsy',\n",
        "    'name': 'Thoracoscopic Lung Biopsy',\n",
        "    'description': 'Thoracoscopic lung biopsy from the right upper lobe',\n",
        "    'when': 'initial'},\n",
        "   {'label': 'Diagnosis',\n",
        "    'id': 'followUpCtScan',\n",
        "    'name': 'Follow-up CT Scan',\n",
        "    'description': 'Follow-up CT scan showing a 4 cm-sized mass in the left lower lobe',\n",
        "    'when': 'one year later'},\n",
        "   {'label': 'Diagnosis',\n",
        "    'id': 'needleBiopsy',\n",
        "    'name': 'Needle Biopsy',\n",
        "    'description': 'Needle biopsy specimen revealing the possibility of a sarcoma',\n",
        "    'when': 'one year later'},\n",
        "   {'label': 'Diagnosis',\n",
        "    'id': 'lobectomy',\n",
        "    'name': 'Lobectomy',\n",
        "    'description': 'Lobectomy performed to remove the mass',\n",
        "    'when': 'one year later'},\n",
        "   {'label': 'Biological',\n",
        "    'id': 'multipleTinyNodules',\n",
        "    'name': 'Multiple Tiny Nodules',\n",
        "    'description': 'Multiple tiny nodules in both lungs'},\n",
        "   {'label': 'Biological',\n",
        "    'id': 'lchCells',\n",
        "    'name': 'LCH Cells',\n",
        "    'description': 'Typical LCH cells with vesicular and grooved nuclei'},\n",
        "   {'label': 'Biological',\n",
        "    'id': 'tumorCells',\n",
        "    'name': 'Tumor Cells',\n",
        "    'description': 'Tumor cells with malignant cytologic features'}],\n",
        "  'relationships': ['case1|FOR|person1',\n",
        "   \"person1|HAS_SYMPTOM{when:'initial',frequency:'',span:''}|fever\",\n",
        "   \"person1|HAS_SYMPTOM{when:'initial',frequency:'',span:''}|chronicCough\",\n",
        "   \"person1|HAS_DISEASE{when:'past'}|pulmonaryTuberculosis\",\n",
        "   \"person1|HAS_DISEASE{when:'initial'}|langerhansCellHistiocytosis\",\n",
        "   \"person1|HAS_DISEASE{when:'one year later'}|sarcoma\",\n",
        "   'chronicCough|SEEN_ON|lungs',\n",
        "   'langerhansCellHistiocytosis|AFFECTS|lungs',\n",
        "   'sarcoma|AFFECTS|lungs',\n",
        "   'person1|HAS_DIAGNOSIS|ctScan',\n",
        "   'person1|HAS_DIAGNOSIS|thoracoscopicLungBiopsy',\n",
        "   'person1|HAS_DIAGNOSIS|followUpCtScan',\n",
        "   'person1|HAS_DIAGNOSIS|needleBiopsy',\n",
        "   'person1|HAS_DIAGNOSIS|lobectomy',\n",
        "   'ctScan|SHOWED|multipleTinyNodules',\n",
        "   'thoracoscopicLungBiopsy|SHOWED|lchCells',\n",
        "   'lobectomy|SHOWED|tumorCells']}\n",
        "\"\"\"\n",
        "\n",
        "que = \"\"\"A 28-year-old previously healthy man presented with a 6-week history of palpitations.\n",
        "The symptoms occurred during rest, 2–3 times per week, lasted up to 30 minutes at a time and were associated with dyspnea.\n",
        "Except for a grade 2/6 holosystolic tricuspid regurgitation murmur (best heard at the left sternal border with inspiratory accentuation), physical examination yielded unremarkable findings.\n",
        "An electrocardiogram (ECG) revealed normal sinus rhythm and a Wolff– Parkinson– White pre-excitation pattern (Fig.1: Top), produced by a right-sided accessory pathway.\n",
        "Transthoracic echocardiography demonstrated the presence of Ebstein's anomaly of the tricuspid valve, with apical displacement of the valve and formation of an “atrialized” right ventricle (a functional unit between the right atrium and the inlet [inflow] portion of the right ventricle) (Fig.2).\n",
        "The anterior tricuspid valve leaflet was elongated (Fig.2C, arrow), whereas the septal leaflet was rudimentary (Fig.2C, arrowhead).\n",
        "Contrast echocardiography using saline revealed a patent foramen ovale with right-to-left shunting and bubbles in the left atrium (Fig.2D).\n",
        "The patient underwent an electrophysiologic study with mapping of the accessory pathway, followed by radiofrequency ablation (interruption of the pathway using the heat generated by electromagnetic waves at the tip of an ablation catheter).\n",
        "His post-ablation ECG showed a prolonged PR interval and an odd “second” QRS complex in leads III, aVF and V2–V4 (Fig.1Bottom), a consequence of abnormal impulse conduction in the “atrialized” right ventricle.\n",
        "The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e43bb3c",
      "metadata": {
        "id": "4e43bb3c"
      },
      "source": [
        "## Prompt Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3938880e",
      "metadata": {
        "id": "3938880e"
      },
      "source": [
        "**⚠️** You need to duplicate `config.env.example` file in the left and rename as `config.env`. Edit the values in this file and provide the values for API keys and Neo4j credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WypXZ0cG54ub",
      "metadata": {
        "id": "WypXZ0cG54ub"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cd7d619",
      "metadata": {
        "id": "8cd7d619"
      },
      "outputs": [],
      "source": [
        "#load_dotenv('/content/drive/MyDrive/Colab Notebooks/GenAI-Playground/config-gcp.env', override=True)\n",
        "#\n",
        "#shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "#PROJECT_ID = os.getenv('PROJECT_ID')\n",
        "#os.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID\n",
        "#os.environ['GCLOUD_REGION'] = 'us-central1'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "605ad98c",
      "metadata": {
        "id": "605ad98c"
      },
      "source": [
        "This is a helper function to talk to the LLM with our prompt and text input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1148d87e",
      "metadata": {
        "id": "1148d87e"
      },
      "outputs": [],
      "source": [
        "def process_gpt(\n",
        "    project_id: str,\n",
        "    model_name: str,\n",
        "    temperature: float,\n",
        "    max_output_tokens: int,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        "    context: str,\n",
        "    prompt: str,\n",
        "    que: str,\n",
        "    examples,\n",
        "    location: str = \"us-central1\"\n",
        "    ) :\n",
        "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
        "    aiplatform.init(project=project_id, location=location)\n",
        "\n",
        "    chat_model = ChatModel.from_pretrained(model_name)\n",
        "    parameters = {\n",
        "      \"temperature\": temperature,\n",
        "      \"max_output_tokens\": max_output_tokens,\n",
        "      \"top_p\": top_p,\n",
        "      \"top_k\": top_k,\n",
        "    }\n",
        "\n",
        "    chat = chat_model.start_chat(\n",
        "      context=context,\n",
        "      examples=examples\n",
        "    )\n",
        "    return chat.send_message(prompt+que,**parameters)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7781a12b",
      "metadata": {
        "id": "7781a12b"
      },
      "source": [
        "This is a simple prompt to start with. If the processing is very complex, you can also chain the prompts as and when required. I am going to use a single prompt here that helps me to extract the text strictly as per the Entities and Relationships defined. This is a simplification. In the real scenario, especially with medical records, you have to leverage on Domain experts to define the Ontology systematically and capture the important information. You might also be fine-tuning the model as and when required.\n",
        "\n",
        "Also, instead of one single large model, you can also consider chaining a number of smaller ones as per your needs.\n",
        "\n",
        "We are going with this graph schema for our case sheet:\n",
        "\n",
        "![schema.png](https://github.com/neo4j-partners/intelligent-app-google-generativeai-neo4j/blob/main/ingestion/schema.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7921fa85",
      "metadata": {
        "id": "7921fa85"
      },
      "outputs": [],
      "source": [
        "prompt=\"\"\"From the Case sheet for a patient below, extract the following Entities & relationships described in the mentioned format \n",
        "0. ALWAYS FINISH THE OUTPUT. Never send partial responses\n",
        "1. First, look for these Entity types in the text and generate as comma-separated format similar to entity type.\n",
        "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. Do not create new entity types that aren't mentioned below. Document must be summarized and stored inside Case entity under `summary` property. You will have to generate as many entities as needed as per the types below:\n",
        "    Entity Types:\n",
        "    label:'Case',id:string,summary:string //Case\n",
        "    label:'Person',id:string,age:string,location:string,gender:string //Patient mentioned in the case\n",
        "    label:'Symptom',id:string,description:string //Symptom Entity; `id` property is the name of the symptom, in lowercase & camel-case & should always start with an alphabet\n",
        "    label:'Disease',id:string,name:string //Disease diagnosed now or previously as per the Case sheet; `id` property is the name of the disease, in lowercase & camel-case & should always start with an alphabet\n",
        "    label:'BodySystem',id:string,name:string //Body Part affected. Eg: Chest, Lungs; id property is the name of the part, in lowercase & camel-case & should always start with an alphabet\n",
        "    label:'Diagnosis',id:string,name:string,description:string,when:string //Diagnostic procedure conducted; `id` property is the summary of the Diagnosis, in lowercase & camel-case & should always start with an alphabet\n",
        "    label:'Biological',id:string,name:string,description:string //Results identified from Diagnosis; `id` property is the summary of the Biological, in lowercase & camel-case & should always start with an alphabet\n",
        "    \n",
        "3. Next generate each relationships as triples of head, relationship and tail. To refer the head and tail entity, use their respective `id` property. Relationship property should be mentioned within brackets as comma-separated. They should follow these relationship types below. You will have to generate as many relationships as needed as defined below:\n",
        "    Relationship types:\n",
        "    case|FOR|person\n",
        "    person|HAS_SYMPTOM{when:string,frequency:string,span:string}|symptom //the properties inside HAS_SYMPTOM gets populated from the Case sheet\n",
        "    person|HAS_DISEASE{when:string}|disease //the properties inside HAS_DISEASE gets populated from the Case sheet\n",
        "    symptom|SEEN_ON|chest\n",
        "    disease|AFFECTS|heart\n",
        "    person|HAS_DIAGNOSIS|diagnosis\n",
        "    diagnosis|SHOWED|biological\n",
        "4. Do not send any response other than code block in the response\n",
        "\n",
        "The output should look like :\n",
        "{\n",
        "    \"entities\": [{\"label\":\"Case\",\"id\":string,\"summary\":string}],\n",
        "    \"relationships\": [\"disease|AFFECTS|heart\"]\n",
        "}\n",
        "\n",
        "Case Sheet:\n",
        "$ctext\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2b4f9a9",
      "metadata": {
        "id": "d2b4f9a9"
      },
      "source": [
        "Let's run our completion task with our LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcbfd725",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcbfd725",
        "outputId": "1136557f-5164-4039-9f6a-9013d22677b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/cloud-large-language-models/locations/us-central1/endpoints/chat-bison-001' (or it may not exist). [reason: \"IAM_PERMISSION_DENIED\"\n",
            "domain: \"aiplatform.googleapis.com\"\n",
            "metadata {\n",
            "  key: \"resource\"\n",
            "  value: \"projects/cloud-large-language-models/locations/us-central1/endpoints/chat-bison-001\"\n",
            "}\n",
            "metadata {\n",
            "  key: \"permission\"\n",
            "  value: \"aiplatform.endpoints.predict\"\n",
            "}\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from string import Template\n",
        "from google.cloud.aiplatform.private_preview.language_models import InputOutputTextPair, ChatModel\n",
        "\n",
        "\n",
        "def run_completion(prompt, results, ctext):\n",
        "    try:\n",
        "      pr = Template(prompt).substitute(ctext=ctext)\n",
        "      res = process_gpt(project_id,\n",
        "                        'chat-bison-001'\n",
        "                        , 0, 1024, 0.8, 40, '''You are a helpful Medical Case Sheet expert who extracts relevant information which will be eventually used to store them on a Neo4j Knowledge Graph after processing''',\n",
        "                        prompt, que, [\n",
        "        InputOutputTextPair(\n",
        "          input_text=prompt+sample_que,\n",
        "          output_text=sample_ans, \n",
        "        )\n",
        "      ], location=\"us-central1\")\n",
        "      results.append(res)\n",
        "      return results\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "prompts = [prompt]\n",
        "results = []\n",
        "\n",
        "for p in prompts:\n",
        "  results = run_completion(p, results, clean_text(sample_que))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N_4Ub0umK6xI",
      "metadata": {
        "id": "N_4Ub0umK6xI"
      },
      "outputs": [],
      "source": [
        "results[0].text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8feb2a60",
      "metadata": {
        "id": "8feb2a60"
      },
      "source": [
        "## Neo4j Cypher Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b96efc5",
      "metadata": {
        "id": "0b96efc5"
      },
      "source": [
        "The entities and relationships we got from the LLM have to be transformed to Cypher so we can write them into Neo4j."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "084047d0",
      "metadata": {
        "id": "084047d0"
      },
      "outputs": [],
      "source": [
        "def get_prop_str(prop_dict, _id):\n",
        "    s = []\n",
        "    for key, val in prop_dict.items():\n",
        "      if key != 'label' and key != 'id':\n",
        "         s.append(_id+\".\"+key+' = \"'+str(val).replace('\\\"', '\"').replace('\"', '\\\"')+'\"') \n",
        "    return ' ON CREATE SET ' + ','.join(s)\n",
        "\n",
        "def get_cypher_compliant_var(_id):\n",
        "    return \"_\"+ re.sub(r'[\\W_]', '', _id)\n",
        "\n",
        "def generate_cypher(in_json):\n",
        "    e_map = {}\n",
        "    e_stmt = []\n",
        "    r_stmt = []\n",
        "    e_stmt_tpl = Template(\"($id:$label{id:'$key'})\")\n",
        "    r_stmt_tpl = Template(\"\"\"\n",
        "      MATCH $src\n",
        "      MATCH $tgt\n",
        "      MERGE ($src_id)-[:$rel]->($tgt_id)\n",
        "    \"\"\")\n",
        "    for obj in in_json:\n",
        "      for j in obj['entities']:\n",
        "          props = ''\n",
        "          label = j['label']\n",
        "          id = j['id']\n",
        "          if label == 'Case':\n",
        "                id = 'c'+str(time.time_ns())\n",
        "          elif label == 'Person':\n",
        "                id = 'p'+str(time.time_ns())\n",
        "          varname = get_cypher_compliant_var(j['id'])\n",
        "          stmt = e_stmt_tpl.substitute(id=varname, label=label, key=id)\n",
        "          e_map[varname] = stmt\n",
        "          e_stmt.append('MERGE '+ stmt + get_prop_str(j, varname))\n",
        "\n",
        "      for st in obj['relationships']:\n",
        "          rels = st.split(\"|\")\n",
        "          src_id = get_cypher_compliant_var(rels[0].strip())\n",
        "          rel = rels[1].strip()\n",
        "          tgt_id = get_cypher_compliant_var(rels[2].strip())\n",
        "          stmt = r_stmt_tpl.substitute(\n",
        "              src_id=src_id, tgt_id=tgt_id, src=e_map[src_id], tgt=e_map[tgt_id], rel=rel)\n",
        "          \n",
        "          r_stmt.append(stmt)\n",
        "\n",
        "    return e_stmt, r_stmt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec143b14",
      "metadata": {
        "id": "ec143b14",
        "outputId": "6d2c5379-3ed9-4be4-ff94-433c21fd1275"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_1/797766079.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ment_cyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_cyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_cypher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_1/231215446.py\u001b[0m in \u001b[0;36mgenerate_cypher\u001b[0;34m(in_json)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mMERGE\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;31m$\u001b[0m\u001b[0msrc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m$\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;31m$\u001b[0m\u001b[0mtgt_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\")\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0min_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mprops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ],
      "source": [
        "ent_cyp, rel_cyp = generate_cypher(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54c69170",
      "metadata": {
        "id": "54c69170"
      },
      "source": [
        "## Data Ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f06013-a653-43cf-be7c-de2888e621f7",
      "metadata": {
        "id": "00f06013-a653-43cf-be7c-de2888e621f7"
      },
      "source": [
        "You will need a Neo4j AuraDS Pro instance.  You can deploy that on Google Cloud Marketplace [here](https://console.cloud.google.com/marketplace/product/endpoints/prod.n4gcp.neo4j.io).\n",
        "\n",
        "With that complete, you'll need to install the Neo4j library and set up your database connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a44d36df-21e4-4bad-969e-23ed11762518",
      "metadata": {
        "id": "a44d36df-21e4-4bad-969e-23ed11762518"
      },
      "outputs": [],
      "source": [
        "%pip install graphdatascience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e621c199-533a-4503-baef-200c5adcd8ad",
      "metadata": {
        "id": "e621c199-533a-4503-baef-200c5adcd8ad"
      },
      "outputs": [],
      "source": [
        "from graphdatascience import GraphDataScience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ecea5ff",
      "metadata": {
        "id": "0ecea5ff"
      },
      "outputs": [],
      "source": [
        "connectionUrl = os.getenv('NEO4J_CONN_URL')\n",
        "username = os.getenv('NEO4J_USER')\n",
        "password = os.getenv('NEO4J_PASSWORD')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddbfa6e8",
      "metadata": {
        "id": "ddbfa6e8"
      },
      "outputs": [],
      "source": [
        "gds = GraphDataScience(connectionUrl, auth=(username, password))\n",
        "gds.version()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "228a3a58",
      "metadata": {
        "id": "228a3a58"
      },
      "source": [
        "Before loading the data, create constraints as below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66756bab",
      "metadata": {
        "id": "66756bab"
      },
      "outputs": [],
      "source": [
        "gds.run_cypher('CREATE CONSTRAINT unique_case_id IF NOT EXISTS FOR (n:Case) REQUIRE n.id IS UNIQUE')\n",
        "gds.run_cypher('CREATE CONSTRAINT unique_person_id IF NOT EXISTS FOR (n:Person) REQUIRE (n.id) IS UNIQUE')\n",
        "gds.run_cypher('CREATE CONSTRAINT unique_symptom_id IF NOT EXISTS FOR (n:Symptom) REQUIRE (n.id) IS UNIQUE')\n",
        "gds.run_cypher('CREATE CONSTRAINT unique_disease_id IF NOT EXISTS FOR (n:Disease) REQUIRE n.id IS UNIQUE')\n",
        "gds.run_cypher('CREATE CONSTRAINT unique_bodysys_id IF NOT EXISTS FOR (n:BodySystem) REQUIRE n.id IS UNIQUE')\n",
        "gds.run_cypher('CREATE CONSTRAINT unique_diag_id IF NOT EXISTS FOR (n:Diagnosis) REQUIRE n.id IS UNIQUE')\n",
        "gds.run_cypher('CREATE CONSTRAINT unique_biological_id IF NOT EXISTS FOR (n:Biological) REQUIRE n.id IS UNIQUE')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "971bf0b3",
      "metadata": {
        "id": "971bf0b3"
      },
      "source": [
        "Ingest the entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7367ece7",
      "metadata": {
        "id": "7367ece7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for e in ent_cyp:\n",
        "    gds.run_cypher(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f811933",
      "metadata": {
        "id": "0f811933"
      },
      "source": [
        "Ingest relationships now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9ff4ad1",
      "metadata": {
        "id": "d9ff4ad1"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for r in rel_cyp:\n",
        "    gds.run_cypher(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c65581a1",
      "metadata": {
        "id": "c65581a1"
      },
      "source": [
        "This is a helper function to ingest all case sheets inside the `data/` directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b707904c",
      "metadata": {
        "id": "b707904c"
      },
      "outputs": [],
      "source": [
        "def run_pipeline(count=191):\n",
        "    txt_files = glob.glob(\"data/case_sheets/*.txt\")[0:count]\n",
        "    print(f\"Running pipeline for {len(txt_files)} files\")\n",
        "    failed_files = process_pipeline(txt_files)\n",
        "    print(failed_files)\n",
        "    return failed_files\n",
        "\n",
        "def process_pipeline(files):\n",
        "    failed_files = []\n",
        "    for f in files:\n",
        "        try:\n",
        "            with open(f, 'r') as file:\n",
        "                print(f\"  {f}: Reading File...\")\n",
        "                data = file.read().rstrip()\n",
        "                text = clean_text(data)\n",
        "                print(f\"    {f}: Extracting E & R\")\n",
        "                results = extract_entities_relationships(f, text)\n",
        "                print(f\"    {f}: Generating Cypher\")\n",
        "                ent_cyp, rel_cyp = generate_cypher(results)\n",
        "                print(f\"    {f}: Ingesting Entities\")\n",
        "                for e in ent_cyp:\n",
        "                    gds.run_cypher(e)\n",
        "                print(f\"    {f}: Ingesting Relationships\")\n",
        "                for r in rel_cyp:\n",
        "                    gds.run_cypher(r)\n",
        "                print(f\"    {f}: Processing DONE\")\n",
        "        except Exception as e:\n",
        "            print(f\"    {f}: Processing Failed with exception {e}\")\n",
        "            failed_files.append(f)\n",
        "    return failed_files\n",
        "            \n",
        "def extract_entities_relationships(f, text):\n",
        "    start = timer()\n",
        "    system = \"You are a helpful Medical Case Sheet expert who extracts relevant information and store them on a Neo4j Knowledge Graph\"\n",
        "    prompts = [prompt1]\n",
        "    all_cypher = \"\"\n",
        "    results = []\n",
        "    for p in prompts:\n",
        "      p = Template(p).substitute(ctext=text)\n",
        "      res = process_gpt(system, p)\n",
        "      results.append(json.loads(res))\n",
        "    end = timer()\n",
        "    elapsed = (end-start)\n",
        "    print(f\"    {f}: E & R took {elapsed}secs\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb4b86a7",
      "metadata": {
        "id": "bb4b86a7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "failed_files = run_pipeline(200)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "653e9c48",
      "metadata": {
        "id": "653e9c48"
      },
      "source": [
        "If processing failed for some files due to API Rate limit or some other error, you can retry as below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e26a851",
      "metadata": {
        "id": "4e26a851"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "failed_files = process_pipeline(failed_files)\n",
        "failed_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d77de32",
      "metadata": {
        "id": "4d77de32"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UrvbzyY_X8uP",
      "metadata": {
        "id": "UrvbzyY_X8uP"
      },
      "source": [
        "## Cypher Generation for Consumption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EMZysiC9YsqC",
      "metadata": {
        "id": "EMZysiC9YsqC"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "def run_completion(prompt, que, results, ctext):\n",
        "    try:\n",
        "      pr = Template(prompt).substitute(ctext=ctext)\n",
        "      examples = [InputOutputTextPair(\n",
        "          input_text=prompt+'Which disease affect most of my patients?',\n",
        "          output_text=\"\"\"MATCH (d:Disease) RETURN d.name as disease, SIZE([(d)-[]-(p:Person) | p]) AS affected_patients ORDER BY affected_patients DESC LIMIT 1\"\"\"\n",
        "        ), InputOutputTextPair(\n",
        "          input_text=prompt+'Which patient has the most number of symptoms?',\n",
        "          output_text=\"\"\"MATCH (p:Person)-[hasSymptom]->(s:Symptom) RETURN p.name AS patient, COUNT(s) AS symptoms ORDER BY symptoms DESC LIMIT 1\"\"\"\n",
        "        )]\n",
        "      res = process_gpt(PROJECT_ID,\n",
        "                        'chat-bison-001'\n",
        "                        , 0, 1024, 0.8, 40, '''You are an assistant that translates english to Neo4j cypher\\n''',\n",
        "                        prompt, que, examples, location=\"us-central1\")\n",
        "      results.append(res)\n",
        "      return results\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "prompt = '''Using this Neo4j schema and Reply ONLY in Cypher when it makes sense.\\nHere are the instructions to follow:\\n1. Use the Neo4j schema to generate cypher compatible ONLY for Neo4j Version 5\\n2. Do not use EXISTS, SIZE keywords in the cypher.\\n3. Use only Nodes and relationships mentioned in the schema while generating the response\\n4. Reply ONLY in Cypher when it makes sense.\\n5. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Heart Disease use `toLower(d.name) contains 'heart disease'`\\n6. Patient node is synonymous to Person\\n\\nSchema:\\nNodes:\\n    label:'Case',id:string,summary:string //Case Node\\n    label:'Person',id:string,age:string,location:string,gender:string //Patient Node\\n    label:'Symptom',id:string,description:string //Symptom Node\\n    label:'Disease',id:string,name:string //Disease Node\\n    label:'BodySystem',id:string,name:string //Node for Body Part affected Eg: Heart, lungs\\n    label:'Diagnosis',id:string,name:string,description:string,when:string //Diagnostic Node\\n    label:'Biological',id:string,name:string,description:string //Node for Results identified from Diagnosis\\n\\nRelationships:\\n    (:Case)-[:FOR]->(Person)\\n    (:Person)-[:HAS_SYMPTOM{when:string,frequency:string,span:string}]->(Symptom)\\n    (:Person)-[:HAS_DISEASE{when:string}]->(:Disease)\\n    (:Symptom)-[:SEEN_ON]->(:BodySystem)\\n    (:Disease)-[:AFFECTS]->(:BodySystem)\\n    (:Person)-[:HAS_DIAGNOSIS]->(:Diagnosis)\\n    (:Diagnosis)-[:SHOWED]->(:Biological)'''      \n",
        "results = []\n",
        "run_completion(prompt, 'Which age group has the most number of heart diseases?', results, clean_text(sample_que))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gWn7q-rfjKSk",
      "metadata": {
        "id": "gWn7q-rfjKSk"
      },
      "source": [
        "As you see, at the moment Cypher generated is not syntactically correct. We might need to do some fine tuning here or pick some other models like Codey."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lrVRKnn8e1Xn",
      "metadata": {
        "id": "lrVRKnn8e1Xn"
      },
      "outputs": [],
      "source": [
        "# To do - move libraries to where they are imported.  Remove unused libraries.\n",
        "\n",
        "#%pip install graphdatascience\n",
        "#%pip install python-dotenv\n",
        "#%pip install retry\n",
        "\n",
        "\n",
        "# To do - move libraries to where they are imported.  Remove unused libraries.\n",
        "\n",
        "#import os\n",
        "#from retry import retry\n",
        "#import re\n",
        "#from string import Template\n",
        "#import json \n",
        "#import ast\n",
        "#import time\n",
        "#import pandas as pd\n",
        "#from graphdatascience import GraphDataScience\n",
        "#import glob\n",
        "#from timeit import default_timer as timer\n",
        "#from dotenv import load_dotenv\n",
        "\n",
        "#from google.cloud import aiplatform\n",
        "#from google.cloud.aiplatform.private_preview.language_models import ChatModel, InputOutputTextPair\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "common-cpu.m108",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
    },
    "kernelspec": {
      "display_name": "Python (Local)",
      "language": "python",
      "name": "local-base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "toc-autonumbering": true,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "toc-showtags": false
  },
  "nbformat": 4,
  "nbformat_minor": 5
}