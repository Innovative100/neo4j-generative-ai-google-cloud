{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3d39ab",
   "metadata": {
    "id": "3d3d39ab"
   },
   "source": [
    "# Ingestion\n",
    "This notebook parses data from XXX (need detail here) using Google Vertex AI Generative AI.  It then uses Generative AI to create Neo4j Cypher queries which write the data to a Neo4j database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a9328-f17d-48ae-b72e-c333fb867eb0",
   "metadata": {
    "id": "f17a9328-f17d-48ae-b72e-c333fb867eb0"
   },
   "source": [
    "## Setup\n",
    "This notebook should be run within Vertex AI Workbench.  Be sure to select \"single user\" when starting a managed notebook to run this.  Otherwise the auth won't allow access to the preview.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb2fda-70c8-4733-ac6f-2678e5cbff47",
   "metadata": {
    "id": "87eb2fda-70c8-4733-ac6f-2678e5cbff47"
   },
   "source": [
    "First we need to install the preview libraries for Generative AI.  It's a new version of the AI platform library.  To access the bucket holding that module, your user account and project will need to be part of the preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43720d2e-05cd-49de-bbb9-15c0b10d768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://vertex_sdk_llm_private_releases/SDK/google_cloud_aiplatform-1.25.dev20230413+language.models-py2.py3-none-any.whl...\n",
      "/ [1 files][  2.4 MiB/  2.4 MiB]                                                \n",
      "Operation completed over 1 objects/2.4 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://vertex_sdk_llm_private_releases/SDK/google_cloud_aiplatform-1.25.dev20230413+language.models-py2.py3-none-any.whl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f0bed25-6b3d-4c6c-91ea-9cd7462cb94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./google_cloud_aiplatform-1.25.dev20230413+language.models-py2.py3-none-any.whl\n",
      "Collecting shapely<2.0.0\n",
      "  Using cached Shapely-1.8.5.post1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Using cached protobuf-4.23.1-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Using cached google_cloud_resource_manager-1.10.1-py2.py3-none-any.whl (321 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "  Using cached proto_plus-1.22.2-py3-none-any.whl (47 kB)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
      "  Using cached google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "Collecting packaging>=14.3\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0\n",
      "  Using cached google_cloud_bigquery-3.10.0-py2.py3-none-any.whl (218 kB)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Using cached google_cloud_storage-2.9.0-py2.py3-none-any.whl (113 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Using cached googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "Collecting google-auth<3.0dev,>=2.14.1\n",
      "  Using cached google_auth-2.18.1-py2.py3-none-any.whl (178 kB)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2\n",
      "  Using cached grpcio-1.54.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Using cached grpcio_status-1.54.2-py3-none-any.whl (5.1 kB)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.6.0\n",
      "  Using cached google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB)\n",
      "Collecting python-dateutil<3.0dev,>=2.7.2\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Using cached grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting six>=1.9.0\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting urllib3<2.0\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Using cached google_crc32c-1.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (171 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Installing collected packages: urllib3, six, shapely, pyasn1, protobuf, packaging, idna, grpcio, google-crc32c, charset-normalizer, certifi, cachetools, rsa, requests, python-dateutil, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, grpcio-status, google-auth, grpc-google-iam-v1, google-api-core, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.16\n",
      "    Uninstalling urllib3-1.26.16:\n",
      "      Successfully uninstalled urllib3-1.26.16\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: shapely\n",
      "    Found existing installation: Shapely 1.8.5.post1\n",
      "    Uninstalling Shapely-1.8.5.post1:\n",
      "      Successfully uninstalled Shapely-1.8.5.post1\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.5.0\n",
      "    Uninstalling pyasn1-0.5.0:\n",
      "      Successfully uninstalled pyasn1-0.5.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.1\n",
      "    Uninstalling protobuf-4.23.1:\n",
      "      Successfully uninstalled protobuf-4.23.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.54.2\n",
      "    Uninstalling grpcio-1.54.2:\n",
      "      Successfully uninstalled grpcio-1.54.2\n",
      "  Attempting uninstall: google-crc32c\n",
      "    Found existing installation: google-crc32c 1.5.0\n",
      "    Uninstalling google-crc32c-1.5.0:\n",
      "      Successfully uninstalled google-crc32c-1.5.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.1.0\n",
      "    Uninstalling charset-normalizer-3.1.0:\n",
      "      Successfully uninstalled charset-normalizer-3.1.0\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.5.7\n",
      "    Uninstalling certifi-2023.5.7:\n",
      "      Successfully uninstalled certifi-2023.5.7\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.3.0\n",
      "    Uninstalling cachetools-5.3.0:\n",
      "      Successfully uninstalled cachetools-5.3.0\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.9\n",
      "    Uninstalling rsa-4.9:\n",
      "      Successfully uninstalled rsa-4.9\n",
      "\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: pyasn1-modules\n",
      "    Found existing installation: pyasn1-modules 0.3.0\n",
      "    Uninstalling pyasn1-modules-0.3.0:\n",
      "      Successfully uninstalled pyasn1-modules-0.3.0\n",
      "  Attempting uninstall: proto-plus\n",
      "    Found existing installation: proto-plus 1.22.2\n",
      "    Uninstalling proto-plus-1.22.2:\n",
      "      Successfully uninstalled proto-plus-1.22.2\n",
      "  Attempting uninstall: googleapis-common-protos\n",
      "    Found existing installation: googleapis-common-protos 1.59.0\n",
      "    Uninstalling googleapis-common-protos-1.59.0:\n",
      "      Successfully uninstalled googleapis-common-protos-1.59.0\n",
      "  Attempting uninstall: google-resumable-media\n",
      "    Found existing installation: google-resumable-media 2.5.0\n",
      "    Uninstalling google-resumable-media-2.5.0:\n",
      "      Successfully uninstalled google-resumable-media-2.5.0\n",
      "  Attempting uninstall: grpcio-status\n",
      "    Found existing installation: grpcio-status 1.54.2\n",
      "    Uninstalling grpcio-status-1.54.2:\n",
      "      Successfully uninstalled grpcio-status-1.54.2\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.18.1\n",
      "    Uninstalling google-auth-2.18.1:\n",
      "      Successfully uninstalled google-auth-2.18.1\n",
      "  Attempting uninstall: grpc-google-iam-v1\n",
      "    Found existing installation: grpc-google-iam-v1 0.12.6\n",
      "    Uninstalling grpc-google-iam-v1-0.12.6:\n",
      "      Successfully uninstalled grpc-google-iam-v1-0.12.6\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 2.11.0\n",
      "    Uninstalling google-api-core-2.11.0:\n",
      "      Successfully uninstalled google-api-core-2.11.0\n",
      "  Attempting uninstall: google-cloud-core\n",
      "    Found existing installation: google-cloud-core 2.3.2\n",
      "    Uninstalling google-cloud-core-2.3.2:\n",
      "      Successfully uninstalled google-cloud-core-2.3.2\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.9.0\n",
      "    Uninstalling google-cloud-storage-2.9.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.9.0\n",
      "  Attempting uninstall: google-cloud-resource-manager\n",
      "    Found existing installation: google-cloud-resource-manager 1.10.1\n",
      "    Uninstalling google-cloud-resource-manager-1.10.1:\n",
      "      Successfully uninstalled google-cloud-resource-manager-1.10.1\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 3.10.0\n",
      "    Uninstalling google-cloud-bigquery-3.10.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-3.10.0\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.25.dev20230413+language.models\n",
      "    Uninstalling google-cloud-aiplatform-1.25.dev20230413+language.models:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.25.dev20230413+language.models\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydata-profiling 4.1.1 requires requests<2.29,>=2.24.0, but you have requests 2.31.0 which is incompatible.\n",
      "tensorboardx 2.6 requires protobuf<4,>=3.8.0, but you have protobuf 4.23.1 which is incompatible.\n",
      "google-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.23.1 which is incompatible.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cachetools-5.3.0 certifi-2023.5.7 charset-normalizer-3.1.0 google-api-core-2.11.0 google-auth-2.18.1 google-cloud-aiplatform-1.25.dev20230413+language.models google-cloud-bigquery-3.10.0 google-cloud-core-2.3.2 google-cloud-resource-manager-1.10.1 google-cloud-storage-2.9.0 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.59.0 grpc-google-iam-v1-0.12.6 grpcio-1.54.2 grpcio-status-1.54.2 idna-3.4 packaging-23.1 proto-plus-1.22.2 protobuf-4.23.1 pyasn1-0.5.0 pyasn1-modules-0.3.0 python-dateutil-2.8.2 requests-2.31.0 rsa-4.9 shapely-1.8.5.post1 six-1.16.0 urllib3-1.26.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user ./google_cloud_aiplatform-1.25.dev20230413+language.models-py2.py3-none-any.whl \"shapely<2.0.0\" --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ad823ee-84ca-4de3-9b29-4b324ac5b9ae",
   "metadata": {
    "id": "4ad823ee-84ca-4de3-9b29-4b324ac5b9ae"
   },
   "outputs": [],
   "source": [
    "# Note, you will need to set your project_id\n",
    "project_id = 'neo4jbusinessdev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42c5a70c-aee1-462b-953a-4c87a524a111",
   "metadata": {
    "id": "42c5a70c-aee1-462b-953a-4c87a524a111"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertexai\n",
    "#import vertexai\n",
    "\n",
    "vertexai.init(project=project_id, location='us-central1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d388e",
   "metadata": {
    "id": "707d388e"
   },
   "source": [
    "## Fine Tune a Model\n",
    "Let us now fine-tune the `text-bison` model to help us extract entities & relationships better. The untuned model makes is good but we tune here to make it better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed392ee-66db-42ee-8c63-d4099b4fc36a",
   "metadata": {
    "id": "2ed392ee-66db-42ee-8c63-d4099b4fc36a"
   },
   "source": [
    "We will use a `jsonl` file in the following format to tune our Ingestion model. Our ingestion model will consume text and extract entities & relationships out of it. Each training example should be JSONL record with two keys, for example:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"input_text\": <input prompt>,\n",
    "  \"output_text\": <associated output>\n",
    "}\n",
    "```\n",
    "\n",
    "Before we get started, we need to upload the input file to Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acde6857-4385-4253-b81d-6aacda16f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "bucket_name = project_id + '-genai'\n",
    "client = storage.Client()\n",
    "\n",
    "try:\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "except:\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    bucket.storage_class = 'STANDARD'\n",
    "    bucket = client.create_bucket(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e918da4-e50a-48ca-a275-dbcc820be8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### What made this file?  We should show the path from dataset onward.\n",
    "filename='finetuning/entity-extraction-trng.jsonl'\n",
    "\n",
    "blob = bucket.blob(filename)\n",
    "blob.upload_from_filename(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb41f9a-1c5f-447d-921d-7514f180cf53",
   "metadata": {},
   "source": [
    "Model tuning will take approximately 35 minutes.  Tuning consumes TPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "xUJ785QJbNZ-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUJ785QJbNZ-",
    "outputId": "2c936197-02f8-47c5-9e2f-c1f99b755a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/tune-large-model-20230525185439?project=803648085855\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/803648085855/locations/europe-west4/pipelineJobs/tune-large-model-20230525185439\n",
      "Tuning has completed. Created Vertex Model: projects/803648085855/locations/us-central1/models/8228474542415151104\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform.private_preview.language_models import TextGenerationModel\n",
    "#from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "#model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison-001\")\n",
    "\n",
    "model.tune_model(\n",
    "  training_data = 'gs://' + bucket_name + '/' + filename,\n",
    "  train_steps = 10,\n",
    "  tuning_job_location = 'europe-west4',\n",
    "  tuned_model_location = 'us-central1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "LXK2Bxm_yLmc",
   "metadata": {
    "id": "LXK2Bxm_yLmc"
   },
   "outputs": [],
   "source": [
    "#model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison-001\")\n",
    "\n",
    "models = model.list_tuned_model_names()\n",
    "\n",
    "#The first model in the list is the one we just tuned.\n",
    "entity_extraction_tuned_model = models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db2e49",
   "metadata": {
    "id": "48db2e49"
   },
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10113a08",
   "metadata": {
    "id": "10113a08"
   },
   "source": [
    "Now, let's define a function that can help clean the input data. The data refers to some figures like scanned images. We don't have them and so we will remove any such references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e430a536",
   "metadata": {
    "id": "e430a536"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "  clean = \"\\n\".join([row for row in text.split(\"\\n\")])\n",
    "  clean = re.sub(r'\\(fig[^)]*\\)', '', clean, flags=re.IGNORECASE)\n",
    "  return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43bb3c",
   "metadata": {
    "id": "4e43bb3c"
   },
   "source": [
    "## Prompt Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72960046",
   "metadata": {
    "id": "72960046"
   },
   "source": [
    "Let's take this case sheet and extract entities and relations using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ad98c",
   "metadata": {
    "id": "605ad98c"
   },
   "source": [
    "This is a helper function to talk to the LLM with our prompt and text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1148d87e",
   "metadata": {
    "id": "1148d87e"
   },
   "outputs": [],
   "source": [
    "def run_text_model(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    prompt: str,\n",
    "    location: str = \"us-central1\",\n",
    "    tuned_model_name: str = \"\",\n",
    "    ) :\n",
    "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "      model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7781a12b",
   "metadata": {
    "id": "7781a12b"
   },
   "source": [
    "This is a simple prompt to start with. If the processing is very complex, you can also chain the prompts as and when required. I am going to use a single prompt here that will extract the text strictly as per the Entities and Relationships defined. This is a simplification. \n",
    "In the real scenario, especially with medical records, you have to leverage on Domain experts to define the Ontology systematically and capture the important information. You should also be mindful of following the relevant regulations around handling health records,\n",
    "\n",
    "Instead of one single large model, you can also consider chaining a number of smaller ones as per your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4f9a9",
   "metadata": {
    "id": "d2b4f9a9"
   },
   "source": [
    "Let's run our completion task with our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcbfd725",
   "metadata": {
    "id": "dcbfd725"
   },
   "outputs": [],
   "source": [
    "def extract_entities_relationships(prompt, tuned_model_name):\n",
    "    try:\n",
    "        #res = run_text_model(project_id, \"text-bison@001\", 0, 1024, 0.8, 40, prompt, location, tuned_model_name)\n",
    "        res = run_text_model(project_id, \"text-bison-001\", 0, 1024, 0.8, 40, prompt, location, tuned_model_name)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7921fa85",
   "metadata": {
    "id": "7921fa85"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"From the Case sheet for a patient below, extract the following Entities & relationships described in the mentioned format \n",
    "1. First, look for these Entity types in the text and generate as comma-separated format similar to entity type.\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. Do not create new entity types that aren't mentioned below. Document must be summarized and stored inside Case entity under `summary` property. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Types:\n",
    "    label:'Case',id:string //Case\n",
    "    label:'Person',id:string,age:string,location:string,gender:string //Patient mentioned in the case\n",
    "    label:'Symptom',id:string,description:string //Symptom Entity; `id` property is the name of the symptom, in lowercase & camel-case & should always start with an alphabet\n",
    "    label:'Disease',id:string,name:string //Disease diagnosed now or previously as per the Case sheet; `id` property is the name of the disease, in lowercase & camel-case & should always start with an alphabet\n",
    "    label:'BodySystem',id:string,name:string //Body Part affected. Eg: Chest, Lungs; id property is the name of the part, in lowercase & camel-case & should always start with an alphabet\n",
    "    label:'Diagnosis',id:string,name:string,description:string,when:string //Diagnostic procedure conducted; `id` property is the summary of the Diagnosis, in lowercase & camel-case & should always start with an alphabet\n",
    "    label:'Biological',id:string,name:string,description:string //Results identified from Diagnosis; `id` property is the summary of the Biological, in lowercase & camel-case & should always start with an alphabet\n",
    "2. Next generate each relationships as triples of head, relationship and tail. To refer the head and tail entity, use their respective `id` property. Relationship property should be mentioned within brackets as comma-separated. They should follow these relationship types below. You will have to generate as many relationships as needed as defined below:\n",
    "    Relationship types:\n",
    "    case|FOR|person\n",
    "    person|HAS_SYMPTOM{when:string,frequency:string,span:string}|symptom //the properties inside HAS_SYMPTOM gets populated from the Case sheet\n",
    "    person|HAS_DISEASE{when:string}|disease //the properties inside HAS_DISEASE gets populated from the Case sheet\n",
    "    symptom|SEEN_ON|chest\n",
    "    disease|AFFECTS|heart\n",
    "    person|HAS_DIAGNOSIS|diagnosis\n",
    "    diagnosis|SHOWED|biological\n",
    "3. Summary & description properties inside entities should be the text summary\n",
    "\n",
    "Output Format (Follow Strictly):\n",
    "{\n",
    "    \"entities\": [{\"label\":\"Case\",\"id\":\"case1\"}, {\"label\":\"Person\",\"id\":\"person1\",\"age\":20,\"gender\":\"female\"}],\n",
    "    \"relationships\": [\"disease|AFFECTS|heart\", \"case1|FOR|person1\", \"person|HAS_DISEASE{when:'2020'}|disease\"]\n",
    "}\n",
    "\n",
    "Question: Now, extract entities & relationships as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6JKeHYgb4op-",
   "metadata": {
    "id": "6JKeHYgb4op-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [{'label': 'Case', 'id': 'case1'},\n",
       "  {'label': 'Person', 'id': 'person1', 'age': '49', 'gender': 'female'},\n",
       "  {'label': 'Disease', 'id': 'disease1', 'name': 'Hepatocellular carcinoma'},\n",
       "  {'label': 'Symptom', 'id': 'symptom1', 'description': 'upper quadrant pain'},\n",
       "  {'label': 'Body', 'id': 'body1', 'name': 'chest'},\n",
       "  {'label': 'Diagnosis',\n",
       "   'id': 'diagnosis1',\n",
       "   'name': 'TACE',\n",
       "   'description': 'transarterial chemoembolization',\n",
       "   'when': '2014-10-04'},\n",
       "  {'label': 'Biological',\n",
       "   'id': 'biological1',\n",
       "   'name': 'oil-like materials',\n",
       "   'description': 'iodinated oil-like materials'},\n",
       "  {'label': 'Symptom', 'id': 'symptom2', 'description': 'weight loss'}],\n",
       " 'relationships': ['case1|FOR|person1',\n",
       "  \"person1|HAS_SYMP_TOM{when: '2014-10-04', frequency: '1', span: '0'}|symptom1\",\n",
       "  \"person1|HAS_DIS_EASE{when: '2014-10-04'}|disease1\",\n",
       "  'disease1|AFFECTS|body1',\n",
       "  'case1|HAS_DIAG_NOSIS|diagnosis1',\n",
       "  'diagnosis1|SHOW_ED|biological1',\n",
       "  'symptom1|SEEN_ON|body1',\n",
       "  'symptom2|SEEN_ON|body1']}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que = \"\"\"A 49-year-old woman was admitted to the Department of Radiology of the Second Affiliated Hospital of Zhejiang University in October 2004 with right upper quadrant pain and weight loss.\n",
    "She was a hepatitis B virus carrier.\n",
    "Her α-fetoprotein level was 1185.3 ng/mL.\n",
    "Ultrasonography and computed tomography (CT) revealed a 10-cm mass in the posterior segments of the right liver lobe.\n",
    "A 1.5-cm mass was also found in the left lateral segment.\n",
    "These clinical signs indicated that the patient had inoperable HCC and Child-Pugh class A cirrhosis.\n",
    "TACE was offered to the patient.\n",
    "Angiogram demonstrated no obvious hepatic arterio-venous shunt, but multiple smaller masses in both lobes of the liver.\n",
    "An emulsion of oxaliplatin, pirarubicin, hydroxycamptothecin and lipiodol were prepared, 35 mL and 3 mL of the mixture were administered intra-arterially to the right and left hepatic artery, respectively.\n",
    "The patient experienced right upper quadrant pain after TACE and had an uneventful recovery.\n",
    "One month later, a second TACE procedure was performed via the right hepatic artery and 40 mL of the mixture was administered.\n",
    "On the next day, she experienced sudden acute dyspnoea and the peripheral oxygen saturation decreased to 90%.\n",
    "The chest X-ray showed some increased reticular shadows in the left lung, especially in the lower zones, and a chest CT scan revealed multiple iodized oil-like high-density materials in parenchyma of the lung (Figure ​1).\n",
    "After 10 mg dexamethasone i.v. and other supportive therapies were administered, the respiratory symptom was attenuated.\n",
    "Two days later, the patient suffered from a serious headache and transient consciousness loss, accompanying nausea and vomiting followed by confusion, lower extremity weakness.\n",
    "Non-contrast enhanced CT scanning showed multiple disseminated hyper-intense lesions in the brain, consistent with deposition of iodized oil (Figure ​2).\n",
    "One week later, her respiratory and neurologic symptoms disappeared completely, and she was discharged.The patient also consequently completed the other three TACE procedures, during which no similar symptoms occurred.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from string import Template\n",
    "_prompt = Template(prompt).substitute(ctext=clean_text(que))\n",
    "\n",
    "import json\n",
    "result = extract_entities_relationships(_prompt, entity_extraction_tuned_model)\n",
    "result = result.split('Answer:\\n ')[1]\n",
    "result = json.loads(result.replace(\"\\'\", \"'\"))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb2a60",
   "metadata": {
    "id": "8feb2a60"
   },
   "source": [
    "## Neo4j Cypher Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96efc5",
   "metadata": {
    "id": "0b96efc5"
   },
   "source": [
    "The entities and relationships we got from the LLM have to be transformed to Cypher so we can write them into Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "084047d0",
   "metadata": {
    "id": "084047d0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_prop_str(prop_dict, _id):\n",
    "    s = []\n",
    "    for key, val in prop_dict.items():\n",
    "      if key != 'label' and key != 'id':\n",
    "         s.append(_id+\".\"+key+' = \"'+str(val).replace('\\\"', '\"').replace('\"', '\\\"')+'\"') \n",
    "    return ' ON CREATE SET ' + ','.join(s)\n",
    "\n",
    "def get_cypher_compliant_var(_id):\n",
    "    return \"_\"+ re.sub(r'[\\W_]', '', _id)\n",
    "\n",
    "def generate_cypher(in_json):\n",
    "    e_map = {}\n",
    "    e_stmt = []\n",
    "    r_stmt = []\n",
    "    e_stmt_tpl = Template(\"($id:$label{id:'$key'})\")\n",
    "    r_stmt_tpl = Template(\"\"\"\n",
    "      MATCH $src\n",
    "      MATCH $tgt\n",
    "      MERGE ($src_id)-[:$rel]->($tgt_id)\n",
    "    \"\"\")\n",
    "    for obj in in_json:\n",
    "      for j in obj['entities']:\n",
    "          props = ''\n",
    "          label = j['label']\n",
    "          id = j['id']\n",
    "          if label == 'Case':\n",
    "                id = 'c'+str(time.time_ns())\n",
    "          elif label == 'Person':\n",
    "                id = 'p'+str(time.time_ns())\n",
    "          varname = get_cypher_compliant_var(j['id'])\n",
    "          stmt = e_stmt_tpl.substitute(id=varname, label=label, key=id)\n",
    "          e_map[varname] = stmt\n",
    "          e_stmt.append('MERGE '+ stmt + get_prop_str(j, varname))\n",
    "\n",
    "      for st in obj['relationships']:\n",
    "          rels = st.split(\"|\")\n",
    "          src_id = get_cypher_compliant_var(rels[0].strip())\n",
    "          rel = rels[1].strip()\n",
    "          tgt_id = get_cypher_compliant_var(rels[2].strip())\n",
    "          stmt = r_stmt_tpl.substitute(\n",
    "              src_id=src_id, tgt_id=tgt_id, src=e_map[src_id], tgt=e_map[tgt_id], rel=rel)\n",
    "          \n",
    "          r_stmt.append(stmt)\n",
    "\n",
    "    return e_stmt, r_stmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec143b14",
   "metadata": {
    "id": "ec143b14",
    "outputId": "6d2c5379-3ed9-4be4-ff94-433c21fd1275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"MERGE (_case1:Case{id:'c1685044473216314108'}) ON CREATE SET \", 'MERGE (_person1:Person{id:\\'p1685044473216548268\\'}) ON CREATE SET _person1.age = \"49\",_person1.gender = \"female\"', 'MERGE (_disease1:Disease{id:\\'disease1\\'}) ON CREATE SET _disease1.name = \"Hepatocellular carcinoma\"', 'MERGE (_symptom1:Symptom{id:\\'symptom1\\'}) ON CREATE SET _symptom1.description = \"upper quadrant pain\"', 'MERGE (_body1:Body{id:\\'body1\\'}) ON CREATE SET _body1.name = \"chest\"', 'MERGE (_diagnosis1:Diagnosis{id:\\'diagnosis1\\'}) ON CREATE SET _diagnosis1.name = \"TACE\",_diagnosis1.description = \"transarterial chemoembolization\",_diagnosis1.when = \"2014-10-04\"', 'MERGE (_biological1:Biological{id:\\'biological1\\'}) ON CREATE SET _biological1.name = \"oil-like materials\",_biological1.description = \"iodinated oil-like materials\"', 'MERGE (_symptom2:Symptom{id:\\'symptom2\\'}) ON CREATE SET _symptom2.description = \"weight loss\"'] [\"\\n      MATCH (_case1:Case{id:'c1685044473216314108'})\\n      MATCH (_person1:Person{id:'p1685044473216548268'})\\n      MERGE (_case1)-[:FOR]->(_person1)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685044473216548268'})\\n      MATCH (_symptom1:Symptom{id:'symptom1'})\\n      MERGE (_person1)-[:HAS_SYMP_TOM{when: '2014-10-04', frequency: '1', span: '0'}]->(_symptom1)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685044473216548268'})\\n      MATCH (_disease1:Disease{id:'disease1'})\\n      MERGE (_person1)-[:HAS_DIS_EASE{when: '2014-10-04'}]->(_disease1)\\n    \", \"\\n      MATCH (_disease1:Disease{id:'disease1'})\\n      MATCH (_body1:Body{id:'body1'})\\n      MERGE (_disease1)-[:AFFECTS]->(_body1)\\n    \", \"\\n      MATCH (_case1:Case{id:'c1685044473216314108'})\\n      MATCH (_diagnosis1:Diagnosis{id:'diagnosis1'})\\n      MERGE (_case1)-[:HAS_DIAG_NOSIS]->(_diagnosis1)\\n    \", \"\\n      MATCH (_diagnosis1:Diagnosis{id:'diagnosis1'})\\n      MATCH (_biological1:Biological{id:'biological1'})\\n      MERGE (_diagnosis1)-[:SHOW_ED]->(_biological1)\\n    \", \"\\n      MATCH (_symptom1:Symptom{id:'symptom1'})\\n      MATCH (_body1:Body{id:'body1'})\\n      MERGE (_symptom1)-[:SEEN_ON]->(_body1)\\n    \", \"\\n      MATCH (_symptom2:Symptom{id:'symptom2'})\\n      MATCH (_body1:Body{id:'body1'})\\n      MERGE (_symptom2)-[:SEEN_ON]->(_body1)\\n    \"]\n"
     ]
    }
   ],
   "source": [
    "ent_cyp, rel_cyp = generate_cypher([result])\n",
    "\n",
    "print(ent_cyp, rel_cyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c69170",
   "metadata": {
    "id": "54c69170",
    "tags": []
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f06013-a653-43cf-be7c-de2888e621f7",
   "metadata": {
    "id": "00f06013-a653-43cf-be7c-de2888e621f7"
   },
   "source": [
    "You will need a Neo4j AuraDS Pro instance.  You can deploy that on Google Cloud Marketplace [here](https://console.cloud.google.com/marketplace/product/endpoints/prod.n4gcp.neo4j.io).\n",
    "\n",
    "With that complete, you'll need to install the Neo4j library and set up your database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a44d36df-21e4-4bad-969e-23ed11762518",
   "metadata": {
    "id": "a44d36df-21e4-4bad-969e-23ed11762518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphdatascience in /home/jupyter/.local/lib/python3.7/site-packages (1.6)\n",
      "Requirement already satisfied: pyarrow<11.0,>=4.0 in /home/jupyter/.local/lib/python3.7/site-packages (from graphdatascience) (10.0.1)\n",
      "Requirement already satisfied: neo4j<6.0,>=4.4.2 in /home/jupyter/.local/lib/python3.7/site-packages (from graphdatascience) (5.8.1)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.0 in /opt/conda/lib/python3.7/site-packages (from graphdatascience) (4.64.1)\n",
      "Requirement already satisfied: pandas<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from graphdatascience) (1.3.5)\n",
      "Requirement already satisfied: multimethod<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from graphdatascience) (1.9.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from neo4j<6.0,>=4.4.2->graphdatascience) (2023.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas<2.0,>=1.0->graphdatascience) (1.21.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/jupyter/.local/lib/python3.7/site-packages (from pandas<2.0,>=1.0->graphdatascience) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jupyter/.local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas<2.0,>=1.0->graphdatascience) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user graphdatascience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e621c199-533a-4503-baef-200c5adcd8ad",
   "metadata": {
    "id": "e621c199-533a-4503-baef-200c5adcd8ad"
   },
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ecea5ff",
   "metadata": {
    "id": "0ecea5ff"
   },
   "outputs": [],
   "source": [
    "# You will need to change these variables\n",
    "connectionUrl = 'neo4j+s://7c44f42f.databases.neo4j.io'\n",
    "username = 'neo4j'\n",
    "password = 'GRvlpOO4-Ozl8iHaV_20ZQqwOaUljgIpnWyKJRmt2Fc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ddbfa6e8",
   "metadata": {
    "id": "ddbfa6e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.6+19'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds = GraphDataScience(connectionUrl, auth=(username, password))\n",
    "gds.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228a3a58",
   "metadata": {
    "id": "228a3a58"
   },
   "source": [
    "Before loading the data, create constraints as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "66756bab",
   "metadata": {
    "id": "66756bab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher('CREATE CONSTRAINT unique_case_id IF NOT EXISTS FOR (n:Case) REQUIRE n.id IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_person_id IF NOT EXISTS FOR (n:Person) REQUIRE (n.id) IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_symptom_id IF NOT EXISTS FOR (n:Symptom) REQUIRE (n.id) IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_disease_id IF NOT EXISTS FOR (n:Disease) REQUIRE n.id IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_bodysys_id IF NOT EXISTS FOR (n:BodySystem) REQUIRE n.id IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_diag_id IF NOT EXISTS FOR (n:Diagnosis) REQUIRE n.id IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_biological_id IF NOT EXISTS FOR (n:Biological) REQUIRE n.id IS UNIQUE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971bf0b3",
   "metadata": {
    "id": "971bf0b3"
   },
   "source": [
    "Ingest the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7367ece7",
   "metadata": {
    "id": "7367ece7"
   },
   "outputs": [
    {
     "ename": "CypherSyntaxError",
     "evalue": "{code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input '': expected\n  \"ALL\"\n  \"ANY\"\n  \"CASE\"\n  \"COLLECT\"\n  \"COUNT\"\n  \"EXISTS\"\n  \"INF\"\n  \"INFINITY\"\n  \"NAN\"\n  \"NONE\"\n  \"REDUCE\"\n  \"SINGLE\"\n  \"allShortestPaths\"\n  \"false\"\n  \"null\"\n  \"shortestPath\"\n  \"true\"\n  an identifier (line 1, column 62 (offset: 61))\n\"MERGE (_case1:Case{id:'c1685044473216314108'}) ON CREATE SET\"\n                                                              ^}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCypherSyntaxError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/graphdatascience/graph_data_science.py\u001b[0m in \u001b[0;36mrun_cypher\u001b[0;34m(self, query, params, database)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mqr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfallback_query_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mqr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdriver_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/graphdatascience/query_runner/neo4j_query_runner.py\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(self, query, params, database)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 )\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Though pandas support may be experimental in the `neo4j` package, it should always\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/neo4j/_sync/work/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, query, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpersonated_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_access_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mbookmarks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotifications_min_severity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotifications_disabled_categories\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         )\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/neo4j/_sync/work/result.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_categories)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/neo4j/_sync/work/result.py\u001b[0m in \u001b[0;36m_attach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exhausted\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/neo4j/_sync/io/_common.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNeo4jError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mServiceUnavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSessionExpired\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miscoroutinefunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__on_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/neo4j/_sync/io/_bolt.py\u001b[0m in \u001b[0;36mfetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0mhydration_hooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhydration_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         )\n\u001b[0;32m--> 803\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midle_since\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/neo4j/_sync/io/_bolt5.py\u001b[0m in \u001b[0;36m_process_message\u001b[0;34m(self, tag, fields)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_server_state_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAILED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_metadata\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mServiceUnavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatabaseUnavailable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/neo4j/_sync/io/_common.py\u001b[0m in \u001b[0;36mon_failure\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mhandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_summary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNeo4jError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhydrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_ignored\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCypherSyntaxError\u001b[0m: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input '': expected\n  \"ALL\"\n  \"ANY\"\n  \"CASE\"\n  \"COLLECT\"\n  \"COUNT\"\n  \"EXISTS\"\n  \"INF\"\n  \"INFINITY\"\n  \"NAN\"\n  \"NONE\"\n  \"REDUCE\"\n  \"SINGLE\"\n  \"allShortestPaths\"\n  \"false\"\n  \"null\"\n  \"shortestPath\"\n  \"true\"\n  an identifier (line 1, column 62 (offset: 61))\n\"MERGE (_case1:Case{id:'c1685044473216314108'}) ON CREATE SET\"\n                                                              ^}"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for e in ent_cyp:\n",
    "    gds.run_cypher(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f811933",
   "metadata": {
    "id": "0f811933"
   },
   "source": [
    "Ingest relationships now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff4ad1",
   "metadata": {
    "id": "d9ff4ad1"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for r in rel_cyp:\n",
    "    gds.run_cypher(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65581a1",
   "metadata": {
    "id": "c65581a1"
   },
   "source": [
    "This is a helper function to ingest all case sheets inside the `data/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707904c",
   "metadata": {
    "id": "b707904c"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def run_pipeline(count=191):\n",
    "    txt_files = glob.glob(\"data/case_sheets/*.txt\")[0:count]\n",
    "    print(f\"Running pipeline for {len(txt_files)} files\")\n",
    "    failed_files = process_pipeline(txt_files)\n",
    "    print(failed_files)\n",
    "    return failed_files\n",
    "\n",
    "def process_pipeline(files):\n",
    "    failed_files = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            with open(f, 'r') as file:\n",
    "                print(f\"  {f}: Reading File...\")\n",
    "                data = file.read().rstrip()\n",
    "                text = clean_text(data)\n",
    "                print(f\"    {f}: Extracting E & R\")\n",
    "                results = extract_entities_relationships(f, text)\n",
    "                print(f\"    {f}: Generating Cypher\")\n",
    "                ent_cyp, rel_cyp = generate_cypher(results)\n",
    "                print(f\"    {f}: Ingesting Entities\")\n",
    "                for e in ent_cyp:\n",
    "                    gds.run_cypher(e)\n",
    "                print(f\"    {f}: Ingesting Relationships\")\n",
    "                for r in rel_cyp:\n",
    "                    gds.run_cypher(r)\n",
    "                print(f\"    {f}: Processing DONE\")\n",
    "        except Exception as e:\n",
    "            print(f\"    {f}: Processing Failed with exception {e}\")\n",
    "            failed_files.append(f)\n",
    "    return failed_files\n",
    "            \n",
    "def extract_entities_relationships(f, text):\n",
    "    start = timer()\n",
    "    system = \"You are a helpful Medical Case Sheet expert who extracts relevant information and store them on a Neo4j Knowledge Graph\"\n",
    "    prompts = [prompt1]\n",
    "    all_cypher = \"\"\n",
    "    results = []\n",
    "    for p in prompts:\n",
    "      p = Template(p).substitute(ctext=text)\n",
    "      res = process_gpt(system, p)\n",
    "      results.append(json.loads(res))\n",
    "    end = timer()\n",
    "    elapsed = (end-start)\n",
    "    print(f\"    {f}: E & R took {elapsed}secs\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b86a7",
   "metadata": {
    "id": "bb4b86a7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "failed_files = run_pipeline(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e9c48",
   "metadata": {
    "id": "653e9c48"
   },
   "source": [
    "If processing failed for some files due to API Rate limit or some other error, you can retry as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26a851",
   "metadata": {
    "id": "4e26a851"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "failed_files = process_pipeline(failed_files)\n",
    "failed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77de32",
   "metadata": {
    "id": "4d77de32"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UrvbzyY_X8uP",
   "metadata": {
    "id": "UrvbzyY_X8uP"
   },
   "source": [
    "## Cypher Generation for Consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c06e57d-72a9-4b86-8b8d-119690895c02",
   "metadata": {},
   "source": [
    "### Tune the model to generate Cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316e464-5dbf-475d-8507-de07ba0bc0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = 'gs://' + bucket_name + '/' + filename\n",
    "train_steps = 10\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "\n",
    "model.tune_model(\n",
    "  training_data=training_data,\n",
    "  train_steps=train_steps,\n",
    "  tuning_job_location=\"europe-west4\",\n",
    "  tuned_model_location=\"us-central1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113928b-b83d-4f41-833b-a92e851cca3a",
   "metadata": {},
   "source": [
    "### Generate Cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f2895-c3c4-401c-a4d6-cf1b80a2050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_to_cypher(prompt, tuned_model_name=''):\n",
    "    try:\n",
    "        res = run_text_model(project_id, \"text-bison@001\", 0, 1024, 0.8, 40, prompt, location, tuned_model_name)\n",
    "        # res = json.loads(res.replace(\"\\'\", \"'\"))\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EMZysiC9YsqC",
   "metadata": {
    "id": "EMZysiC9YsqC"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Context:\n",
    "You are an expert Neo4j Cypher translator who understands the question in english and convert it to Cypher based on the Neo4j Schema provided.\n",
    "Here are the instructions to follow:\n",
    "1. Use the Neo4j schema to generate cypher compatible ONLY for Neo4j Version 5\n",
    "2. Do not use EXISTS, SIZE keywords in the cypher.\n",
    "3. Use only Nodes and relationships mentioned in the schema while generating the response\n",
    "4. Reply ONLY in Cypher when it makes sense.\n",
    "5. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Heart Disease use `toLower(d.name) contains 'heart disease'`\n",
    "6. Patient node is synonymous to Person.\n",
    "Now, use this Neo4j schema and Reply ONLY in Cypher when it makes sense.\n",
    "Schema:\n",
    "Nodes:\n",
    "    label:'Case',id:string,summary:string //Case Node\n",
    "    label:'Person',id:string,age:string,location:string,gender:string //Patient Node\n",
    "    label:'Symptom',id:string,description:string //Symptom Node\n",
    "    label:'Disease',id:string,name:string //Disease Node\n",
    "    label:'BodySystem',id:string,name:string //Node for Body Part affected Eg: Heart, lungs\n",
    "    label:'Diagnosis',id:string,name:string,description:string,when:string //Diagnostic Node\n",
    "    label:'Biological',id:string,name:string,description:string //Node for Results identified from Diagnosis\n",
    "Relationships:\n",
    "    (:Case)-[:FOR]->(Person)\n",
    "    (:Person)-[:HAS_SYMPTOM{when:string,frequency:string,span:string}]->(Symptom)\n",
    "    (:Person)-[:HAS_DISEASE{when:string}]->(:Disease)\n",
    "    (:Symptom)-[:SEEN_ON]->(:BodySystem)\n",
    "    (:Disease)-[:AFFECTS]->(:BodySystem)\n",
    "    (:Person)-[:HAS_DIAGNOSIS]->(:Diagnosis)\n",
    "    (:Diagnosis)-[:SHOWED]->(:Biological)\n",
    "\n",
    "So, for this question: 'How many teenagers have cough related ailments?', you will answer : MATCH (p:Person)-[:HAS_SYMPTOM]->(s:Symptom) WHERE toInteger(p.age) > 12 AND toInteger(p.age) < 20 AND toLower(s.description) CONTAINS 'cough' RETURN COUNT(p) \n",
    "Because:\n",
    "1. As per schema definition of nodes & relationships above, Person node is related to Symptom node via HAS_SYMPTOM relationship.\n",
    "2. Person's `age` property is a string. So we need to convert to Integer using toInteger before comparing.\n",
    "3. Teenagers are aged 13 to 19. So we add age filter.\n",
    "4. Symptoms are stored in `description` property as per schema. And we are doing a case-insensitive match to get the cough symptom\n",
    "5. Finally, we return the number of patients who match the input criteria using COUNT function\n",
    "\n",
    "\n",
    "Ouput Format (Strict): //Only code as output. No other text\n",
    "MATCH (d:Disease) RETURN d.name as disease, SIZE([(d)-[]-(p:Person) | p]) AS affected_patients ORDER BY affected_patients DESC LIMIT 1\n",
    "\n",
    "Question:\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "que = 'How many teenagers have cough related ailments?'\n",
    "_prompt = Template(prompt).substitute(ctext=clean_text(que))\n",
    "\n",
    "response = english_to_cypher(_prompt, 'projects/803648085855/locations/us-central1/models/2255294061638320128')\n",
    "cypher = response.split('Answer:\\n ')[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad060d0-e38a-4b0e-996a-c7307d225d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
