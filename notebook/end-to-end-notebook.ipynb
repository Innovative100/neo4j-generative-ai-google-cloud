{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3d39ab",
   "metadata": {
    "id": "3d3d39ab"
   },
   "source": [
    "# 1. Intelligent App with Google GenAI & Neo4j\n",
    "In this notebook, let's explore how to effectively leverage Google GenAI to build and consume knowledge on Neo4j.\n",
    "\n",
    "This notebook parses data from a public [corpus of Resumes / Curriculum Vitae](https://github.com/florex/resume_corpus) using Google Vertex AI Generative AI's `text-bison` model. The model will be prompted to recognise ad extract Entities & Relationships. We will then generate Neo4j Cypher queries using them and write the data to a Neo4j database.\n",
    "We will again use a `text-bison` model and prompt it to convert questions in english to Cypher - Neo4j's query language, which can be used for data retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a9328-f17d-48ae-b72e-c333fb867eb0",
   "metadata": {
    "id": "f17a9328-f17d-48ae-b72e-c333fb867eb0"
   },
   "source": [
    "## Setup\n",
    "This notebook should be run within Vertex AI Workbench.  Be sure to select \"single user\" when starting a managed notebook to run this.  Otherwise the auth won't allow access to the preview.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb2fda-70c8-4733-ac6f-2678e5cbff47",
   "metadata": {
    "id": "87eb2fda-70c8-4733-ac6f-2678e5cbff47"
   },
   "source": [
    "First we need to install the latest libraries for Generative AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43720d2e-05cd-49de-bbb9-15c0b10d768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user google-cloud-aiplatform --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f91036-7018-465f-b4af-8d5523e7ed3a",
   "metadata": {},
   "source": [
    "You will need to restart the kernel after the pip install completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad823ee-84ca-4de3-9b29-4b324ac5b9ae",
   "metadata": {
    "id": "4ad823ee-84ca-4de3-9b29-4b324ac5b9ae"
   },
   "outputs": [],
   "source": [
    "# Note, you will need to set your project_id\n",
    "project_id = 'neo4jbusinessdev'\n",
    "location = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c5a70c-aee1-462b-953a-4c87a524a111",
   "metadata": {
    "id": "42c5a70c-aee1-462b-953a-4c87a524a111"
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=project_id, location='us-central1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43bb3c",
   "metadata": {
    "id": "4e43bb3c"
   },
   "source": [
    "## Prompt Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72960046",
   "metadata": {
    "id": "72960046"
   },
   "source": [
    "In the upcoming sections, we will extract knowledge adhering to the following schema. This is a very Simplified schema to denote a Resume. Normally, you will have Domain Experts who come up with an ideal Ontology.\n",
    "\n",
    "![schema](images/schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7781a12b",
   "metadata": {
    "id": "7781a12b"
   },
   "source": [
    "To achieve our Extraction goal as per the schema, I am going to chain a series of prompts, each focused on only one task - to extract a specific entity. By this way, you can avoid Token limitations. Also, the quality of extraction will be good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "7921fa85",
   "metadata": {
    "id": "7921fa85"
   },
   "outputs": [],
   "source": [
    "person_prompt_tpl=\"\"\"From the Curriculum Vitae text for a job aspirant below, extract Entities strictly as instructed below\n",
    "1. First, look for this Entity type in the text and generate as comma-separated format similar to entity type.\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. NEVER create new entity types that aren't mentioned below. Document must be summarized and stored inside Person entity under `description` property\n",
    "    Entity Types:\n",
    "    label:'Person',id:string,role:string,description:string //Person Node\n",
    "2. Description property should be a crisp text summary and MUST NOT be more than 100 characters\n",
    "3. If you cannot find any information on the entities & relationships above, it is okay to return empty value. DO NOT create fictious data\n",
    "4. Do NOT create duplicate entities\n",
    "5. Restrict yourself to extract only Person information. No Position, Company, Education or Skill information should be focussed.\n",
    "Example Output JSON:\n",
    "{\"entities\": [{\"label\":\"Person\",\"id\":\"person1\",\"role\":\"Prompt Developer\",\"description\":\"Prompt Developer with more than 30 years of LLM experience\"}]}\n",
    "\n",
    "Question: Now, extract the Person for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "d903cb98-ce84-43bb-b860-71a394604a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "postion_prompt_tpl=\"\"\"From the Curriculum Vitae text for a job aspirant below, extract Entities & relationships strictly as instructed below\n",
    "1. First, look for these Entity types in the text and generate as comma-separated format.\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. NEVER create new entity types that aren't mentioned below. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Types:\n",
    "    label:'Position',id:string,title:string,location:string,startDate:string,endDate:string,url:string //Position Node\n",
    "    label:'Company',id:string,name:string //Company Node\n",
    "2. Next generate each relationships as triples of head, relationship and tail. To refer the head and tail entity, use their respective `id` property. NEVER create new Relationship types that aren't mentioned below:\n",
    "    Relationship definition:\n",
    "    position|AT_COMPANY|company //Ensure this is a string in the generated output\n",
    "3. If you cannot find any information on the entities & relationships above, it is okay to return empty value. DO NOT create fictious data\n",
    "4. Do NOT create duplicate entities. \n",
    "5. Restrict yourself to extract only Position and Company information. No Education or Skill information should be focussed.\n",
    " Example Output JSON:\n",
    "{\"entities\": [{\"label\":\"Position\",\"id\":\"position1\",\"title\":\"Software Engineer\",\"location\":\"Singapore\",startDate:\"2021-01-01\",endDate:\"present\"},{\"label\":\"Position\",\"id\":\"position2\",\"title\":\"Senior Software Engineer\",\"location\":\"Mars\",startDate:\"2020-01-01\",endDate:\"2020-12-31\"},{label:\"Company\",id:\"company1\",name:\"Neo4j Singapore Pte Ltd\"},{\"label\":\"Company\",\"id\":\"company2\",\"name\":\"Neo4j Mars Inc\"}],\"relationships\": [\"position1|AT_COMPANY|company1\",\"position2|AT_COMPANY|company2\"]}\n",
    "\n",
    "Question: Now, extract entities & relationships as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ffdf7091-20ff-4537-9aa8-b96a9325b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_prompt_tpl=\"\"\"From the Curriculum Vitae text below, extract Entities strictly as instructed below\n",
    "1. Look for prominent Skill Entities in the text. The`id` property of each entity must be alphanumeric and must be unique among the entities. NEVER create new entity types that aren't mentioned below:\n",
    "    Entity Definition:\n",
    "    label:'Skill',id:string,name:string,level:string //Skill Node\n",
    "Example Output Format:\n",
    "{\"entities\": [{\"label\":\"Skill\",\"id\":\"skill1\",\"name\":\"Neo4j\",\"level\":\"expert\"},{\"label\":\"Skill\",\"id\":\"skill2\",\"name\":\"Pytorch\",\"level\":\"expert\"}]}\n",
    "\n",
    "Question: Now, extract entities as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "e188d63a-1a13-4363-8917-1af3123cbda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_prompt_tpl=\"\"\"From the Curriculum Vitae text for a job aspirant below, extract Entities strictly as instructed below\n",
    "1. Look for this Education entity type and generate as comma-separated format similar to entity type.\n",
    "   `id` property of each entity must be alphanumeric and must be unique among the entities. You will be referring this property to define the relationship between entities. NEVER create other entity types that aren't mentioned below. You will have to generate as many entities as needed as per the types below:\n",
    "    Entity Definition:\n",
    "    label:'Education',id:string,degree:string,university:string,graduation_date:string,score:string,url:string //Education Node\n",
    "2. If you cannot find any information on the entities above, it is okay to return empty value. DO NOT create fictious data\n",
    "3. Do NOT create duplicate entities or properties\n",
    "4. Strictly extract only Education. No Skill or other Entity should be extracted\n",
    "Output JSON (Strict):\n",
    "{\"entities\": [{\"label\":\"Education\",\"id\":\"education1\",\"degree\":\"Bachelor of Science\",\"graduationDate\":\"May 2022\",\"score\":\"5.0\"}]}\n",
    "\n",
    "Question: Now, extract entities as mentioned above for the text below -\n",
    "$ctext\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ad98c",
   "metadata": {
    "id": "605ad98c"
   },
   "source": [
    "This is a helper function to talk to the LLM with our prompt and text input. We will use the `text-bison` base model. In your usecase, you might need to finetune it. VertexAI provides an elegant way to finetune it. The weights will be staying within your tenant and the base model is frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1148d87e",
   "metadata": {
    "id": "1148d87e"
   },
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "def run_text_model(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    prompt: str,\n",
    "    location: str = \"us-central1\",\n",
    "    tuned_model_name: str = \"\",\n",
    "    ) :\n",
    "    \"\"\"Text Completion Use a Large Language Model.\"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "      model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbfd725",
   "metadata": {
    "id": "dcbfd725"
   },
   "outputs": [],
   "source": [
    "def extract_entities_relationships(prompt, tuned_model_name):\n",
    "    try:\n",
    "        res = run_text_model(project_id, \"text-bison@001\", 0, 1024, 0.8, 40, prompt, location, tuned_model_name)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf22363-093f-4dd3-b882-7e10f0d5e6e4",
   "metadata": {},
   "source": [
    "Now, let's run our extraction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "6JKeHYgb4op-",
   "metadata": {
    "id": "6JKeHYgb4op-"
   },
   "outputs": [],
   "source": [
    "sample_que = \"\"\"Contractor Contractor Contractor Jacksonville, FL Work Experience Contractor Criterion Systems - Naval Air Station Key West, FL April 2015 to November 2018 Provide support for Command Naval Region South East, Naval Air Station Key West† ï Worked in conjunction with other contract personnel as well as government civilian and military personnel in the accomplishment of the tasks associated with the contract† ï Communicated with the IT Director if the time lines for specific tasks could not be met and provided detailed information on the impediments, risks associated with not meeting timelines and a mitigation strategy to complete the milestones as soon as possible† ï Documented all tasks accomplished in a monthly status report† ï Provided Navy Marine Corp Intranet, NMCI, support in the form of guidance, education and assistance to ensure that the end users at Naval Region Southeast and its subordinate installations/detachments receive the NMCI systems and applications required to perform mission deliverables† ï Provided the Activity Contract Technical Representatives (ACTRs) and NMCI managers training, guidance, and support services for all NMCI functions to include Defense Messaging System (DMS), IT Administration, and customer support† ï DMS support included providing operational support, maintenance, and distribution of official US Navy messages via DMS, the message traffic included both unclassified and classified messages† ï IT Administration and customer support included providing support for work requests and resolving all IT related issues† ï Provided Tier 1 hardware, software and network connectivity support† ï Identified, researched, and resolved all technical problems† ï Responded to all telephone calls, email and personnel requests for all technical support† ï Documented, tracked and monitored problems to ensure a timely resolution† ï Assisted end users with email delivery analysis and remediation† ï Performed routine maintenance for both desktop and networked printers to include exchanging parts, toners, drums, and maintenance kits† ï Administered Active Directory groups and distribution lists† ï Administered share drive folders and permissions† ï Provided technical support to include establishing and troubleshooting Video Teleconference Calls, VTC, as needed† ï Provided complete classroom support to instructors conducting training that included loading NMCI approved Commercial Off the Shelf, COTS, and Government Off the Shelf, GOTS, software, troubleshoot software, and tested for compatibility with Windows 10 computers† ï Submitted and tracked via Remedy Ticketing Systems all base wide issues that were reported† ï Assisted with the Blackberry activation and configuration of Blackberry Enterprise Server email accounts† ï Updated Global Address Lists in Exchange via Active Directory† ï Provided 1st tier support and escalated as deemed necessary to include hardware, software, and connectivity support to both military and civilian personnel System Administrator DMI Mobile Solutions May 2013 to March 2015 Provide support for Command Naval Region South East, Naval Air Station Key West† ï Worked in conjunction with other contract personnel as well as government civilian and military personnel in the accomplishment of the tasks associated with the contract† ï Communicated with the IT Director if the time lines for specific tasks could not be met and provided detailed information on the impediments, risks associated with not meeting timelines and a mitigation strategy to complete the milestones as soon as possible† ï Documented all tasks accomplished in a monthly status report† ï Provided Navy Marine Corp Intranet, NMCI, support in the form of guidance, education and assistance to ensure that the end users at Naval Region Southeast and its subordinate installations/detachments receive the NMCI systems and applications required to perform mission deliverables† ï Provided the Activity Contract Technical Representatives (ACTRs) and NMCI managers training, guidance, and support services for all NMCI functions to include Defense Messaging System (DMS), IT Administration, and customer support† ï DMS support included providing operational support, maintenance, and distribution of official US Navy messages via DMS, the message traffic included both unclassified and classified messages† ï IT Administration and customer support included providing support for work requests and resolving all IT related issues† ï Provided Tier 1 hardware, software and network connectivity support† ï Identified, researched, and resolved all technical problems† ï Responded to all telephone calls, email and personnel requests for all technical support† ï Documented, tracked and monitored problems to ensure a timely resolution† ï Assisted end users with email delivery analysis and remediation† ï Performed routine maintenance for both desktop and networked printers to include exchanging parts, toners, drums, and maintenance kits† ï Administered Active Directory groups and distribution lists† ï Administered share drive folders and permissions† ï Provided technical support to include establishing and troubleshooting Video Teleconference Calls, VTC, as needed† ï Provided complete classroom support to instructors conducting training that included loading NMCI approved Commercial Off the Shelf, COTS, and Government Off the Shelf, GOTS, software, troubleshoot software, and tested for compatibility with Windows 10 computers† ï Submitted and tracked via Remedy Ticketing Systems all base wide issues that were reported† ï Assisted with the Blackberry activation and configuration of Blackberry Enterprise Server email accounts† ï Updated Global Address Lists in Exchange via Active Directory† ï Provided 1st tier support and escalated as deemed necessary to include hardware, software, and connectivity support to both military and civilian personnel Systems Administrator SAIC November 2008 to May 2013 Provide support for Command Naval Region South East, Naval Air Station Key West† ï Worked in conjunction with other contract personnel as well as government civilian and military personnel in the accomplishment of the tasks associated with the contract† ï Communicated with the IT Director if the time lines for specific tasks could not be met and provided detailed information on the impediments, risks associated with not meeting timelines and a mitigation strategy to complete the milestones as soon as possible† ï Documented all tasks accomplished in a monthly status report† ï Provided Navy Marine Corp Intranet, NMCI, support in the form of guidance, education and assistance to ensure that the end users at Naval Region Southeast and its subordinate installations/detachments receive the NMCI systems and applications required to perform mission deliverables† ï Provided the Activity Contract Technical Representatives (ACTRs) and NMCI managers training, guidance, and support services for all NMCI functions to include Defense Messaging System (DMS), IT Administration, and customer support† ï DMS support included providing operational support, maintenance, and distribution of official US Navy messages via DMS, the message traffic included both unclassified and classified messages† ï IT Administration and customer support included providing support for work requests and resolving all IT related issues† ï Provided Tier 1 hardware, software and network connectivity support† ï Identified, researched, and resolved all technical problems† ï Responded to all telephone calls, email and personnel requests for all technical support† ï Documented, tracked and monitored problems to ensure a timely resolution† ï Assisted end users with email delivery analysis and remediation† ï Performed routine maintenance for both desktop and networked printers to include exchanging parts, toners, drums, and maintenance kits† ï Administered Active Directory groups and distribution lists† ï Administered share drive folders and permissions† ï Provided technical support to include establishing and troubleshooting Video Teleconference Calls, VTC, as needed† ï Provided complete classroom support to instructors conducting training that included loading NMCI approved Commercial Off the Shelf, COTS, and Government Off the Shelf, GOTS, software, troubleshoot software, and tested for compatibility with Windows 10 computers† ï Submitted and tracked via Remedy Ticketing Systems all base wide issues that were reported† ï Assisted with the Blackberry activation and configuration of Blackberry Enterprise Server email accounts† ï Updated Global Address Lists in Exchange via Active Directory† ï Provided 1st tier support and escalated as deemed necessary to include hardware, software, and connectivity support to both military and civilian personnel Network Administrator Keys Federal Credit Union September 1998 to September 2008 ï Administered and maintained network that included updating Cisco Routers/Switches IOS† ï Installed/troubleshoot both desktop and network printers to include all preventive maintenance tasks† ï Created/administered user and exchange email accounts† ï Ensured security of the network by regulating and monitoring access to share drive files, password administration, and backed up all files via Veritas Backup to ensure the integrity in the event of network outages, natural disasters† ï Documented network configurations and prepared backup strategies and procedures† ï Identified and documented network problems, possible causes, and ramifications† ï Continually assessed the current systems to make sure it met the needs of Keys Federal Credit Union† ï Monitored servers, IIS, Exchange, SQL, Print servers, via Paessler PRTG Enterprise Console† ï Developed, implemented, and tested off site systems to provide business continuity in case of on-site emergencies such as power failures, hurricanes, fires, and floods† ï Provided all aspects of troubleshooting and fielded questions encountered by staff members in regards to all applications and software† ï Trained all staff on both software and hardware usage† ï Stayed abreast of all recent developments, new technologies and products, and emerging communication strategies and methods† ï Continually ensured that all skills are up to date through education and by reading computer related literature† ï Administered Hyland Onbase SQL Enterprise Server which housed all imaging of credit union legal documents to include configuration of the ODBC client which provided connectivity to the Onbase server† ï Provided recommendations on all purchased computer and network systems Education Weslaco High School - Weslaco, TX 1979 to 1982\"\"\"\n",
    "from string import Template\n",
    "prompts = [person_prompt_tpl, postion_prompt_tpl, skill_prompt_tpl, edu_prompt_tpl]\n",
    "import json\n",
    "results = {\"entities\": [], \"relationships\": []}\n",
    "for p in prompts:\n",
    "    _prompt = Template(p).substitute(ctext=sample_que)\n",
    "    _extraction = extract_entities_relationships(_prompt, '')\n",
    "    if 'Answer:\\n' in _extraction:\n",
    "        _extraction = _extraction.split('Answer:\\n ')[1]\n",
    "    if _extraction.strip() == '':\n",
    "        continue\n",
    "    try:\n",
    "        _extraction = json.loads(_extraction.replace(\"\\'\", \"'\").replace('`', ''))\n",
    "    except json.JSONDecodeError:\n",
    "        print(_extraction)\n",
    "        #Temp hack to ignore Skills cut off by token limitation\n",
    "        _extraction = _extraction[:_extraction.rfind(\"}\")+1] + ']}'\n",
    "        _extraction = json.loads(_extraction.replace(\"\\'\", \"'\"))\n",
    "    results[\"entities\"].extend(_extraction[\"entities\"])\n",
    "    if \"relationships\" in _extraction:\n",
    "        results[\"relationships\"].extend(_extraction[\"relationships\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "df4b37bf-f355-4571-95e1-f307a65483b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_id = results[\"entities\"][0][\"id\"]\n",
    "for e in results[\"entities\"][1:]:\n",
    "    if e['label'] == 'Position':\n",
    "        results[\"relationships\"].append(f\"{person_id}|HAS_POSITION|{e['id']}\")\n",
    "    if e['label'] == 'Skill':\n",
    "        results[\"relationships\"].append(f\"{person_id}|HAS_SKILL|{e['id']}\")\n",
    "    if e['label'] == 'Education':\n",
    "        results[\"relationships\"].append(f\"{person_id}|HAS_EDUCATION|{e['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720e425-17fe-41b1-999a-076bbf7a7470",
   "metadata": {},
   "source": [
    "The extracted entities & relationships will look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4a07624b-a1bb-493e-820a-144841b8a6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [{'label': 'Person',\n",
       "   'id': 'person1',\n",
       "   'role': 'Systems Administrator',\n",
       "   'description': 'Systems Administrator with over 20 years of experience in the IT industry'},\n",
       "  {'label': 'Position',\n",
       "   'id': 'position1',\n",
       "   'title': 'Systems Administrator',\n",
       "   'location': 'Key West, FL',\n",
       "   'startDate': 'September 1998',\n",
       "   'endDate': 'September 2008',\n",
       "   'url': ''},\n",
       "  {'label': 'Position',\n",
       "   'id': 'position2',\n",
       "   'title': 'Systems Administrator',\n",
       "   'location': 'Key West, FL',\n",
       "   'startDate': 'November 2008',\n",
       "   'endDate': 'May 2013',\n",
       "   'url': ''},\n",
       "  {'label': 'Position',\n",
       "   'id': 'position3',\n",
       "   'title': 'Systems Administrator',\n",
       "   'location': 'Key West, FL',\n",
       "   'startDate': 'April 2015',\n",
       "   'endDate': 'November 2018',\n",
       "   'url': ''},\n",
       "  {'label': 'Company', 'id': 'company1', 'name': 'Keys Federal Credit Union'},\n",
       "  {'label': 'Company', 'id': 'company2', 'name': 'SAIC'},\n",
       "  {'label': 'Company', 'id': 'company3', 'name': 'DMI Mobile Solutions'},\n",
       "  {'label': 'Company', 'id': 'company4', 'name': 'Criterion Systems'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill1',\n",
       "   'name': 'Active Directory',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill2', 'name': 'Cisco', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill3', 'name': 'Cisco IOS', 'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill4',\n",
       "   'name': 'Cisco Switch',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill5',\n",
       "   'name': 'Cisco Router',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill6',\n",
       "   'name': 'Cisco Systems',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill7', 'name': 'COTS', 'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill8',\n",
       "   'name': 'Defense Messaging System',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill9', 'name': 'GOTS', 'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill10',\n",
       "   'name': 'Hyland Onbase',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill11',\n",
       "   'name': 'IT Administration',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill12',\n",
       "   'name': 'Microsoft Exchange',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill13',\n",
       "   'name': 'Microsoft Windows',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill14',\n",
       "   'name': 'Network Administration',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill15',\n",
       "   'name': 'Network Security',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill16',\n",
       "   'name': 'Network Troubleshooting',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill17', 'name': 'NMCI', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill18', 'name': 'SQL Server', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill19', 'name': 'Verizon', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill20', 'name': 'Windows 10', 'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill21',\n",
       "   'name': 'Windows Server',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill22',\n",
       "   'name': 'Active Directory',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill23', 'name': 'Cisco', 'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill24', 'name': 'Cisco IOS', 'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill25',\n",
       "   'name': 'Cisco Switch',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill26',\n",
       "   'name': 'Cisco Router',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill27',\n",
       "   'name': 'Cisco Systems',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill28', 'name': 'COTS', 'level': 'expert'},\n",
       "  {'label': 'Skill',\n",
       "   'id': 'skill29',\n",
       "   'name': 'Defense Messaging System',\n",
       "   'level': 'expert'},\n",
       "  {'label': 'Skill', 'id': 'skill30', 'name': 'GOTS', 'level': 'expert'},\n",
       "  {'label': 'Education',\n",
       "   'id': 'education1',\n",
       "   'degree': 'Associate of Arts',\n",
       "   'university': 'Weslaco High School',\n",
       "   'graduationDate': '1982',\n",
       "   'score': '3.0'}],\n",
       " 'relationships': ['position1|AT_COMPANY|company1',\n",
       "  'position2|AT_COMPANY|company2',\n",
       "  'position3|AT_COMPANY|company3',\n",
       "  'position3|AT_COMPANY|company4',\n",
       "  'person1|HAS_POSITION|position1',\n",
       "  'person1|HAS_POSITION|position2',\n",
       "  'person1|HAS_POSITION|position3',\n",
       "  'person1|HAS_SKILL|skill1',\n",
       "  'person1|HAS_SKILL|skill2',\n",
       "  'person1|HAS_SKILL|skill3',\n",
       "  'person1|HAS_SKILL|skill4',\n",
       "  'person1|HAS_SKILL|skill5',\n",
       "  'person1|HAS_SKILL|skill6',\n",
       "  'person1|HAS_SKILL|skill7',\n",
       "  'person1|HAS_SKILL|skill8',\n",
       "  'person1|HAS_SKILL|skill9',\n",
       "  'person1|HAS_SKILL|skill10',\n",
       "  'person1|HAS_SKILL|skill11',\n",
       "  'person1|HAS_SKILL|skill12',\n",
       "  'person1|HAS_SKILL|skill13',\n",
       "  'person1|HAS_SKILL|skill14',\n",
       "  'person1|HAS_SKILL|skill15',\n",
       "  'person1|HAS_SKILL|skill16',\n",
       "  'person1|HAS_SKILL|skill17',\n",
       "  'person1|HAS_SKILL|skill18',\n",
       "  'person1|HAS_SKILL|skill19',\n",
       "  'person1|HAS_SKILL|skill20',\n",
       "  'person1|HAS_SKILL|skill21',\n",
       "  'person1|HAS_SKILL|skill22',\n",
       "  'person1|HAS_SKILL|skill23',\n",
       "  'person1|HAS_SKILL|skill24',\n",
       "  'person1|HAS_SKILL|skill25',\n",
       "  'person1|HAS_SKILL|skill26',\n",
       "  'person1|HAS_SKILL|skill27',\n",
       "  'person1|HAS_SKILL|skill28',\n",
       "  'person1|HAS_SKILL|skill29',\n",
       "  'person1|HAS_SKILL|skill30',\n",
       "  'person1|HAS_EDUCATION|education1']}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb2a60",
   "metadata": {
    "id": "8feb2a60"
   },
   "source": [
    "## Data Ingestion Cypher Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96efc5",
   "metadata": {
    "id": "0b96efc5"
   },
   "source": [
    "The entities and relationships we got from the LLM have to be transformed to Cypher so we can write them into Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "084047d0",
   "metadata": {
    "id": "084047d0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_prop_str(prop_dict, _id):\n",
    "    s = []\n",
    "    for key, val in prop_dict.items():\n",
    "      if key != 'label' and key != 'id':\n",
    "         s.append(_id+\".\"+key+' = \"'+str(val).replace('\\\"', '\"').replace('\"', '\\\"')+'\"') \n",
    "    return ' ON CREATE SET ' + ','.join(s)\n",
    "\n",
    "def get_cypher_compliant_var(_id):\n",
    "    s = \"_\"+ re.sub(r'[\\W_]', '', _id).lower() #avoid numbers appearing as firstchar; replace spaces\n",
    "    return s[:20] #restrict variable size\n",
    "\n",
    "def generate_cypher(in_json):\n",
    "    e_map = {}\n",
    "    e_stmt = []\n",
    "    r_stmt = []\n",
    "    e_stmt_tpl = Template(\"($id:$label{id:'$key'})\")\n",
    "    r_stmt_tpl = Template(\"\"\"\n",
    "      MATCH $src\n",
    "      MATCH $tgt\n",
    "      MERGE ($src_id)-[:$rel]->($tgt_id)\n",
    "    \"\"\")\n",
    "    for obj in in_json:\n",
    "      for j in obj['entities']:\n",
    "          props = ''\n",
    "          label = j['label']\n",
    "          id = ''\n",
    "          if label == 'Person':\n",
    "            id = 'p'+str(time.time_ns())\n",
    "          elif label == 'Position':\n",
    "            id = 'j'+str(time.time_ns())\n",
    "          elif label == 'Education':\n",
    "            id = 'e'+str(time.time_ns())\n",
    "          else:\n",
    "                id = get_cypher_compliant_var(j['name'])\n",
    "          if label in ['Person', 'Position', 'Education', 'Skill', 'Company']:\n",
    "            varname = get_cypher_compliant_var(j['id'])\n",
    "            stmt = e_stmt_tpl.substitute(id=varname, label=label, key=id)\n",
    "            e_map[varname] = stmt\n",
    "            e_stmt.append('MERGE '+ stmt + get_prop_str(j, varname))\n",
    "\n",
    "      for st in obj['relationships']:\n",
    "          rels = st.split(\"|\")\n",
    "          src_id = get_cypher_compliant_var(rels[0].strip())\n",
    "          rel = rels[1].strip()\n",
    "          if rel in ['HAS_SKILL', 'HAS_EDUCATION', 'AT_COMPANY', 'HAS_POSITION']: #we ignore other relationships\n",
    "            tgt_id = get_cypher_compliant_var(rels[2].strip())\n",
    "            stmt = r_stmt_tpl.substitute(\n",
    "              src_id=src_id, tgt_id=tgt_id, src=e_map[src_id], tgt=e_map[tgt_id], rel=rel)\n",
    "            r_stmt.append(stmt)\n",
    "\n",
    "    return e_stmt, r_stmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ec143b14",
   "metadata": {
    "id": "ec143b14",
    "outputId": "6d2c5379-3ed9-4be4-ff94-433c21fd1275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MERGE (_person1:Person{id:\\'p1685102194065206925\\'}) ON CREATE SET _person1.role = \"Systems Administrator\",_person1.description = \"Systems Administrator with over 20 years of experience in the IT industry\"', 'MERGE (_position1:Position{id:\\'j1685102194065241742\\'}) ON CREATE SET _position1.title = \"Systems Administrator\",_position1.location = \"Key West, FL\",_position1.startDate = \"September 1998\",_position1.endDate = \"September 2008\",_position1.url = \"\"', 'MERGE (_position2:Position{id:\\'j1685102194065257867\\'}) ON CREATE SET _position2.title = \"Systems Administrator\",_position2.location = \"Key West, FL\",_position2.startDate = \"November 2008\",_position2.endDate = \"May 2013\",_position2.url = \"\"', 'MERGE (_position3:Position{id:\\'j1685102194065270601\\'}) ON CREATE SET _position3.title = \"Systems Administrator\",_position3.location = \"Key West, FL\",_position3.startDate = \"April 2015\",_position3.endDate = \"November 2018\",_position3.url = \"\"', 'MERGE (_company1:Company{id:\\'_keysfederalcreditun\\'}) ON CREATE SET _company1.name = \"Keys Federal Credit Union\"', 'MERGE (_company2:Company{id:\\'_saic\\'}) ON CREATE SET _company2.name = \"SAIC\"', 'MERGE (_company3:Company{id:\\'_dmimobilesolutions\\'}) ON CREATE SET _company3.name = \"DMI Mobile Solutions\"', 'MERGE (_company4:Company{id:\\'_criterionsystems\\'}) ON CREATE SET _company4.name = \"Criterion Systems\"', 'MERGE (_skill1:Skill{id:\\'_activedirectory\\'}) ON CREATE SET _skill1.name = \"Active Directory\",_skill1.level = \"expert\"', 'MERGE (_skill2:Skill{id:\\'_cisco\\'}) ON CREATE SET _skill2.name = \"Cisco\",_skill2.level = \"expert\"', 'MERGE (_skill3:Skill{id:\\'_ciscoios\\'}) ON CREATE SET _skill3.name = \"Cisco IOS\",_skill3.level = \"expert\"', 'MERGE (_skill4:Skill{id:\\'_ciscoswitch\\'}) ON CREATE SET _skill4.name = \"Cisco Switch\",_skill4.level = \"expert\"', 'MERGE (_skill5:Skill{id:\\'_ciscorouter\\'}) ON CREATE SET _skill5.name = \"Cisco Router\",_skill5.level = \"expert\"', 'MERGE (_skill6:Skill{id:\\'_ciscosystems\\'}) ON CREATE SET _skill6.name = \"Cisco Systems\",_skill6.level = \"expert\"', 'MERGE (_skill7:Skill{id:\\'_cots\\'}) ON CREATE SET _skill7.name = \"COTS\",_skill7.level = \"expert\"', 'MERGE (_skill8:Skill{id:\\'_defensemessagingsys\\'}) ON CREATE SET _skill8.name = \"Defense Messaging System\",_skill8.level = \"expert\"', 'MERGE (_skill9:Skill{id:\\'_gots\\'}) ON CREATE SET _skill9.name = \"GOTS\",_skill9.level = \"expert\"', 'MERGE (_skill10:Skill{id:\\'_hylandonbase\\'}) ON CREATE SET _skill10.name = \"Hyland Onbase\",_skill10.level = \"expert\"', 'MERGE (_skill11:Skill{id:\\'_itadministration\\'}) ON CREATE SET _skill11.name = \"IT Administration\",_skill11.level = \"expert\"', 'MERGE (_skill12:Skill{id:\\'_microsoftexchange\\'}) ON CREATE SET _skill12.name = \"Microsoft Exchange\",_skill12.level = \"expert\"', 'MERGE (_skill13:Skill{id:\\'_microsoftwindows\\'}) ON CREATE SET _skill13.name = \"Microsoft Windows\",_skill13.level = \"expert\"', 'MERGE (_skill14:Skill{id:\\'_networkadministrati\\'}) ON CREATE SET _skill14.name = \"Network Administration\",_skill14.level = \"expert\"', 'MERGE (_skill15:Skill{id:\\'_networksecurity\\'}) ON CREATE SET _skill15.name = \"Network Security\",_skill15.level = \"expert\"', 'MERGE (_skill16:Skill{id:\\'_networktroubleshoot\\'}) ON CREATE SET _skill16.name = \"Network Troubleshooting\",_skill16.level = \"expert\"', 'MERGE (_skill17:Skill{id:\\'_nmci\\'}) ON CREATE SET _skill17.name = \"NMCI\",_skill17.level = \"expert\"', 'MERGE (_skill18:Skill{id:\\'_sqlserver\\'}) ON CREATE SET _skill18.name = \"SQL Server\",_skill18.level = \"expert\"', 'MERGE (_skill19:Skill{id:\\'_verizon\\'}) ON CREATE SET _skill19.name = \"Verizon\",_skill19.level = \"expert\"', 'MERGE (_skill20:Skill{id:\\'_windows10\\'}) ON CREATE SET _skill20.name = \"Windows 10\",_skill20.level = \"expert\"', 'MERGE (_skill21:Skill{id:\\'_windowsserver\\'}) ON CREATE SET _skill21.name = \"Windows Server\",_skill21.level = \"expert\"', 'MERGE (_skill22:Skill{id:\\'_activedirectory\\'}) ON CREATE SET _skill22.name = \"Active Directory\",_skill22.level = \"expert\"', 'MERGE (_skill23:Skill{id:\\'_cisco\\'}) ON CREATE SET _skill23.name = \"Cisco\",_skill23.level = \"expert\"', 'MERGE (_skill24:Skill{id:\\'_ciscoios\\'}) ON CREATE SET _skill24.name = \"Cisco IOS\",_skill24.level = \"expert\"', 'MERGE (_skill25:Skill{id:\\'_ciscoswitch\\'}) ON CREATE SET _skill25.name = \"Cisco Switch\",_skill25.level = \"expert\"', 'MERGE (_skill26:Skill{id:\\'_ciscorouter\\'}) ON CREATE SET _skill26.name = \"Cisco Router\",_skill26.level = \"expert\"', 'MERGE (_skill27:Skill{id:\\'_ciscosystems\\'}) ON CREATE SET _skill27.name = \"Cisco Systems\",_skill27.level = \"expert\"', 'MERGE (_skill28:Skill{id:\\'_cots\\'}) ON CREATE SET _skill28.name = \"COTS\",_skill28.level = \"expert\"', 'MERGE (_skill29:Skill{id:\\'_defensemessagingsys\\'}) ON CREATE SET _skill29.name = \"Defense Messaging System\",_skill29.level = \"expert\"', 'MERGE (_skill30:Skill{id:\\'_gots\\'}) ON CREATE SET _skill30.name = \"GOTS\",_skill30.level = \"expert\"', 'MERGE (_education1:Education{id:\\'e1685102194065591246\\'}) ON CREATE SET _education1.degree = \"Associate of Arts\",_education1.university = \"Weslaco High School\",_education1.graduationDate = \"1982\",_education1.score = \"3.0\"'] [\"\\n      MATCH (_position1:Position{id:'j1685102194065241742'})\\n      MATCH (_company1:Company{id:'_keysfederalcreditun'})\\n      MERGE (_position1)-[:AT_COMPANY]->(_company1)\\n    \", \"\\n      MATCH (_position2:Position{id:'j1685102194065257867'})\\n      MATCH (_company2:Company{id:'_saic'})\\n      MERGE (_position2)-[:AT_COMPANY]->(_company2)\\n    \", \"\\n      MATCH (_position3:Position{id:'j1685102194065270601'})\\n      MATCH (_company3:Company{id:'_dmimobilesolutions'})\\n      MERGE (_position3)-[:AT_COMPANY]->(_company3)\\n    \", \"\\n      MATCH (_position3:Position{id:'j1685102194065270601'})\\n      MATCH (_company4:Company{id:'_criterionsystems'})\\n      MERGE (_position3)-[:AT_COMPANY]->(_company4)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_position1:Position{id:'j1685102194065241742'})\\n      MERGE (_person1)-[:HAS_POSITION]->(_position1)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_position2:Position{id:'j1685102194065257867'})\\n      MERGE (_person1)-[:HAS_POSITION]->(_position2)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_position3:Position{id:'j1685102194065270601'})\\n      MERGE (_person1)-[:HAS_POSITION]->(_position3)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill1:Skill{id:'_activedirectory'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill1)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill2:Skill{id:'_cisco'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill2)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill3:Skill{id:'_ciscoios'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill3)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill4:Skill{id:'_ciscoswitch'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill4)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill5:Skill{id:'_ciscorouter'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill5)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill6:Skill{id:'_ciscosystems'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill6)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill7:Skill{id:'_cots'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill7)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill8:Skill{id:'_defensemessagingsys'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill8)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill9:Skill{id:'_gots'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill9)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill10:Skill{id:'_hylandonbase'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill10)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill11:Skill{id:'_itadministration'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill11)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill12:Skill{id:'_microsoftexchange'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill12)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill13:Skill{id:'_microsoftwindows'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill13)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill14:Skill{id:'_networkadministrati'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill14)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill15:Skill{id:'_networksecurity'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill15)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill16:Skill{id:'_networktroubleshoot'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill16)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill17:Skill{id:'_nmci'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill17)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill18:Skill{id:'_sqlserver'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill18)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill19:Skill{id:'_verizon'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill19)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill20:Skill{id:'_windows10'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill20)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill21:Skill{id:'_windowsserver'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill21)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill22:Skill{id:'_activedirectory'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill22)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill23:Skill{id:'_cisco'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill23)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill24:Skill{id:'_ciscoios'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill24)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill25:Skill{id:'_ciscoswitch'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill25)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill26:Skill{id:'_ciscorouter'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill26)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill27:Skill{id:'_ciscosystems'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill27)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill28:Skill{id:'_cots'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill28)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill29:Skill{id:'_defensemessagingsys'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill29)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_skill30:Skill{id:'_gots'})\\n      MERGE (_person1)-[:HAS_SKILL]->(_skill30)\\n    \", \"\\n      MATCH (_person1:Person{id:'p1685102194065206925'})\\n      MATCH (_education1:Education{id:'e1685102194065591246'})\\n      MERGE (_person1)-[:HAS_EDUCATION]->(_education1)\\n    \"]\n"
     ]
    }
   ],
   "source": [
    "ent_cyp, rel_cyp = generate_cypher([results])\n",
    "\n",
    "print(ent_cyp, rel_cyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c69170",
   "metadata": {
    "id": "54c69170",
    "tags": []
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f06013-a653-43cf-be7c-de2888e621f7",
   "metadata": {
    "id": "00f06013-a653-43cf-be7c-de2888e621f7"
   },
   "source": [
    "You will need a Neo4j AuraDS Pro instance.  You can deploy that on Google Cloud Marketplace [here](https://console.cloud.google.com/marketplace/product/endpoints/prod.n4gcp.neo4j.io).\n",
    "\n",
    "With that complete, you'll need to install the Neo4j library and set up your database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d36df-21e4-4bad-969e-23ed11762518",
   "metadata": {
    "id": "a44d36df-21e4-4bad-969e-23ed11762518"
   },
   "outputs": [],
   "source": [
    "%pip install --user graphdatascience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e621c199-533a-4503-baef-200c5adcd8ad",
   "metadata": {
    "id": "e621c199-533a-4503-baef-200c5adcd8ad"
   },
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ecea5ff",
   "metadata": {
    "id": "0ecea5ff"
   },
   "outputs": [],
   "source": [
    "# You will need to change these variables\n",
    "connectionUrl = 'neo4j+s://7929d24d.databases.neo4j.io'\n",
    "username = 'neo4j'\n",
    "password = 'MhfEDkNieOS79LCTR6KbGNyblfmfjeAmaBbdx-70wVg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddbfa6e8",
   "metadata": {
    "id": "ddbfa6e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.6+19'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds = GraphDataScience(connectionUrl, auth=(username, password))\n",
    "gds.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228a3a58",
   "metadata": {
    "id": "228a3a58"
   },
   "source": [
    "Before loading the data, create constraints as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66756bab",
   "metadata": {
    "id": "66756bab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher('CREATE CONSTRAINT unique_person_id IF NOT EXISTS FOR (n:Person) REQUIRE (n.id) IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_position_id IF NOT EXISTS FOR (n:Position) REQUIRE (n.id) IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_skill_id IF NOT EXISTS FOR (n:Skill) REQUIRE n.id IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_education_id IF NOT EXISTS FOR (n:Education) REQUIRE n.id IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_company_id IF NOT EXISTS FOR (n:Company) REQUIRE n.id IS UNIQUE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971bf0b3",
   "metadata": {
    "id": "971bf0b3"
   },
   "source": [
    "Ingest the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367ece7",
   "metadata": {
    "id": "7367ece7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 161 ms, sys: 15.4 ms, total: 176 ms\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for e in ent_cyp:\n",
    "    gds.run_cypher(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f811933",
   "metadata": {
    "id": "0f811933"
   },
   "source": [
    "Ingest relationships now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d9ff4ad1",
   "metadata": {
    "id": "d9ff4ad1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 128 ms, sys: 15.6 ms, total: 144 ms\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for r in rel_cyp:\n",
    "    gds.run_cypher(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f5076-188c-49d4-9df9-914540d45618",
   "metadata": {},
   "source": [
    "Your ingested data from the above commands might look like this:\n",
    "![ingested data](images/ingested_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65581a1",
   "metadata": {
    "id": "c65581a1"
   },
   "source": [
    "We got thousands of Resumes in the `data` directory. Let us run a pipeline to ingest only a few of them now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "b707904c",
   "metadata": {
    "id": "b707904c"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def run_pipeline(start=0, count=1):\n",
    "    txt_files = glob.glob(\"data/*.txt\")[start:count]\n",
    "    print(f\"Running pipeline for {len(txt_files)} files\")\n",
    "    failed_files = process_pipeline(txt_files)\n",
    "    print(failed_files)\n",
    "    return failed_files\n",
    "\n",
    "def process_pipeline(files):\n",
    "    failed_files = []\n",
    "    i = 0\n",
    "    for f in files:\n",
    "        i += 1\n",
    "        try:\n",
    "            with open(f, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                print(f\"  {f}: Reading File No. ({i})\")\n",
    "                data = file.read().rstrip()\n",
    "                text = data\n",
    "                print(f\"    {f}: Extracting Entities & Relationships\")\n",
    "                results = run_extraction(f, text)\n",
    "                print(f\"    {f}: Generating Cypher\")\n",
    "                ent_cyp, rel_cyp = generate_cypher(results)\n",
    "                print(f\"    {f}: Ingesting Entities\")\n",
    "                for e in ent_cyp:\n",
    "                    gds.run_cypher(e)\n",
    "                print(f\"    {f}: Ingesting Relationships\")\n",
    "                for r in rel_cyp:\n",
    "                    gds.run_cypher(r)\n",
    "                print(f\"    {f}: Processing DONE\")\n",
    "        except Exception as e:\n",
    "            print(f\"    {f}: Processing Failed with exception {e}\")\n",
    "            failed_files.append(f)\n",
    "    return failed_files\n",
    "        \n",
    "from timeit import default_timer as timer\n",
    "def run_extraction(f, text):\n",
    "    start = timer()\n",
    "    prompts = [person_prompt_tpl, postion_prompt_tpl, skill_prompt_tpl, edu_prompt_tpl]\n",
    "    results = {\"entities\": [], \"relationships\": []}\n",
    "    for p in prompts:\n",
    "        _prompt = Template(p).substitute(ctext=text)\n",
    "        _extraction = extract_entities_relationships(_prompt, '')\n",
    "        if 'Answer:\\n' in _extraction:\n",
    "            _extraction = _extraction.split('Answer:\\n ')[1]\n",
    "        if _extraction.strip() == '':\n",
    "            continue\n",
    "        try:\n",
    "            _extraction = json.loads(_extraction.replace(\"\\'\", \"'\"))\n",
    "        except json.JSONDecodeError:\n",
    "            #Temp hack to ignore Skills cut off by token limitation\n",
    "            _extraction = _extraction[:_extraction.rfind(\"}\")+1] + ']}'\n",
    "            _extraction = json.loads(_extraction.replace(\"\\'\", \"'\"))\n",
    "        results[\"entities\"].extend(_extraction[\"entities\"])\n",
    "        if \"relationships\" in _extraction:\n",
    "            results[\"relationships\"].extend(_extraction[\"relationships\"])\n",
    "    person_id = results[\"entities\"][0][\"id\"]\n",
    "    for e in results[\"entities\"][1:]:\n",
    "        if e['label'] == 'Position':\n",
    "            results[\"relationships\"].append(f\"{person_id}|HAS_POSITION|{e['id']}\")\n",
    "        if e['label'] == 'Skill':\n",
    "            results[\"relationships\"].append(f\"{person_id}|HAS_SKILL|{e['id']}\")\n",
    "        if e['label'] == 'Education':\n",
    "            results[\"relationships\"].append(f\"{person_id}|HAS_EDUCATION|{e['id']}\")\n",
    "    end = timer()\n",
    "    elapsed = (end-start)\n",
    "    print(f\"    {f}: Entity Extraction took {elapsed}secs\")\n",
    "    return [results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2feb86-2938-4842-9e7b-f2ba45bada1c",
   "metadata": {},
   "source": [
    "Lets run the pipeline only for the first 5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b86a7",
   "metadata": {
    "id": "bb4b86a7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "failed_files = run_pipeline(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e9c48",
   "metadata": {
    "id": "653e9c48"
   },
   "source": [
    "If processing failed for some files due to API Rate limit, you can retry as below. For token limitation error, it is better to chunk the text and retry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26a851",
   "metadata": {
    "id": "4e26a851"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "failed_files = process_pipeline(failed_files)\n",
    "failed_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UrvbzyY_X8uP",
   "metadata": {
    "id": "UrvbzyY_X8uP"
   },
   "source": [
    "## Cypher Generation for Consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c06e57d-72a9-4b86-8b8d-119690895c02",
   "metadata": {},
   "source": [
    "### Tune the model to generate Cypher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe379dfb-023f-4664-82e8-7fc8e016ba5d",
   "metadata": {},
   "source": [
    "The `text-bison` base model, at the time of this writing needs some finetuning to generate Syntactically Correct Cypher Statements. So, let us fine-tune this model to generate Cypher. In this section, we will tune the model only using 30 Cypher statements. With this limited tuning, the model achieves some Cypher generation capability but it is not State of The Art. In Production scenario, you need to aim for more training data. The total training time takes more than an hour. It also involves TPU resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785e716-2ba1-4d9f-b949-296c9285a1bc",
   "metadata": {},
   "source": [
    "First, let us upload our training set in `jsonl` format to a GCS bucket. We will use this file `finetuning/eng-to-cypher-trng.jsonl` for our fine-tuning. You can take a look over the data there.\n",
    "\n",
    "VertexAI expects you to adhere to this format for each line of the `jsonl` file. \n",
    "```json\n",
    "{\"input_text\": \"MY_INPUT_PROMPT\", \"output_text\": \"CYPHER_QUERY\"} \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2ede9-643b-4adc-a30f-0f26fb61fd47",
   "metadata": {},
   "source": [
    "When you got some changes in the training data, ensure that you upload the updated file in a different name than your previous tuning exercises. Because VertexAI caches data uploaded previously, it tends to skip any file validation and resorts to the previously uploaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "6316e464-5dbf-475d-8507-de07ba0bc0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "bucket_name = project_id + '-genai'\n",
    "client = storage.Client()\n",
    "try:\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "except:\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    bucket.storage_class = 'STANDARD'\n",
    "    bucket = client.create_bucket(bucket)\n",
    "\n",
    "upload_name = f\"finetuning/eng-to-cypher-trng-{time}.jsonl\" #this ensures vertexai reloads the file\n",
    "filename = 'finetuning/eng-to-cypher-trng.jsonl'\n",
    "blob = bucket.blob(upload_name)\n",
    "blob.upload_from_filename(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8cd32-e509-4ec1-bd74-ba6ee9142c3f",
   "metadata": {},
   "source": [
    "Let's tune the model for a hundred training steps. When you the below code, the following sequence happens:\n",
    "1. Pipeline Validation\n",
    "2. Dataset Export\n",
    "3. Prompt Validation\n",
    "4. jsonl to tfrecord conversion\n",
    "5. Parameter Composition for Adapter tuning\n",
    "6. LLM Tuning\n",
    "7. Model uploading and\n",
    "8. Endpoint deployment\n",
    "\n",
    "\n",
    "![finetuning-process](images/finetune-seq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52790af0-83c8-4373-8716-91fcea4b87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = 'gs://' + bucket_name + '/' + upload_name\n",
    "train_steps = 100\n",
    "\n",
    "vertexai.init(project=project_id, location=location)\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "\n",
    "model.tune_model(\n",
    "  training_data=training_data,\n",
    "  train_steps=train_steps,\n",
    "  tuning_job_location=\"europe-west4\",\n",
    "  tuned_model_location=\"us-central1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b313858-08be-4e62-bd49-c120afac0b44",
   "metadata": {},
   "source": [
    "To get the details of the fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "92112808-bb7c-460f-b06e-a9c319bbc8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/803648085855/locations/us-central1/models/4627846640332439552'"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "models = model.list_tuned_model_names()\n",
    "#The first model in the list is the one we just tuned.\n",
    "entity_extraction_tuned_model = models[0]\n",
    "entity_extraction_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113928b-b83d-4f41-833b-a92e851cca3a",
   "metadata": {},
   "source": [
    "### Generate Cypher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d788b-4b31-402f-926f-be048d774886",
   "metadata": {},
   "source": [
    "Lets create a wrapper to call the text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "5e9f2895-c3c4-401c-a4d6-cf1b80a2050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_to_cypher(prompt, tuned_model_name=''):\n",
    "    try:\n",
    "        res = run_text_model(project_id, \"text-bison@001\", 0.1, 1024, 0.95, 40, prompt, location, tuned_model_name)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d2f78f-74f7-4ef2-bb65-ed9c83d69f95",
   "metadata": {},
   "source": [
    "We have to create Prompt Template that clearly states what schema to use, what kind of Cypher to generate and how. We will provide Question-Answer example and also a reason why that answer is arrived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "EMZysiC9YsqC",
   "metadata": {
    "id": "EMZysiC9YsqC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MATCH (p:Person)-[:HAS_SKILL]->(s:Skill) WHERE toLower(s.name) CONTAINS 'word' RETURN COUNT(p)\""
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Context:\n",
    "You are an expert Neo4j Cypher translator who understands the question in english and convert to Cypher strictly based on the Neo4j Schema provided and the instructions below:\n",
    "1. Use the Neo4j schema to generate cypher compatible ONLY for Neo4j Version 5\n",
    "2. Do not use EXISTS, SIZE keywords in the cypher. Use alias when using the WITH keyword\n",
    "3. Use only Nodes and relationships mentioned in the schema while generating the response\n",
    "4. Reply ONLY in Cypher\n",
    "5. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Company name use `toLower(c.name) contains 'neo4j'`\n",
    "6. Candidate node is synonymous to Person\n",
    "7. Always use aliases to refer properties in the query\n",
    "Now, use this Neo4j schema and Reply ONLY in Cypher when it makes sense.\n",
    "Schema:\n",
    "Nodes:\n",
    "    label:'Person',id:string,role:string,description:string //Person Node\n",
    "    label:'Position',id:string,title:string,location:string,startDate:string,endDate:string,url:string //Position Node\n",
    "    label:'Company',id:string,name:string //Company Node\n",
    "    label:'Skill',id:string,name:string,level:string //Skill Node\n",
    "    label:'Education',id:string,degree:string,university:string,graduation_date:string,score:string,url:string //Education Node\n",
    "Relationships:\n",
    "    (:Person)-[:HAS_POSITION]->(:Position)\n",
    "    (:Position)-[:AT_COMPANY]->(:Company)\n",
    "    (:Person)-[:HAS_SKILL]->(:Skill)\n",
    "    (:Person)-[:HAS_EDUCATION]->(:Education)\n",
    "Ouput Format (Strict): //Only code as output. No other text\n",
    "MATCH (p:Person)-[:HAS_SKILL]->(s:Skill) WHERE toLower(p.name) CONTAINS 'java' AND toLower(p.level) CONTAINS 'expert' RETURN COUNT(p) \n",
    "\n",
    "Question: How many Texas-based experts do I have on Delphi?\n",
    "Answer:\n",
    "MATCH (p:Person)-[:HAS_SKILL]->(s:Skill) \n",
    "MATCH (p)-[:HAS_POSITION]->(pos:Position)\n",
    "WHERE toLower(s.name) CONTAINS 'delphi' AND toLower(s.level) CONTAINS 'expert' \n",
    "AND (toLower(pos.location) CONTAINS 'texas' OR toLower(pos.location) CONTAINS 'tx') RETURN COUNT(p)\n",
    "\n",
    "Reason:\n",
    "1. As per schema definition of nodes & relationships above, Person node is related to Skill node via HAS_SKILL relationship.\n",
    "2. From the schema, Skill has name and levels as properties. Expertise can be checked using `level`\n",
    "3. Since Texas can be denoted as TX, we search for the Position's location as either 'texas' or 'tx'\n",
    "4. Finally, we return the number of persons who match the input criteria using COUNT function\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "que = 'How many experts do we have on MS Word?'\n",
    "_prompt = prompt.replace('{question}', que)\n",
    "\n",
    "cypher = english_to_cypher(_prompt, entity_extraction_tuned_model)\n",
    "if 'Answer:\\n ' in cypher:\n",
    "    cypher = cypher.split('Answer:\\n ')[1]\n",
    "cypher = cypher.replace('\\n', '')\n",
    "cypher\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac31d0-ca59-47cc-81be-ebbd121d86d8",
   "metadata": {},
   "source": [
    "## Skill Finder Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9212ac86-6c96-4640-a545-c9655e8d52d8",
   "metadata": {},
   "source": [
    "With our tuned model now working, let's create a Chatbot that can help our interaction with Neo4j using only English.\n",
    "\n",
    "We will be using Langchain to quickly build a chatbot that converts english to cypher, execute it on Neo4j and which will be then augmented using GenAI before sending the response to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947df081-47ea-4f94-b2f0-76d291987a80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### VertexAI LangChain\n",
    "Langchain for VertexAI is currently not yet committed to the LangChain codebase. Until then, we will use the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "8265a652-3df0-4e88-8a13-81ef5d188120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any, Mapping, List, Dict, Optional, Tuple, Union\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from pydantic import BaseModel, Extra, root_validator\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.schema import Generation, LLMResult\n",
    "from langchain.schema import AIMessage, BaseMessage, ChatGeneration, ChatResult, HumanMessage, SystemMessage\n",
    "\n",
    "from vertexai.preview.language_models import TextGenerationResponse, ChatSession\n",
    "\n",
    "\n",
    "def rate_limit(max_per_minute):\n",
    "  period = 60 / max_per_minute\n",
    "  print('Waiting')\n",
    "  while True:\n",
    "    before = time.time()\n",
    "    yield\n",
    "    after = time.time()\n",
    "    elapsed = after - before\n",
    "    sleep_time = max(0, period - elapsed)\n",
    "    if sleep_time > 0:\n",
    "      print('.', end='')\n",
    "      time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "class _VertexCommon(BaseModel):\n",
    "    \"\"\"Wrapper around Vertex AI large language models.\n",
    "\n",
    "    To use, you should have the\n",
    "    ``google.cloud.aiplatform.private_preview.language_models`` python package\n",
    "    installed.\n",
    "    \"\"\"\n",
    "    client: Any = None #: :meta private:\n",
    "    model_name: str = \"text-bison@001\"\n",
    "    \"\"\"Model name to use.\"\"\"\n",
    "\n",
    "    temperature: float = 0.2\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "\n",
    "    top_p: int = 0.8\n",
    "    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n",
    "\n",
    "    top_k: int = 40\n",
    "    \"\"\"The number of highest probability tokens to keep for top-k filtering.\"\"\"\n",
    "\n",
    "    max_output_tokens: int = 200\n",
    "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _default_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the default parameters for calling Vertex AI API.\"\"\"\n",
    "        return {\n",
    "            \"temperature\": self.temperature,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"max_output_tokens\": self.max_output_tokens\n",
    "        }\n",
    "\n",
    "    def _predict(self, prompt: str, stop: Optional[List[str]]) -> str:\n",
    "        res = self.client.predict(prompt, **self._default_params)\n",
    "        return self._enforce_stop_words(res.text, stop)\n",
    "\n",
    "    def _enforce_stop_words(self, text: str, stop: Optional[List[str]]) -> str:\n",
    "        if stop:\n",
    "            return enforce_stop_tokens(text, stop)\n",
    "        return text\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"vertex_ai\"\n",
    "\n",
    "class VertexLLM(_VertexCommon, LLM):\n",
    "    model_name: str = \"text-bison@001\"\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
    "        try:\n",
    "            from vertexai.preview.language_models import TextGenerationModel\n",
    "        except ImportError:\n",
    "            raise ValueError(\n",
    "                \"Could not import Vertex AI LLM python package. \"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            values[\"client\"] = TextGenerationModel.from_pretrained(values[\"model_name\"])\n",
    "        except AttributeError:\n",
    "            raise ValueError(\n",
    "                \"Could not set Vertex Text Model client.\"\n",
    "            )\n",
    "\n",
    "        return values\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Call out to Vertex AI's create endpoint.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to pass into the model.\n",
    "\n",
    "        Returns:\n",
    "            The string generated by the model.\n",
    "        \"\"\"\n",
    "        return self._predict(prompt, stop)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class _MessagePair:\n",
    "    \"\"\"InputOutputTextPair represents a pair of input and output texts.\"\"\"\n",
    "\n",
    "    question: HumanMessage\n",
    "    answer: AIMessage\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class _ChatHistory:\n",
    "    \"\"\"InputOutputTextPair represents a pair of input and output texts.\"\"\"\n",
    "\n",
    "    history: List[_MessagePair] = field(default_factory=list)\n",
    "    system_message: Optional[SystemMessage] = None\n",
    "\n",
    "\n",
    "def _parse_chat_history(history: List[BaseMessage]) -> _ChatHistory:\n",
    "    \"\"\"Parses a sequence of messages into history.\n",
    "\n",
    "    A sequency should be either (SystemMessage, HumanMessage, AIMessage,\n",
    "    HumanMessage, AIMessage, ...) or (HumanMessage, AIMessage, HumanMessage,\n",
    "    AIMessage, ...).\n",
    "    \"\"\"\n",
    "    if not history:\n",
    "        return _ChatHistory()\n",
    "    first_message = history[0]\n",
    "    system_message = first_message if isinstance(first_message, SystemMessage) else None\n",
    "    chat_history = _ChatHistory(system_message=system_message)\n",
    "    messages_left = history[1:] if system_message else history\n",
    "    # if len(messages_left) % 2 != 0:\n",
    "    #     raise ValueError(\n",
    "    #         f\"Amount of messages in history should be even, got {len(messages_left)}!\"\n",
    "    #     )\n",
    "    for question, answer in zip(messages_left[::2], messages_left[1::2]):\n",
    "        if not isinstance(question, HumanMessage) or not isinstance(answer, AIMessage):\n",
    "            raise ValueError(\n",
    "                \"A human message should follow a bot one, \"\n",
    "                f\"got {question.type}, {answer.type}.\"\n",
    "            )\n",
    "        chat_history.history.append(_MessagePair(question=question, answer=answer))\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "class _VertexChatCommon(_VertexCommon):\n",
    "    \"\"\"Wrapper around Vertex AI Chat large language models.\n",
    "\n",
    "    To use, you should have the\n",
    "    ``vertexai.preview.language_models`` python package\n",
    "    installed.\n",
    "    \"\"\"\n",
    "    model_name: str = \"chat-bison@001\"\n",
    "    \"\"\"Model name to use.\"\"\"\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
    "        try:\n",
    "            from vertexai.preview.language_models import ChatModel\n",
    "        except ImportError:\n",
    "            raise ValueError(\n",
    "                \"Could not import Vertex AI LLM python package. \"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            values[\"client\"] = ChatModel.from_pretrained(values[\"model_name\"])\n",
    "        except AttributeError:\n",
    "            raise ValueError(\n",
    "                \"Could not set Vertex Text Model client.\"\n",
    "            )\n",
    "\n",
    "        return values\n",
    "\n",
    "    def _response_to_chat_results(\n",
    "        self, response: TextGenerationResponse, stop: Optional[List[str]]\n",
    "    ) -> ChatResult:\n",
    "        text = self._enforce_stop_words(response.text, stop)\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=text))])\n",
    "\n",
    "\n",
    "class VertexChat(_VertexChatCommon, BaseChatModel):\n",
    "    \"\"\"Wrapper around Vertex AI large language models.\n",
    "\n",
    "    To use, you should have the\n",
    "    ``vertexai.preview.language_models`` python package\n",
    "    installed.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = \"chat-bison@001\"\n",
    "    chat: Any = None  #: :meta private:\n",
    "\n",
    "    def send_message(\n",
    "        self, message: Union[HumanMessage, str], stop: Optional[List[str]] = None\n",
    "    ) -> ChatResult:\n",
    "        text = message.content if isinstance(message, BaseMessage) else message\n",
    "        response = self.chat.send_message(text)\n",
    "        text = self._enforce_stop_words(response.text, stop)\n",
    "        return ChatResult(generations=[ChatGeneration(message=AIMessage(content=text))])\n",
    "\n",
    "    def _generate(\n",
    "        self, messages: List[BaseMessage], stop: Optional[List[str]] = None\n",
    "    ) -> ChatResult:\n",
    "        if not messages:\n",
    "            raise ValueError(\n",
    "                \"You should provide at least one message to start the chat!\"\n",
    "            )\n",
    "        question = messages[-1]\n",
    "        if not isinstance(question, HumanMessage):\n",
    "            raise ValueError(\n",
    "                f\"Last message in the list should be from human, got {question.type}.\"\n",
    "            )\n",
    "        self.start_chat(messages[:-1])\n",
    "        return self.send_message(question)\n",
    "\n",
    "    def start_chat(self, messages: List[BaseMessage]) -> None:\n",
    "        \"\"\"Starts a chat.\"\"\"\n",
    "        history = _parse_chat_history(messages)\n",
    "        context = history.system_message.content if history.system_message else None\n",
    "        self.chat = self.client.start_chat(context=context, **self._default_params)\n",
    "        for pair in history.history:\n",
    "            self.chat._history.append((pair.question.content, pair.answer.content))\n",
    "\n",
    "    def clear_chat(self) -> None:\n",
    "        self.chat = None\n",
    "\n",
    "    @property\n",
    "    def history(self) -> List[BaseMessage]:\n",
    "        \"\"\"Chat history.\"\"\"\n",
    "        history: List[BaseMessage] = []\n",
    "        if self.chat:\n",
    "            for question, answer in self.chat._history:\n",
    "                history.append(HumanMessage(content=question))\n",
    "                history.append(AIMessage(content=answer))\n",
    "        return history\n",
    "\n",
    "    async def _agenerate(\n",
    "        self, messages: List[BaseMessage], stop: Optional[List[str]] = None\n",
    "    ) -> ChatResult:\n",
    "        raise NotImplementedError(\n",
    "            \"\"\"Vertex AI doesn't support async requests at the moment.\"\"\"\n",
    "        )\n",
    "\n",
    "class VertexMultiTurnChat(_VertexChatCommon, BaseChatModel):\n",
    "    \"\"\"Wrapper around Vertex AI large language models.\"\"\"\n",
    "\n",
    "    model_name: str = \"chat-bison@001\"\n",
    "    chat: Optional[ChatSession] = None\n",
    "\n",
    "    def clear_chat(self) -> None:\n",
    "        self.chat = None\n",
    "\n",
    "    def start_chat(self, message: Optional[SystemMessage] = None) -> None:\n",
    "        if self.chat:\n",
    "            raise ValueError(\"Chat has already been started. Please, clear it first.\")\n",
    "        if message and not isinstance(message, SystemMessage):\n",
    "            raise ValueError(\"Context should be a system message\")\n",
    "        context = message.content if message else None\n",
    "        self.chat = self.client.start_chat(context=context, **self._default_params)\n",
    "\n",
    "    @property\n",
    "    def history(self) -> List[Tuple[str]]:\n",
    "        \"\"\"Chat history.\"\"\"\n",
    "        if self.chat:\n",
    "            return self.chat._history\n",
    "        return []\n",
    "\n",
    "    def _generate(\n",
    "        self, messages: List[BaseMessage], stop: Optional[List[str]] = None\n",
    "    ) -> ChatResult:\n",
    "        if len(messages) != 1:\n",
    "            raise ValueError(\n",
    "                \"You should send exactly one message to the chat each turn.\"\n",
    "            )\n",
    "        if not self.chat:\n",
    "            raise ValueError(\"You should start_chat first!\")\n",
    "        response = self.chat.send_message(messages[0].content)\n",
    "        return self._response_to_chat_results(response, stop=stop)\n",
    "\n",
    "    async def _agenerate(\n",
    "        self, messages: List[BaseMessage], stop: Optional[List[str]] = None\n",
    "    ) -> ChatResult:\n",
    "        raise NotImplementedError(\n",
    "            \"\"\"Vertex AI doesn't support async requests at the moment.\"\"\"\n",
    "        )\n",
    "\n",
    "class VertexEmbeddings(Embeddings, BaseModel):\n",
    "    \"\"\"Wrapper around Vertex AI large language models embeddings API.\n",
    "\n",
    "    To use, you should have the\n",
    "    ``google.cloud.aiplatform.private_preview.language_models`` python package\n",
    "    installed.\n",
    "    \"\"\"\n",
    "    model_name: str = \"textembedding-gecko@001\"\n",
    "    \"\"\"Model name to use.\"\"\"\n",
    "\n",
    "    model: Any\n",
    "    requests_per_minute: int = 15\n",
    "\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
    "        try:\n",
    "            from vertexai.preview.language_models import TextEmbeddingModel\n",
    "\n",
    "        except ImportError:\n",
    "            raise ValueError(\n",
    "                \"Could not import Vertex AI LLM python package. \"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            values[\"model\"] = TextEmbeddingModel\n",
    "\n",
    "        except AttributeError:\n",
    "            raise ValueError(\n",
    "                \"Could not set Vertex Text Model client.\"\n",
    "            )\n",
    "\n",
    "        return values\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "      \"\"\"Call Vertex LLM embedding endpoint for embedding docs\n",
    "      Args:\n",
    "          texts: The list of texts to embed.\n",
    "      Returns:\n",
    "          List of embeddings, one for each text.\n",
    "      \"\"\"\n",
    "      self.model = self.model.from_pretrained(self.model_name)\n",
    "\n",
    "      limiter = rate_limit(self.requests_per_minute)\n",
    "      results = []\n",
    "      docs = list(texts)\n",
    "\n",
    "      while docs:\n",
    "        # Working in batches of 2 because the API apparently won't let\n",
    "        # us send more than 2 documents per request to get embeddings.\n",
    "        head, docs = docs[:2], docs[2:]\n",
    "        # print(f'Sending embedding request for: {head!r}')\n",
    "        chunk = self.model.get_embeddings(head)\n",
    "        results.extend(chunk)\n",
    "        next(limiter)\n",
    "\n",
    "      return [r.values for r in results]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "      \"\"\"Call Vertex LLM embedding endpoint for embedding query text.\n",
    "      Args:\n",
    "        text: The text to embed.\n",
    "      Returns:\n",
    "        Embedding for the text.\n",
    "      \"\"\"\n",
    "      single_result = self.embed_documents([text])\n",
    "      return single_result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e1171-1c1f-4fff-8cf3-58426e76f030",
   "metadata": {},
   "source": [
    "### Neo4j LangChain module\n",
    "Neo4j released a LangChain agent that can convert english to cypher and augment the result with LLM based on your DB schema. This makes Graph Consumption easier for non-cypher experts. \n",
    "\n",
    "![neo4j-langchain](images/langchain-neo4j.png)\n",
    "\n",
    "Let's see how to use it. First, we have to create Neo4jGraph & VertexLLM Connection objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "5945f4b2-fae8-4f63-b44e-340409f4d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=connectionUrl, \n",
    "    username=\"neo4j\", \n",
    "    password=password\n",
    ")\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    VertexLLM(model_name='text-bison@001',\n",
    "            max_output_tokens=1024,\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            top_k=40), graph=graph, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795e6a9-fa92-4a5f-8cc5-44a20d89d142",
   "metadata": {},
   "source": [
    "That's it! You can run the agent now. Simply provide the command in english. You get Cypher generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "4bacd47c-b9dc-4da3-9344-149b23665cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:HAS_SKILL]->(s:Skill) WHERE s.name = \"MS Word\" RETURN count(p)\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'count(p)': 4}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We have 4 experts on MS Word.'"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"\"\"How many experts do we have on MS Word?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f2c16-16cb-41f0-b50c-a8047c1a1517",
   "metadata": {},
   "source": [
    "### Chatbot!\n",
    "Time to build a chatbot. We will be using Gradio to quickly try out our chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3679c6ac-9eee-4fef-ba57-d77af0d3c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user gradio --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21511e12-0dbb-4b94-8723-d25b4fe8c641",
   "metadata": {},
   "source": [
    "Running the code below will render a Chat widget. You can view the Cypher generated for your input below this rendering. \n",
    "P.S: Due to Quota limitations, you might be facing errors while submitting the input. You need to wait a while in between your queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "2a060728-90a7-48fd-a197-ecd099b887bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "Running on public URL: https://82df6cc5bf8520af37.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://82df6cc5bf8520af37.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:HAS_EDUCATION]->(e:Education)-[:DEGREE]->(d:Degree) WHERE d.name = 'Bachelor' WITH p, e, d MATCH (p)-[:HAS_SKILL]->(s:Skill) RETURN s.name AS skill\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (p:Person)-[:HAS_POSITION]->(pos:Position)-[:AT_COMPANY]->(c:Company) RETURN c.name AS company, count(p) AS count\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'company': 'Mutual Benefit Group', 'count': 1}, {'company': 'Genworth Financial', 'count': 3}, {'company': 'Kaybamz Inc', 'count': 1}, {'company': 'Lastcard Managment Inc', 'count': 1}, {'company': 'Express Scripts', 'count': 1}, {'company': 'Apex Systems Inc', 'count': 1}, {'company': 'Charter Communications', 'count': 2}, {'company': 'LevelUp RPO', 'count': 1}, {'company': 'Ally Bank', 'count': 1}, {'company': 'ITT Corporation', 'count': 1}, {'company': 'Oakhurst, NJ', 'count': 1}, {'company': 'Baker Hughes, Inc.', 'count': 1}, {'company': 'Small Consulting Jobs', 'count': 1}, {'company': 'Houston & Galveston Area Council', 'count': 1}, {'company': 'Akzo Nobel - International Paints', 'count': 2}, {'company': \"Cabela's Inc.\", 'count': 2}, {'company': 'IT Network Consultants', 'count': 1}, {'company': 'Credit Solutions, LLC', 'count': 1}, {'company': 'Wendell Wilson Business Technology Consulting LLC.', 'count': 1}, {'company': 'Ronin Bushido Aikido', 'count': 1}, {'company': 'Actavis Pharmaceuticals, Inc', 'count': 1}, {'company': 'Sheridan Healthcare Inc', 'count': 1}, {'company': 'Office Depot, Inc', 'count': 1}, {'company': 'YHA/Humana', 'count': 1}, {'company': 'UPM', 'count': 1}, {'company': 'The Salvation Army', 'count': 1}, {'company': 'Blount Energy Partners inc', 'count': 1}, {'company': 'Green Dream Group', 'count': 1}, {'company': 'ACA Alliance', 'count': 1}, {'company': 'LNG NASA INTERNATIONAL SDN.BHD', 'count': 1}, {'company': 'MIDDLE EAST TELECOMMUNICATIONS COMPANY (METCO)', 'count': 1}, {'company': 'Amazon', 'count': 1}, {'company': 'Equity Residential', 'count': 1}, {'company': 'XO communications, L.L.C', 'count': 1}, {'company': 'Skyline Technologies', 'count': 1}, {'company': 'MBA Solutions', 'count': 1}, {'company': 'Washington Tech Solutions', 'count': 1}, {'company': 'Principal Financial Group', 'count': 1}, {'company': \"Kohl's Department Stores\", 'count': 1}, {'company': 'PCubed', 'count': 1}, {'company': 'Tech Marine Business', 'count': 1}, {'company': 'Department of the Navy', 'count': 1}, {'company': 'Net@Work', 'count': 1}, {'company': '211 San Diego', 'count': 1}, {'company': 'Alert1', 'count': 1}, {'company': 'Suburban Community Hospital', 'count': 1}, {'company': 'NTTDATA/Guardian Life', 'count': 1}, {'company': 'Global Emergency Resources', 'count': 1}, {'company': 'Unknown', 'count': 1}, {'company': 'Lockheed Martin', 'count': 1}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "llm=VertexLLM(model_name='text-bison@001',\n",
    "                            max_output_tokens=1024,\n",
    "                            temperature=0,\n",
    "                            top_p=0.95,\n",
    "                            top_k=40)\n",
    "agent_chain = chain\n",
    "def chat_response(input_text):\n",
    "    response = agent_chain.run(input_text)\n",
    "    return response\n",
    "\n",
    "interface = gr.Interface(fn=chat_response, inputs=\"text\", outputs=\"text\", \n",
    "                         description=\"Skill Finder Chatbot\")\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9e37f-4cdf-4f09-aaa3-f57db2ae44b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
