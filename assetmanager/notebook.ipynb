{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asset Manager Example\n",
    "In this notebook, let's explore how to leverage Google Generative AI to build and consume a knowledge graph in Neo4j.\n",
    "\n",
    "This notebook parses Form-13 data From SEC EDGAR. The Form 13 files are semi structured data that are pretty nasty to parse  We'll use generative AI to do it for us.  We will then also use the LLM to generate Cypher statments to load the extracted data into a Neo4j graph.  Then, we'll use a chatbot to query the knowledge graph we've created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First off, check that the Python environment you installed in the readme is running this notebook. Make sure you select the `py38` kernel in the top right of this notebook. You should see a 3.8 version when you run this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.17 | packaged by conda-forge | (default, Jun 16 2023, 07:06:00) \\n[GCC 11.4.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we install and import some libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphdatascience in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (1.7)\n",
      "Requirement already satisfied: multimethod<2.0,>=1.0 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from graphdatascience) (1.9.1)\n",
      "Requirement already satisfied: neo4j<6.0,>=4.4.2 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from graphdatascience) (5.11.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from graphdatascience) (2.0.3)\n",
      "Requirement already satisfied: pyarrow<13.0,>=4.0 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from graphdatascience) (12.0.1)\n",
      "Requirement already satisfied: textdistance<5.0,>=4.0 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from graphdatascience) (4.5.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.0 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from graphdatascience) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from graphdatascience) (4.7.1)\n",
      "Requirement already satisfied: pytz in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from neo4j<6.0,>=4.4.2->graphdatascience) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from pandas<3.0,>=1.0->graphdatascience) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from pandas<3.0,>=1.0->graphdatascience) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/jupyter/.local/lib/python3.8/site-packages (from pandas<3.0,>=1.0->graphdatascience) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->graphdatascience) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "E0816 02:28:15.147075566    8449 backup_poller.cc:127]                 Run client channel backup poller: UNKNOWN:pollset_work {created_time:\"2023-08-16T02:28:15.146754795+00:00\", children:[UNKNOWN:Bad file descriptor {created_time:\"2023-08-16T02:28:15.146667145+00:00\", errno:9, os_error:\"Bad file descriptor\", syscall:\"epoll_wait\"}]}\n",
      "Requirement already satisfied: langchain in /home/jupyter/.local/lib/python3.8/site-packages (0.0.265)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/jupyter/.local/lib/python3.8/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/jupyter/.local/lib/python3.8/site-packages (from langchain) (2.0.20)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/jupyter/.local/lib/python3.8/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/jupyter/.local/lib/python3.8/site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /home/jupyter/.local/lib/python3.8/site-packages (from langchain) (0.0.22)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/jupyter/.local/lib/python3.8/site-packages (from langchain) (2.8.5)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/jupyter/.local/lib/python3.8/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/jupyter/.local/lib/python3.8/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /home/jupyter/.local/lib/python3.8/site-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/jupyter/.local/lib/python3.8/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/jupyter/.local/lib/python3.8/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jupyter/.local/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/jupyter/.local/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyter/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyter/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/jupyter/.local/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/jupyter/.local/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: google-cloud-aiplatform in /home/jupyter/.local/lib/python3.8/site-packages (1.30.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /home/jupyter/.local/lib/python3.8/site-packages (from google-cloud-aiplatform) (2.11.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /home/jupyter/.local/lib/python3.8/site-packages (from google-cloud-aiplatform) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /home/jupyter/.local/lib/python3.8/site-packages (from google-cloud-aiplatform) (4.24.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from google-cloud-aiplatform) (23.1)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /home/jupyter/.local/lib/python3.8/site-packages (from google-cloud-aiplatform) (2.10.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /home/jupyter/.local/lib/python3.8/site-packages (from google-cloud-aiplatform) (3.11.4)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /home/jupyter/.local/lib/python3.8/site-packages (from google-cloud-aiplatform) (1.10.3)\n",
      "Requirement already satisfied: shapely<2.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/jupyter/.local/lib/python3.8/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /home/jupyter/.local/lib/python3.8/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.22.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /home/jupyter/.local/lib/python3.8/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/jupyter/.local/lib/python3.8/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.57.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /home/jupyter/.local/lib/python3.8/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.57.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /home/jupyter/.local/lib/python3.8/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /home/jupyter/.local/lib/python3.8/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /home/jupyter/.local/lib/python3.8/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/jupyter/.local/lib/python3.8/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/jupyter/.local/lib/python3.8/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/jupyter/.local/lib/python3.8/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.16)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/jupyter/.local/lib/python3.8/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jupyter/.local/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyter/.local/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyter/.local/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.7.22)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/jupyter/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting gradio\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/58/c8/d155c02c382b7bcb3733d6025bb15c818d82768230b3fa550eeb39a24a82/gradio-3.40.1-py3-none-any.whl.metadata\n",
      "  Downloading gradio-3.40.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Obtaining dependency information for aiofiles<24.0,>=22.0 from https://files.pythonhosted.org/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl.metadata\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: aiohttp~=3.0 in /home/jupyter/.local/lib/python3.8/site-packages (from gradio) (3.8.5)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio)\n",
      "  Obtaining dependency information for altair<6.0,>=4.2.0 from https://files.pythonhosted.org/packages/b2/20/5c3b89d6f8d9938325a9330793438389e0dc94c34d921f6da35ec62095f3/altair-5.0.1-py3-none-any.whl.metadata\n",
      "  Downloading altair-5.0.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting fastapi (from gradio)\n",
      "  Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/09/ae/8378894f9fbdf0297cdffdc79496ccd779166d675fec47cad8d2ca782739/fastapi-0.101.1-py3-none-any.whl.metadata\n",
      "  Downloading fastapi-0.101.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio-client>=0.4.0 (from gradio)\n",
      "  Obtaining dependency information for gradio-client>=0.4.0 from https://files.pythonhosted.org/packages/85/29/f3b1e5f54b893665cd703a9ad3b67e534d50c5a2cf14557571a8bb91b0d8/gradio_client-0.4.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio_client-0.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx (from gradio)\n",
      "  Obtaining dependency information for httpx from https://files.pythonhosted.org/packages/ec/91/e41f64f03d2a13aee7e8c819d82ee3aa7cdc484d18c0ae859742597d5aa0/httpx-0.24.1-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.24.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.14.0 (from gradio)\n",
      "  Obtaining dependency information for huggingface-hub>=0.14.0 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio)\n",
      "  Obtaining dependency information for importlib-resources<7.0,>=1.3 from https://files.pythonhosted.org/packages/25/d4/592f53ce2f8dde8be5720851bd0ab71cc2e76c55978e4163ef1ab7e389bb/importlib_resources-6.0.1-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.0.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jinja2<4.0 (from gradio)\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py[linkify]>=2.0.0 (from gradio)\n",
      "  Obtaining dependency information for markdown-it-py[linkify]>=2.0.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Obtaining dependency information for markupsafe~=2.0 from https://files.pythonhosted.org/packages/de/e2/32c14301bb023986dff527a49325b6259cab4ebb4633f69de54af312fc45/MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting matplotlib~=3.0 (from gradio)\n",
      "  Obtaining dependency information for matplotlib~=3.0 from https://files.pythonhosted.org/packages/b4/c2/f74e0deb26379aead0956a6ecf9acd4587debba0c7abe4bd8fe53fe04ec2/matplotlib-3.7.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata\n",
      "  Downloading matplotlib-3.7.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
      "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.0 in /home/jupyter/.local/lib/python3.8/site-packages (from gradio) (1.24.4)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Obtaining dependency information for orjson~=3.0 from https://files.pythonhosted.org/packages/17/4f/98e4100b74bc0083ffee1b07aeda0cc01cba7a781b0a4587a35f7251f983/orjson-3.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading orjson-3.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from gradio) (23.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from gradio) (2.0.3)\n",
      "Collecting pillow<11.0,>=8.0 (from gradio)\n",
      "  Obtaining dependency information for pillow<11.0,>=8.0 from https://files.pythonhosted.org/packages/ff/8c/5927a58c43ebc16e508eef325fdc6473b569e2474d3b4be49798aa371007/Pillow-10.0.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading Pillow-10.0.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /home/jupyter/.local/lib/python3.8/site-packages (from gradio) (1.10.12)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart (from gradio)\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /home/jupyter/.local/lib/python3.8/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: requests~=2.0 in /home/jupyter/.local/lib/python3.8/site-packages (from gradio) (2.31.0)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from gradio) (4.7.1)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Obtaining dependency information for uvicorn>=0.14.0 from https://files.pythonhosted.org/packages/79/96/b0882a1c3f7ef3dd86879e041212ae5b62b4bd352320889231cc735a8e8f/uvicorn-0.23.2-py3-none-any.whl.metadata\n",
      "  Downloading uvicorn-0.23.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio)\n",
      "  Downloading websockets-11.0.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jupyter/.local/lib/python3.8/site-packages (from aiohttp~=3.0->gradio) (1.3.1)\n",
      "Collecting jsonschema>=3.0 (from altair<6.0,>=4.2.0->gradio)\n",
      "  Obtaining dependency information for jsonschema>=3.0 from https://files.pythonhosted.org/packages/2b/ff/af59fd34bc4d7ac3e6e0cd1f3c10317d329b6c1aee179e8b24ad9a79fbac/jsonschema-4.19.0-py3-none-any.whl.metadata\n",
      "  Downloading jsonschema-4.19.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting toolz (from altair<6.0,>=4.2.0->gradio)\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec (from gradio-client>=0.4.0->gradio)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.14.0->gradio)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/00/45/ec3407adf6f6b5bf867a4462b2b0af27597a26bd3cd6e2534cb6ab029938/filelock-3.12.2-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.16.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py[linkify]>=2.0.0->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio)\n",
      "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib~=3.0->gradio)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/32/c8/aa9e87941002150b1a8e7087e48da1c76290268b9fdfa3034a98a5806198/contourpy-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading contourpy-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib~=3.0->gradio)\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib~=3.0->gradio)\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/d3/e2/0dc07c946b98a036b25b97899bf38d51aef24f793b80ac686f07f817b7ac/fonttools-4.42.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading fonttools-4.42.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib~=3.0->gradio)\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing<3.1,>=2.3.1 (from matplotlib~=3.0->gradio)\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
      "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
      "INFO: pip is still looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
      "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
      "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio)\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyter/.local/lib/python3.8/site-packages (from requests~=2.0->gradio) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/.local/lib/python3.8/site-packages (from requests~=2.0->gradio) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyter/.local/lib/python3.8/site-packages (from requests~=2.0->gradio) (2023.7.22)\n",
      "Collecting click>=7.0 (from uvicorn>=0.14.0->gradio)\n",
      "  Obtaining dependency information for click>=7.0 from https://files.pythonhosted.org/packages/1a/70/e63223f8116931d365993d4a6b7ef653a4d920b41d03de7c59499962821f/click-8.1.6-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
      "  Obtaining dependency information for starlette<0.28.0,>=0.27.0 from https://files.pythonhosted.org/packages/58/f8/e2cca22387965584a409795913b774235752be4176d276714e15e1a58884/starlette-0.27.0-py3-none-any.whl.metadata\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n",
      "  Obtaining dependency information for httpcore<0.18.0,>=0.15.0 from https://files.pythonhosted.org/packages/94/2c/2bde7ff8dd2064395555220cbf7cba79991172bf5315a07eb3ac7688d9f1/httpcore-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-0.17.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting sniffio (from httpx->gradio)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from httpcore<0.18.0,>=0.15.0->httpx->gradio)\n",
      "  Obtaining dependency information for anyio<5.0,>=3.0 from https://files.pythonhosted.org/packages/19/24/44299477fe7dcc9cb58d0a57d5a7588d6af2ff403fdd2d47a246c91a3246/anyio-3.7.1-py3-none-any.whl.metadata\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio)\n",
      "  Obtaining dependency information for jsonschema-specifications>=2023.03.6 from https://files.pythonhosted.org/packages/1c/24/83349ac2189cc2435e84da3f69ba3c97314d3c0622628e55171c6798ed80/jsonschema_specifications-2023.7.1-py3-none-any.whl.metadata\n",
      "  Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting pkgutil-resolve-name>=1.3.10 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio)\n",
      "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio)\n",
      "  Obtaining dependency information for referencing>=0.28.4 from https://files.pythonhosted.org/packages/be/8e/56d6f1e2d591f4d6cbcba446cac4a1b0dc4f584537e2071d9bcee8eeab6b/referencing-0.30.2-py3-none-any.whl.metadata\n",
      "  Downloading referencing-0.30.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio)\n",
      "  Obtaining dependency information for rpds-py>=0.7.1 from https://files.pythonhosted.org/packages/7f/19/aa000240fa9343858a967b08037792695a66560ce5054528198190ce291b/rpds_py-0.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading rpds_py-0.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio)\n",
      "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/jupyter/conda_env/py38/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Collecting exceptiongroup (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio)\n",
      "  Obtaining dependency information for exceptiongroup from https://files.pythonhosted.org/packages/ad/83/b71e58666f156a39fb29417e4c8ca4bc7400c0dd4ed9e8842ab54dc8c344/exceptiongroup-1.1.3-py3-none-any.whl.metadata\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Downloading gradio-3.40.1-py3-none-any.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading altair-5.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-0.4.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Downloading importlib_resources-6.0.1-py3-none-any.whl (34 kB)\n",
      "Downloading MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading matplotlib-3.7.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.9.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.0/140.0 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Pillow-10.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.101.1-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.6-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fonttools-4.42.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema-4.19.0-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
      "Downloading referencing-0.30.2-py3-none-any.whl (25 kB)\n",
      "Downloading rpds_py-0.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=79910c2f252598f1aa1d3c51b5abb26279af15ea9892d2114d0b82c2b40e8ebc\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/75/a3/1a/2f3f90b9a4eb0408109ae1b5bae01efbdf8ab4ef98797433e4\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, toolz, sniffio, semantic-version, rpds-py, python-multipart, pyparsing, pkgutil-resolve-name, pillow, orjson, mdurl, markupsafe, kiwisolver, importlib-resources, h11, fsspec, fonttools, filelock, exceptiongroup, cycler, contourpy, click, aiofiles, uvicorn, referencing, matplotlib, markdown-it-py, linkify-it-py, jinja2, huggingface-hub, anyio, starlette, mdit-py-plugins, jsonschema-specifications, httpcore, jsonschema, httpx, fastapi, gradio-client, altair, gradio\n",
      "\u001b[33m  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script uvicorn is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script markdown-it is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script jsonschema is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script httpx is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts gradio and upload_theme are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed aiofiles-23.2.1 altair-5.0.1 anyio-3.7.1 click-8.1.6 contourpy-1.1.0 cycler-0.11.0 exceptiongroup-1.1.3 fastapi-0.101.1 ffmpy-0.3.1 filelock-3.12.2 fonttools-4.42.0 fsspec-2023.6.0 gradio-3.40.1 gradio-client-0.4.0 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.16.4 importlib-resources-6.0.1 jinja2-3.1.2 jsonschema-4.19.0 jsonschema-specifications-2023.7.1 kiwisolver-1.4.4 linkify-it-py-2.0.2 markdown-it-py-2.2.0 markupsafe-2.1.3 matplotlib-3.7.2 mdit-py-plugins-0.3.3 mdurl-0.1.2 orjson-3.9.4 pillow-10.0.0 pkgutil-resolve-name-1.3.10 pydub-0.25.1 pyparsing-3.0.9 python-multipart-0.0.6 referencing-0.30.2 rpds-py-0.9.2 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.27.0 toolz-0.12.0 uc-micro-py-1.0.2 uvicorn-0.23.2 websockets-11.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install graphdatascience\n",
    "%pip install --user langchain  # library for combining functional steps around LLM calls\n",
    "%pip install --user google-cloud-aiplatform  # library for accessing VertexAI\n",
    "%pip install --user gradio  # for building the chat interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Definition\n",
    "\n",
    "In the upcoming sections, we will extract knowledge adhering to the following schema. This is a very Simplified schema to denote investment management entities and companies they own through common stock. Normally, you will have Domain Experts who come up with a richer data model, and you can extend the below to work on more data/forms to fill such a model.\n",
    "\n",
    "To achieve our Extraction goal as per the schema, We will use a series of prompts, each focused on only one task - to extract a specific entity. By this way, you can go for more granular extraction. The prompts I used here can be improved and in production scenario, you should consider running QA on the prompt pipelines to ensure that the extracted information is correct.\n",
    "\n",
    "Let's go in this order to gather the data in accordance to out data model:\n",
    "\n",
    "1. Extract Manager Information\n",
    "2. Extract Filing Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr_info_tpl = \"\"\"From the text below, extract the following as json. Do not miss any of these information.\n",
    "* The tags mentioned below may or may not namespaced. So extract accordingly. Eg: <ns1:tag> is equal to <tag>\n",
    "* \"name\" - The name from the <name> tag under <filingManager> tag\n",
    "* \"street1\" - The manager's street1 address from the <com:street1> tag under <address> tag\n",
    "* \"street2\" - The manager's street2 address from the <com:street2> tag under <address> tag\n",
    "* \"city\" - The manager's city address from the <com:city> tag under <address> tag\n",
    "* \"stateOrCounty\" - The manager's stateOrCounty address from the <com:stateOrCountry> tag under <address> tag\n",
    "* \"zipCode\" - The manager's zipCode from the <com:zipCode> tag under <address> tag\n",
    "* \"reportCalendarOrQuarter\" - The reportCalendarOrQuarter from the <reportCalendarOrQuarter> tag under <address> tag\n",
    "* Just return me the JSON enclosed by 3 backticks. No other text in the response\n",
    "\n",
    "Text:\n",
    "$ctext\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filing_info_tpl = \"\"\"The text below contains a list of investments. Each instance of <infoTable> tag represents a unique investment. \n",
    "For each investment, please extract the below variables into json then combine into a list enclosed by 3 back ticks. Please use the quated names below while doing this\n",
    "* \"cusip\" - The cusip from the <cusip> tag under <infoTable> tag\n",
    "* \"name\" - The name under the <nameOfIssuer> tag.\n",
    "* \"value\" - The value from the <value> tag under <infoTable> tag. Return as a number. \n",
    "* \"shares\" - The sshPrnamt from the <sshPrnamt> tag under <infoTable> tag. Return as a number. \n",
    "* \"sshPrnamtType\" - The sshPrnamtType from the <sshPrnamtType> tag under <infoTable> tag\n",
    "* \"investmentDiscretion\" - The investmentDiscretion from the <investmentDiscretion> tag under <infoTable> tag\n",
    "* \"votingSole\" - The votingSole from the <votingSole> tag under <infoTable> tag\n",
    "* \"votingShared\" - The votingShared from the <votingShared> tag under <infoTable> tag\n",
    "* \"votingNone\" - The votingNone from the <votingNone> tag under <infoTable> tag\n",
    "\n",
    "Text:\n",
    "$ctext\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Using LLMs\n",
    "Let's create some helper function to talk to the LLM with our prompt and text input. We will use the text-bison base model. In your use case, you might need to tune it. VertexAI provides an elegant way to finetune it. The weights will be staying within your tenant and the base model is frozen.\n",
    "\n",
    "First off, you'll need to set your project_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'neo4jbusinessdev'\n",
    "location = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for calling language model\n",
    "def run_text_model(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    prompt: str,\n",
    "    location: str = location,\n",
    "    tuned_model_name: str = None,\n",
    "    ) :\n",
    "    \"\"\"Text Completion Use a Large Language Model.\"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    if tuned_model_name is None:\n",
    "        model = TextGenerationModel.from_pretrained(model_name)\n",
    "    else:\n",
    "        model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for entity extraction / parsing\n",
    "def extract_entities_relationships(prompt, tuned_model_name=None):\n",
    "    try:\n",
    "        res = run_text_model(project_id, \"text-bison@001\", 0, 1024, 0.8, 1, prompt, location, tuned_model_name)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting function for chunking up filing information to avoid hitting LLM token limits\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def split_filing_info(s, chunk_size=5):\n",
    "    pattern = '(</(\\w+:)?infoTable>)'\n",
    "    splitter = re.findall(pattern, s)[0][0]\n",
    "    _parts = s.split(splitter)\n",
    "    if len(_parts) > chunk_size:\n",
    "        chunks_of_list = np.array_split(_parts, len(_parts)/chunk_size) # max 5 filings per part\n",
    "        chunks_of_str = map(lambda x: splitter.join(x), chunks_of_list)\n",
    "        return list(chunks_of_str)\n",
    "    else:\n",
    "        return [s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Example for Parsing\n",
    "Let's start with one Form 13 file to see how we can parse it with generative AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket('neo4j-datasets')\n",
    "blob = bucket.blob('form13/raw/raw_2023-05-15_archives_edgar_data_1027451_0000919574-23-003245.txt')\n",
    "\n",
    "inp_text = blob.download_as_string().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SEC-DOCUMENT>0000919574-23-003245.txt : 20230515\n",
      "<SEC-HEADER>0000919574-23-003245.hdr.sgml : 20230515\n",
      "<ACCEPTANCE-DATETIME>20230515103943\n",
      "ACCESSION NUMBER:\t\t0000919574-23-003245\n",
      "CONFORMED SUBMISSION TYPE:\t13F-HR\n",
      "PUBLIC DOCUMENT COUNT:\t\t2\n",
      "CONFORMED PERIOD OF REPORT:\t20230331\n",
      "FILED AS OF DATE:\t\t20230515\n",
      "DATE AS OF CHANGE:\t\t20230515\n",
      "EFFECTIVENESS DATE:\t\t20230515\n",
      "\n",
      "FILER:\n",
      "\n",
      "\tCOMPANY DATA:\t\n",
      "\t\tCOMPANY CONFORMED NAME:\t\t\tTIGER MANAGEMENT L.L.C.\n",
      "\t\tCENTRAL INDEX KEY:\t\t\t0001027451\n",
      "\t\tIRS NUMBER:\t\t\t\t000000000\n",
      "\t\tSTATE OF INCORPORATION:\t\t\tDE\n",
      "\n",
      "\tFILING VALUES:\n",
      "\t\tFORM TYPE:\t\t13F-HR\n",
      "\t\tSEC ACT:\t\t1934 Act\n",
      "\t\tSEC FILE NUMBER:\t028-05892\n",
      "\t\tFILM NUMBER:\t\t23919492\n",
      "\n",
      "\tBUSINESS ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t101 PARK AVENUE\n",
      "\t\tCITY:\t\t\tNEW YORK\n",
      "\t\tSTATE:\t\t\tNY\n",
      "\t\tZIP:\t\t\t10178\n",
      "\t\tBUSINESS PHONE:\t\t212-984-2500\n",
      "\n",
      "\tMAIL ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t101 PARK AVENUE\n",
      "\t\tCITY:\t\t\tNEW YORK\n",
      "\t\tSTATE:\t\t\tNY\n",
      "\t\tZIP:\t\t\t10178\n",
      "\n",
      "\tFORMER COMPANY:\t\n",
      "\t\tFORMER CONFORMED NAME:\tTIGER MANAGEMENT LLC/NY\n",
      "\t\tDATE OF NAME CHANGE:\t20010606\n",
      "</SEC-HEADER>\n",
      "<DOCUMENT>\n",
      "<TYPE>13F-HR\n",
      "<SEQUENCE>1\n",
      "<FILENAME>primary_doc.xml\n",
      "<TEXT>\n",
      "<XML>\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<edgarSubmission xsi:schemaLocation=\"http://www.sec.gov/edgar/thirteenffiler eis_13F_Filer.xsd\" xmlns=\"http://www.sec.gov/edgar/thirteenffiler\" xmlns:ns1=\"http://www.sec.gov/edgar/common\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n",
      "  <schemaVersion>X0202</schemaVersion>\n",
      "  <headerData>\n",
      "    <submissionType>13F-HR</submissionType>\n",
      "    <filerInfo>\n",
      "      <liveTestFlag>LIVE</liveTestFlag>\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print(inp_text[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = inp_text.split('<XML>')\n",
    "manager_info = contents[1].split('</XML>')[0].strip()\n",
    "filing_info = contents[2].split('</XML>')[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Manager Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=project_id, location=location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the text below, extract the following as json. Do not miss any of these information.\n",
      "* The tags mentioned below may or may not namespaced. So extract accordingly. Eg: <ns1:tag> is equal to <tag>\n",
      "* \"name\" - The name from the <name> tag under <filingManager> tag\n",
      "* \"street1\" - The manager's street1 address from the <com:street1> tag under <address> tag\n",
      "* \"street2\" - The manager's street2 address from the <com:street2> tag under <address> tag\n",
      "* \"city\" - The manager's city address from the <com:city> tag under <address> tag\n",
      "* \"stateOrCounty\" - The manager's stateOrCounty address from the <com:stateOrCountry> tag under <address> tag\n",
      "* \"zipCode\" - The manager's zipCode from the <com:zipCode> tag under <address> tag\n",
      "* \"reportCalendarOrQuarter\" - The reportCalendarOrQuarter from the <reportCalendarOrQuarter> tag under <address> tag\n",
      "* Just return me the JSON enclosed by 3 backticks. No other text in the response\n",
      "\n",
      "Text:\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<edgarSubmission xsi:schemaLocation=\"http://www.sec.gov/edgar/thirteenffiler eis_13F_Filer.xsd\" xmlns=\"http://www.sec.gov/edgar/thirteenffiler\" xmlns:ns1=\"http://www.sec.gov/edgar/common\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n",
      "  <schemaVersion>X0202</schemaVersion>\n",
      "  <headerData>\n",
      "    <submissionType>13F-HR</submissionType>\n",
      "    <filerInfo>\n",
      "      <liveTestFlag>LIVE</liveTestFlag>\n",
      "      <flags>\n",
      "        <confirmingCopyFlag>false</confirmingCopyFlag>\n",
      "        <returnCopyFlag>false</returnCopyFlag>\n",
      "        <overrideInternetFlag>false</overrideInternetFlag>\n",
      "      </flags>\n",
      "      <filer>\n",
      "        <credentials>\n",
      "          <cik>0001027451</cik>\n",
      "          <ccc>XXXXXXXX</ccc>\n",
      "        </credentials>\n",
      "      </filer>\n",
      "      <periodOfReport>03-31-2023</periodOfReport>\n",
      "    </filerInfo>\n",
      "  </headerData>\n",
      "  <formData>\n",
      "    <coverPage>\n",
      "      <reportCalendarOrQuarter>03-31-2023</reportCalendarOrQuarter>\n",
      "      <isAmendment>false</isAmendment>\n",
      "      <filingManager>\n",
      "        <name>TIGER MANAGEMENT L.L.C.</name>\n",
      "        <address>\n",
      "          <ns1:street1>101 PARK AVENUE</ns1:street1>\n",
      "          <ns1:city>NEW YORK</ns1:city>\n",
      "          <ns1:stateOrCountry>NY</ns1:stateOrCountry>\n",
      "          <ns1:zipCode>10178</ns1:zipCode>\n",
      "        </address>\n",
      "      </filingManager>\n",
      "      <reportType>13F HOLDINGS REPORT</reportType>\n",
      "      <form13FFileNumber>028-05892</form13FFileNumber>\n",
      "      <provideInfoForInstruction5>N</provideInfoForInstruction5>\n",
      "    </coverPage>\n",
      "    <signatureBlock>\n",
      "      <name>Elouise Manhertz</name>\n",
      "      <title>Chief Financial Officer</title>\n",
      "      <phone>212-984-8869</phone>\n",
      "      <signature>/s/ Elouise Manhertz</signature>\n",
      "      <city>New York</city>\n",
      "      <stateOrCountry>NY</stateOrCountry>\n",
      "      <signatureDate>05-15-2023</signatureDate>\n",
      "    </signatureBlock>\n",
      "    <summaryPage>\n",
      "      <otherIncludedManagersCount>0</otherIncludedManagersCount>\n",
      "      <tableEntryTotal>33</tableEntryTotal>\n",
      "      <tableValueTotal>187894142</tableValueTotal>\n",
      "    </summaryPage>\n",
      "  </formData>\n",
      "</edgarSubmission>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from string import Template\n",
    "\n",
    "prompt = Template(mgr_info_tpl).substitute(ctext=manager_info)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the LLM to parse out the data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'TIGER MANAGEMENT L.L.C.',\n",
       " 'street1': '101 PARK AVENUE',\n",
       " 'street2': '',\n",
       " 'city': 'NEW YORK',\n",
       " 'stateOrCounty': 'NY',\n",
       " 'zipCode': '10178',\n",
       " 'reportCalendarOrQuarter': '03-31-2023'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "manager_data = json.loads(extract_entities_relationships(prompt).split('```')[1].strip('json'))\n",
    "manager_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Filing Information\n",
    "We will parse filing info in a similar manner to manager information. Because the filings include a list of many entries however, we will want to split the input into chunks so as not to exceed input or output token limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filing_info_chunks = split_filing_info(filing_info)\n",
    "len(filing_info_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ```\n",
      "[\n",
      "  {\n",
      "    \"cusip\": \"02079K305\",\n",
      "    \"name\": \"ALPHABET INC\",\n",
      "    \"value\": 1488692,\n",
      "    \"shares\": 14352,\n",
      "    \"sshPrnamtType\": \"SH\",\n",
      "    \"investmentDiscretion\": \"SOLE\",\n",
      "    \"votingSole\": 14352,\n",
      "    \"votingShared\": 0,\n",
      "    \"votingNone\": 0\n",
      "  },\n",
      "  {\n",
      "    \"cusip\": \"023135106\",\n",
      "    \"name\": \"AMAZON COM INC\",\n",
      "    \"value\": 582556,\n",
      "    \"shares\": 5640,\n",
      "    \"sshPrnamtType\": \"SH\",\n",
      "    \"investmentDiscretion\": \"SOLE\",\n",
      "    \"votingSole\": 5640,\n",
      "    \"votingShared\": 0,\n",
      "    \"votingNone\": 0\n",
      "  },\n",
      "  {\n",
      "    \"cusip\": \"049468101\",\n",
      "    \"name\": \"ATLASSIAN CORPORATION\",\n",
      "    \"value\": 205404,\n",
      "    \"shares\": 1200,\n",
      "    \"sshPrnamtType\": \"SH\",\n",
      "    \"investmentDiscretion\": \"SOLE\",\n",
      "    \"votingSole\": 1200,\n",
      "    \"votingShared\": 0,\n",
      "    \"votingNone\": 0\n",
      "  },\n",
      "  {\n",
      "    \"cusip\": \"053332102\",\n",
      "    \"name\": \"AUTOZONE INC\",\n",
      "    \"value\": 9218063,\n",
      "    \"shares\": 3750,\n",
      "    \"sshPrnamtType\": \"SH\",\n",
      "    \"investmentDiscretion\": \"SOLE\",\n",
      "    \"votingSole\": 3750,\n",
      "    \"votingShared\": 0,\n",
      "    \"votingNone\": 0\n",
      "  },\n",
      "  {\n",
      "    \"cusip\": \"19260Q107\",\n",
      "    \"name\": \"COINBASE GLOBAL INC\",\n",
      "    \"value\": 1086458,\n",
      "    \"shares\": 16079,\n",
      "    \"sshPrnamtType\": \"SH\",\n",
      "    \"investmentDiscretion\": \"SOLE\",\n",
      "    \"votingSole\": 16079,\n",
      "    \"votingShared\": 0,\n",
      "    \"votingNone\": 0\n",
      "  },\n",
      "  {\n",
      "    \"cusip\": \"247361702\",\n",
      "    \"name\": \"DELTA AIR LINES INC DEL\",\n",
      "    \"value\": 7008444,\n",
      "    \"shares\": 200700,\n",
      "    \"sshPrnamtType\": \"SH\",\n",
      "    \"investmentDiscretion\": \"SOLE\",\n",
      "    \"votingSole\": 200700,\n",
      "    \"votingShared\": 0,\n",
      "    \"votingNone\": 0\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = Template(filing_info_tpl).substitute(ctext=filing_info_chunks[0])\n",
    "response = extract_entities_relationships(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion - Test Example\n",
    "\n",
    "Let's walk through the steps to do this with just the 1 form above first, then we can move on to parsing and ingesting all the Form 13s.\n",
    "\n",
    "To start, we can run the LLM parsing over all the filing info from the form then combine the resulting json into a list condusive for Neo4j loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cusip': '02079K305',\n",
       "  'name': 'ALPHABET INC',\n",
       "  'value': 1488692,\n",
       "  'shares': 14352,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 14352,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 0,\n",
       "  'managerName': 'TIGER MANAGEMENT L.L.C.',\n",
       "  'reportCalendarOrQuarter': '03-31-2023'},\n",
       " {'cusip': '023135106',\n",
       "  'name': 'AMAZON COM INC',\n",
       "  'value': 582556,\n",
       "  'shares': 5640,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 5640,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 0,\n",
       "  'managerName': 'TIGER MANAGEMENT L.L.C.',\n",
       "  'reportCalendarOrQuarter': '03-31-2023'},\n",
       " {'cusip': '049468101',\n",
       "  'name': 'ATLASSIAN CORPORATION',\n",
       "  'value': 205404,\n",
       "  'shares': 1200,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 1200,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 0,\n",
       "  'managerName': 'TIGER MANAGEMENT L.L.C.',\n",
       "  'reportCalendarOrQuarter': '03-31-2023'},\n",
       " {'cusip': '053332102',\n",
       "  'name': 'AUTOZONE INC',\n",
       "  'value': 9218063,\n",
       "  'shares': 3750,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 3750,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 0,\n",
       "  'managerName': 'TIGER MANAGEMENT L.L.C.',\n",
       "  'reportCalendarOrQuarter': '03-31-2023'},\n",
       " {'cusip': '19260Q107',\n",
       "  'name': 'COINBASE GLOBAL INC',\n",
       "  'value': 1086458,\n",
       "  'shares': 16079,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 16079,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 0,\n",
       "  'managerName': 'TIGER MANAGEMENT L.L.C.',\n",
       "  'reportCalendarOrQuarter': '03-31-2023'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filings_list = []\n",
    "for filing_info_chunk in filing_info_chunks:\n",
    "    prompt = Template(filing_info_tpl).substitute(ctext=filing_info_chunk)\n",
    "    response = extract_entities_relationships(prompt)\n",
    "    filings_list.extend(json.loads(response.replace('```', '')))\n",
    "\n",
    "for item in filings_list:\n",
    "    item['managerName'] = manager_data['name']\n",
    "    item['reportCalendarOrQuarter'] = manager_data['reportCalendarOrQuarter']\n",
    "filings_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filings_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Neo4j Connection\n",
    "Now, we're going to load data into Neo4j.  To do so, you will, of course, need a Neo4j instance.  The easiest way to get started with Neo4j on Google Cloud is with Aura, the Neo4j managed service.  That comes in a few flavors, various combinations of Professional and Enterprise, conflated with DB and DS. Those stand for database and data science.  The data science version includes the database as well.\n",
    "\n",
    "For our purposes, we want a Neo4j AuraDS Professional.  You can deploy that from the Marketplace [here](https://console.cloud.google.com/marketplace/product/endpoints/prod.n4gcp.neo4j.io).\n",
    "\n",
    "When you deploy select \"1,000,000\" nodes and \"2,000,000\" relationships.  Select \"Node Embedding\" for algorithms.  That should give you a good instance for this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to change these to match your credentials\n",
    "NEO4J_URI = 'neo4j+s://xxxxx.databases.neo4j.io'\n",
    "NEO4J_PASSWORD = 'your password'\n",
    "\n",
    "# You can leave this as is.\n",
    "NEO4J_USERNAME = 'neo4j'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a connection to the database using the Graph Data Science API.  Note this will not work with an AuraDB instance since it does not include Graph Data Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "gds = GraphDataScience(\n",
    "    NEO4J_URI,\n",
    "    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),\n",
    "    aura_ds=True\n",
    ")\n",
    "gds.set_database('neo4j')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading we should create uniqueness constraints for nodes.  This acts as a unique id and an index and is necessary for fast, efficient, queries.  In general, if you notice ingestion is super slow (and getting slower) with Neo4j, double check that you created indexes.  For this small sample it won't matter, but it will certainly imact as we ingest more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher('CREATE CONSTRAINT unique_manager IF NOT EXISTS FOR (n:Manager) REQUIRE (n.name) IS UNIQUE')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_company_id IF NOT EXISTS FOR (n:Company) REQUIRE (n.cusip) IS UNIQUE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Merge in the data we can use parameterized cypher queries.  Basically we are going to send filings in batches (in this sample case just one batch) for each node and relationship type and insert them as parameters in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_node_merge_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company_node_merge_count\n",
       "0                        33"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Company Nodes\n",
    "\n",
    "gds.run_cypher('''\n",
    "UNWIND $records AS record\n",
    "MERGE (c:Company {cusip: record.cusip})\n",
    "SET c.name = record.name\n",
    "RETURN count(c) AS company_node_merge_count\n",
    "''', params={'records':filings_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Manager Node. In this case we just have one\n",
    "\n",
    "gds.run_cypher('''\n",
    "MERGE (m:Manager {name: $name})\n",
    "''', params={'name':manager_data['name']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owns_relationship_merge_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   owns_relationship_merge_count\n",
       "0                             33"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create OWNS Relationship\n",
    "\n",
    "gds.run_cypher('''\n",
    "UNWIND $records AS record\n",
    "MATCH (m:Manager {name: record.managerName})\n",
    "MATCH (c:Company {cusip: record.cusip})\n",
    "MERGE(m)-[r:OWNS]->(c)\n",
    "SET r.reportCalendarOrQuarter = record.reportCalendarOrQuarter,\n",
    "    r.value = record.value,\n",
    "    r.shares = record.shares\n",
    "RETURN count(r) AS owns_relationship_merge_count\n",
    "''', params={'records':filings_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now load Neo4j Browser through the Aura GUI to check the graph and see the loaded sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Multiple Form 13 Files\n",
    "We will make a pipeline using the methods above.  In this case we will take a two-step approach, first parse all the data, then chunk that data and ingest into Neo4j.\n",
    "\n",
    "To start, lets take a look at all the Form 13 samples we have in cloud storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44142 total raw form13 files\n"
     ]
    }
   ],
   "source": [
    "blobs = storage_client.list_blobs('neo4j-datasets', prefix='form13/raw/', delimiter='/')\n",
    "file_names = [blob.name for blob in blobs if '.txt' in blob.name]\n",
    "print(f'{len(file_names)} total raw form13 files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, let's just use the first 5 in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['form13/raw/raw_2022-01-03_archives_edgar_data_1026200_0001567619-22-000057.txt',\n",
       " 'form13/raw/raw_2022-01-03_archives_edgar_data_1315339_0001315339-22-000001.txt',\n",
       " 'form13/raw/raw_2022-01-03_archives_edgar_data_1384943_0001384943-22-000001.txt',\n",
       " 'form13/raw/raw_2022-01-03_archives_edgar_data_1452208_0001104659-22-000174.txt',\n",
       " 'form13/raw/raw_2022-01-03_archives_edgar_data_1624809_0001624809-22-000001.txt']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_file_names = file_names[:5]\n",
    "sample_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for getting filing info\n",
    "\n",
    "def get_manager_and_filing_info(raw_txt):\n",
    "    contents = raw_txt.split('<XML>')\n",
    "    manager_info = contents[1].split('</XML>')[0].strip()\n",
    "    filing_info = contents[2].split('</XML>')[0].strip()\n",
    "    \n",
    "    return manager_info, filing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Parsing 5 Form 13 Files ===\n",
      "--- parsing form13/raw/raw_2022-01-03_archives_edgar_data_1026200_0001567619-22-000057.txt ---\n",
      "getting file text from gcloud....\n",
      "getting file contents...\n",
      "Parsing submission and manager info...\n",
      "Parsing filing info...\n",
      "--- parsing form13/raw/raw_2022-01-03_archives_edgar_data_1315339_0001315339-22-000001.txt ---\n",
      "getting file text from gcloud....\n",
      "getting file contents...\n",
      "Parsing submission and manager info...\n",
      "Parsing filing info...\n",
      "--- parsing form13/raw/raw_2022-01-03_archives_edgar_data_1384943_0001384943-22-000001.txt ---\n",
      "getting file text from gcloud....\n",
      "getting file contents...\n",
      "Parsing submission and manager info...\n",
      "Parsing filing info...\n",
      "--- parsing form13/raw/raw_2022-01-03_archives_edgar_data_1452208_0001104659-22-000174.txt ---\n",
      "getting file text from gcloud....\n",
      "getting file contents...\n",
      "Parsing submission and manager info...\n",
      "Parsing filing info...\n",
      "--- parsing form13/raw/raw_2022-01-03_archives_edgar_data_1624809_0001624809-22-000001.txt ---\n",
      "getting file text from gcloud....\n",
      "getting file contents...\n",
      "Parsing submission and manager info...\n",
      "Parsing filing info...\n"
     ]
    }
   ],
   "source": [
    "print(f'=== Parsing {len(sample_file_names)} Form 13 Files ===')\n",
    "\n",
    "filings_list = []\n",
    "manager_list = []\n",
    "\n",
    "for file_name in sample_file_names:\n",
    "    \n",
    "    print(f'--- parsing {file_name} ---')\n",
    "    try:\n",
    "        #Get raw form13 file\n",
    "        print('getting file text from gcloud....')\n",
    "        blob = bucket.blob(file_name)\n",
    "        raw_text = blob.download_as_string().decode()\n",
    "\n",
    "        #Get raw manager and filing info from file\n",
    "        print('getting file contents...')\n",
    "        manager_info, filing_info = get_manager_and_filing_info(raw_text)\n",
    "\n",
    "        #Parse manager info into dict using LLM\n",
    "        print('Parsing submission and manager info...')\n",
    "        mng_prompt = Template(mgr_info_tpl).substitute(ctext=manager_info)\n",
    "        mng_response = extract_entities_relationships(mng_prompt)\n",
    "        manager_data = json.loads(mng_response.replace('```', ''))\n",
    "        manager_list.append({'name': manager_data['name']})\n",
    "\n",
    "        #Parse filing info into list of dicts using LLM\n",
    "        print('Parsing filing info...')\n",
    "        for filing_info_chunk in filing_info_chunks:\n",
    "            filing_prompt = Template(filing_info_tpl).substitute(ctext=filing_info_chunk)\n",
    "            filing_response = extract_entities_relationships(filing_prompt)\n",
    "            filings_list.extend(json.loads(filing_response.replace('```', '')))\n",
    "        for item in filings_list: #Add information from manager_info to enable OWNS relationship loading\n",
    "            item['managerName'] = manager_data['name']\n",
    "            item['reportCalendarOrQuarter'] = manager_data['reportCalendarOrQuarter']\n",
    "    except Exception as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manager_node_merge_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   manager_node_merge_count\n",
       "0                         5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge Manager Nodes.\n",
    "\n",
    "gds.run_cypher('''\n",
    "UNWIND $records AS record\n",
    "MERGE (m:Manager {name: record.name})\n",
    "RETURN count(m) AS manager_node_merge_count\n",
    "''', params={'records':manager_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the dataset gets bigger we will want to chunk up the filings we send to Neo4j\n",
    "\n",
    "def chunks(xs, n=10_000):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   company_node_merge_count\n",
      "0                       165\n"
     ]
    }
   ],
   "source": [
    "# Creat Company Nodes\n",
    "\n",
    "for d in chunks(filings_list):\n",
    "    res = gds.run_cypher('''\n",
    "    UNWIND $records AS record\n",
    "    MERGE (c:Company {cusip: record.cusip})\n",
    "    SET c.name = record.name\n",
    "    RETURN count(c) AS company_node_merge_count\n",
    "    ''', params={'records':d})\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   owns_relationship_merge_count\n",
      "0                            165\n"
     ]
    }
   ],
   "source": [
    "# Create the OWNS Relationships\n",
    "\n",
    "for d in chunks(filings_list):\n",
    "    res = gds.run_cypher('''\n",
    "    UNWIND $records AS record\n",
    "    MATCH (m:Manager {name: record.managerName})\n",
    "    MATCH (c:Company {cusip: record.cusip})\n",
    "    MERGE(m)-[r:OWNS]->(c)\n",
    "    SET r.reportCalendarOrQuarter = record.reportCalendarOrQuarter,\n",
    "        r.value = record.value,\n",
    "        r.shares = record.shares\n",
    "    RETURN count(r) AS owns_relationship_merge_count\n",
    "    ''', params={'records':d})\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!  You've used an LLM to parse and load data into Neo4j, creating a knowledge graph.  At this point, you may want to open Neo4j Browser or Bloom through the Aura console and explore the data you've loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot\n",
    "We're now going to show how to layer a chat bot on top of the knowledge graph we created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load more data\n",
    "We showed how to load data with the LLM.  Now we're going to load some more rows from a CSV so our chatbot has more to work with.  This is going to take a while to run, ~15 minutes or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher('''CREATE CONSTRAINT unique_company_id IF NOT EXISTS FOR (p:Company) REQUIRE (p.cusip) IS NODE KEY;''')\n",
    "gds.run_cypher('''CREATE CONSTRAINT unique_manager IF NOT EXISTS FOR (p:Manager) REQUIRE (p.managerName) IS NODE KEY;''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher('''LOAD CSV WITH HEADERS FROM 'https://storage.googleapis.com/neo4j-datasets/form13/form13-v2.csv' AS row\n",
    "MERGE (c:Company {cusip:row.cusip})\n",
    "ON CREATE SET c.companyName=row.companyName;''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher('''LOAD CSV WITH HEADERS FROM 'https://storage.googleapis.com/neo4j-datasets/form13/form13-v2.csv' AS row\n",
    "MERGE (m:Manager {managerName:row.managerName});''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher('''LOAD CSV WITH HEADERS FROM 'https://storage.googleapis.com/neo4j-datasets/form13/form13-v2.csv' AS row\n",
    "MATCH (m:Manager {managerName:row.managerName})\n",
    "MATCH (c:Company {cusip:row.cusip})\n",
    "MERGE (m)-[r:OWNS {reportCalendarOrQuarter:date(row.reportCalendarOrQuarter)}]->(c)\n",
    "SET r.value = toFloat(row.value), r.shares = toInteger(row.shares);''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrypoint Agent\n",
    "We can put a procedural step in front of our chatbot to help us handle non-relevant responses, which will cause Cypher syntax errors to be thrown otherwise.  We will use the `text-bison` model for this step, and we will give it a simple prompt that instructs it to decide if a question coming in is relevant to the database.  If so, it can be passed through to the Cypher generation chain.  If not, the agent can simply issue and answer and we never have to try to generate Cypher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import VertexAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "entrypoint_prompt_template = \"\"\"\n",
    "\n",
    "You are an entrypoint for a finance/investment database.  If your prompt is not related to finance, investments, companies, \n",
    "or similar topics, try to answer the prompt if you can in chat style and remind the user that you are a chatbot for accessing a\n",
    "finance database.  If it is about those topics, return True as your entire answer.\n",
    "\n",
    "The next text is the user prompt:\n",
    "\n",
    "{prompt}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "agent_llm = VertexAI(model_name=\"text-bison\", temperature=0)\n",
    "entrypoint = LLMChain(\n",
    "    llm=agent_llm,\n",
    "    prompt=PromptTemplate.from_template(entrypoint_prompt_template)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll combine these elements into a function that will handle this initial query test.  This can be called later in the chatbot code before we hit the Cypher generation chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrypoint_agent(nl_query):\n",
    "    agent_response = entrypoint(nl_query)\n",
    "    return {\n",
    "        \"query\": nl_query,\n",
    "        \"response\": agent_response[\"text\"],\n",
    "        \"is_relevant\": agent_response[\"text\"] == \"True\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test a few examples of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "{'query': 'Hello, how are you doing?', 'response': 'Hello, I am doing well. I am a chatbot for accessing a finance database. How can I help you today?', 'is_relevant': False}\n",
      "\n",
      "2.\n",
      "{'query': 'What is the average temperature of the surface of the sun?', 'response': \"I am a chatbot for accessing a finance database. I can't help you with that.\", 'is_relevant': False}\n",
      "\n",
      "3.\n",
      "{'query': 'Can you tell me which investors had the best year in 2022?', 'response': 'True', 'is_relevant': True}\n"
     ]
    }
   ],
   "source": [
    "print(\"1.\")\n",
    "print(entrypoint_agent(\"Hello, how are you doing?\"))\n",
    "print(\"\\n2.\")\n",
    "print(entrypoint_agent(\"What is the average temperature of the surface of the sun?\"))\n",
    "print(\"\\n3.\")\n",
    "print(entrypoint_agent(\"Can you tell me which investors had the best year in 2022?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cypher Generation\n",
    "We have to use a prompt template that clearly states what schema to use, the principles that the chatbot should follow in generating responses, and some few-shot examples to help the chatbot be more accurate in its query generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt/template \n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"You are an expert Neo4j Cypher translator who understands the question in english and convert to Cypher strictly based on the Neo4j Schema provided and following the instructions below:\n",
    "1. Generate Cypher query compatible ONLY for Neo4j Version 5\n",
    "2. Do not use EXISTS, SIZE keywords in the cypher. Use alias when using the WITH keyword\n",
    "3. Please do not use same variable names for different nodes and relationships in the query.\n",
    "4. Use only Nodes and relationships mentioned in the schema\n",
    "5. Always enclose the Cypher output inside 3 backticks\n",
    "6. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Company name use `toLower(c.name) contains 'neo4j'`\n",
    "7. Candidate node is synonymous to Manager\n",
    "8. Always use aliases to refer the node in the query\n",
    "9. 'Answer' is NOT a Cypher keyword. Answer should never be used in a query.\n",
    "10. Please generate only one Cypher query per question. \n",
    "11. Cypher is NOT SQL. So, do not mix and match the syntaxes.\n",
    "12. Every Cypher query always starts with a MATCH keyword.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "Samples:\n",
    "Question: Which fund manager owns most shares? What is the total portfolio value?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) RETURN m.managerName as manager, sum(distinct o.shares) as ownedShares, sum(o.value) as portfolioValue ORDER BY ownedShares DESC LIMIT 10\n",
    "\n",
    "Question: Which fund manager owns most companies? How many shares?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) RETURN m.managerName as manager, count(distinct c) as ownedCompanies, sum(distinct o.shares) as ownedShares ORDER BY ownedCompanies DESC LIMIT 10\n",
    "\n",
    "Question: What are the top 10 investments for Vanguard?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) contains \"vanguard\" RETURN c.companyName as Investment, sum(DISTINCT o.shares) as totalShares, sum(DISTINCT o.value) as investmentValue order by investmentValue desc limit 10\n",
    "\n",
    "Question: What other fund managers are investing in same companies as Vanguard?\n",
    "Answer: MATCH (m1:Manager) -[:OWNS]-> (c1:Company) <-[o:OWNS]- (m2:Manager) WHERE toLower(m1.managerName) contains \"vanguard\" AND elementId(m1) <> elementId(m2) RETURN m2.managerName as manager, sum(DISTINCT o.shares) as investedShares, sum(DISTINCT o.value) as investmentValue ORDER BY investmentValue LIMIT 10\n",
    "\n",
    "Question: What are the top investors for Apple?\n",
    "Answer: MATCH (m1:Manager) -[o:OWNS]-> (c1:Company) WHERE toLower(c1.companyName) contains \"apple\" RETURN distinct m1.managerName as manager, sum(o.value) as totalInvested ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: What are the other top investments for fund managers investing in Apple?\n",
    "Answer: MATCH (c1:Company) <-[:OWNS]- (m1:Manager) -[o:OWNS]-> (c2:Company) WHERE toLower(c1.companyName) contains \"apple\" AND elementId(c1) <> elementId(c2) RETURN DISTINCT c2.companyName as company, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: What are the top investors in the last 3 months?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE date() > o.reportCalendarOrQuarter > o.reportCalendarOrQuarter - duration({{months:3}}) RETURN distinct m.managerName as manager, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: What are top investments in last 6 months for Vanguard?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) contains \"vanguard\" AND date() > o.reportCalendarOrQuarter > date() - duration({{months:6}}) RETURN distinct c.companyName as company, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: Who are Apple's top investors in last 3 months?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(c.companyName) contains \"apple\" AND date() > o.reportCalendarOrQuarter > date() - duration({{months:3}}) RETURN distinct m.managerName as investor, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: Which fund manager under 200 million has similar investment strategy as Vanguard?\n",
    "Answer: MATCH (m1:Manager) -[o1:OWNS]-> (:Company) <-[o2:OWNS]- (m2:Manager) WHERE toLower(m1.managerName) CONTAINS \"vanguard\" AND elementId(m1) <> elementId(m2) WITH distinct m2 AS m2, sum(distinct o2.value) AS totalVal WHERE totalVal < 200000000 RETURN m2.managerName AS manager, totalVal*0.000001 AS totalVal ORDER BY totalVal DESC LIMIT 10\n",
    "\n",
    "Question: Who are common investors in Apple and Amazon?\n",
    "Answer: MATCH (c1:Company) <-[:OWNS]- (m:Manager) -[:OWNS]-> (c2:Company) WHERE toLower(c1.companyName) contains \"apple\" AND toLower(c2.companyName) CONTAINS \"amazon\" RETURN DISTINCT m.managerName LIMIT 50\n",
    "\n",
    "Question: What are Vanguard's top investments by shares for 2023?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) CONTAINS \"vanguard\" AND date({{year:2023}}) = date.truncate('year',o.reportCalendarOrQuarter) RETURN c.companyName AS investment, sum(o.value) AS totalValue ORDER BY totalValue DESC LIMIT 10\n",
    "\n",
    "Question: What are Vanguard's top investments by value for 2023?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) CONTAINS \"vanguard\" AND date({{year:2023}}) = date.truncate('year',o.reportCalendarOrQuarter) RETURN c.companyName AS investment, sum(o.shares) AS totalShares ORDER BY totalShares DESC LIMIT 10\n",
    "\n",
    "Question: {question}\n",
    "Answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a LangChain prompt template.  This defines the inputs that will be included as parameters into the prompt sent to the Cypher generation bot.  In our example, the inputs will be `schema` and `question`.  The question comes from the end user.  The schema is automatically inserted by the LangChain `GraphCypherQAChain` via a built in method to `Neo4jGraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\",\"question\"], validate_template=True, template=CYPHER_GENERATION_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to connect to the graph via LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url = NEO4J_URI, \n",
    "    username = NEO4J_USERNAME, \n",
    "    password = NEO4J_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are defining our `chain` object, which combines Neo4j Q/A and VertexAI's `code-bison` LLM.  When the user gives a query, it first goes through `GraphCypherQAChain`, which generates a Cypher query according to the rules laid out in our prompt above.  That result set then goes to the `VertexAI` step of the chain, where the LLM is given the Neo4j result set and instructed to roll it into a natural language response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    VertexAI(model_name='code-bison',\n",
    "            max_output_tokens=2048,\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            top_k=0.40), graph=graph, verbose=True,\n",
    "            cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "    return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have a few examples of how we can get answers from the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) CONTAINS \"blackrock\" RETURN c.companyName AS Investment, sum(DISTINCT o.shares) AS totalShares, sum(DISTINCT o.value) AS investmentValue order by investmentValue desc limit 10\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'Investment': 'Apple Inc', 'totalShares': 3084362729, 'investmentValue': 304538995305000.0}, {'Investment': 'MICROSOFT CORP', 'totalShares': 1588389433, 'investmentValue': 282697628073000.0}, {'Investment': 'AMAZON COM INC', 'totalShares': 1784427354, 'investmentValue': 112783300990000.0}, {'Investment': 'NVIDIA CORPORATION', 'totalShares': 539750644, 'investmentValue': 77323879849000.0}, {'Investment': 'UNITEDHEALTH GROUP INC', 'totalShares': 220675563, 'investmentValue': 74901917261000.0}, {'Investment': 'Johnson & Johnson', 'totalShares': 600132459, 'investmentValue': 66382342451000.0}, {'Investment': 'EXXON MOBIL CORP', 'totalShares': 849919665, 'investmentValue': 62624867042000.0}, {'Investment': 'TESLA INC', 'totalShares': 528248761, 'investmentValue': 59003867451000.0}, {'Investment': 'JPMORGAN CHASE & CO', 'totalShares': 586443138, 'investmentValue': 51420867302000.0}, {'Investment': 'PROCTER AND GAMBLE CO', 'totalShares': 483314816, 'investmentValue': 48286890186000.0}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Final answer: The top 10 investments for Blackrock are:\n",
      "1. Apple Inc\n",
      "2. Microsoft Corp\n",
      "3. Amazon.com Inc\n",
      "4. NVIDIA Corporation\n",
      "5. UnitedHealth Group Inc\n",
      "6. Johnson & Johnson\n",
      "7. Exxon Mobil Corp\n",
      "8. Tesla Inc\n",
      "9. JPMorgan Chase & Co\n",
      "10. Procter & Gamble Co\n"
     ]
    }
   ],
   "source": [
    "r2 = chain(\"\"\"What are the top 10 investments for Blackrock?\"\"\")\n",
    "print(f\"Final answer: {r2['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (c1:Company) <-[:OWNS]- (m1:Manager) -[o:OWNS]-> (c2:Company) WHERE toLower(c1.companyName) CONTAINS \"astrazeneca\" AND elementId(c1) <> elementId(c2) RETURN DISTINCT c2.companyName as company, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'company': 'SPDR S&P 500 ETF TR', 'totalInvested': 327922309895000.0, 'totalShares': 1229482000}, {'company': 'MICROSOFT CORP', 'totalInvested': 243900032685000.0, 'totalShares': 1608747980}, {'company': 'Apple Inc', 'totalInvested': 241033252745000.0, 'totalShares': 2916923878}, {'company': 'AMAZON COM INC', 'totalInvested': 123072858935000.0, 'totalShares': 2359233778}, {'company': 'ISHARES TR', 'totalInvested': 104442565293000.0, 'totalShares': 1463346482}, {'company': 'TESLA INC CALL', 'totalInvested': 93992366480000.0, 'totalShares': 864643000}, {'company': 'INVESCO QQQ TR PUT', 'totalInvested': 78031957994000.0, 'totalShares': 373626750}, {'company': 'NVIDIA CORPORATION', 'totalInvested': 76878718479000.0, 'totalShares': 600079703}, {'company': 'TESLA INC', 'totalInvested': 66103011765000.0, 'totalShares': 637242353}, {'company': 'Alphabet Inc. Class A', 'totalInvested': 59250976322000.0, 'totalShares': 1295667686}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Final answer: The other top investments for fund managers investing in AstraZeneca are SPDR S&P 500 ETF TR, Microsoft Corp, Apple Inc, Amazon.com Inc, iShares Tr, Tesla Inc Call, Invesco QQQ Tr Put, NVIDIA CORPORATION, Tesla Inc, and Alphabet Inc. Class A.\n"
     ]
    }
   ],
   "source": [
    "r3 = chain(\"\"\"What are other top investments for fund managers investing in AstraZeneca?\"\"\")\n",
    "print(f\"Final answer: {r3['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m1:Manager) -[o1:OWNS]-> (:Company) <-[o2:OWNS]- (m2:Manager) WHERE toLower(m1.managerName) CONTAINS \"blackrock\" AND elementId(m1) <> elementId(m2) WITH distinct m2 AS m2, sum(distinct o2.value) AS totalVal WHERE totalVal < 200000000 RETURN m2.managerName AS manager, totalVal*0.000001 AS totalVal ORDER BY totalVal DESC LIMIT 10\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'manager': 'LAKE STREET ADVISORS GROUP, LLC', 'totalVal': 197.487}, {'manager': 'INCA Investments LLC', 'totalVal': 197.18599999999998}, {'manager': 'M Holdings Securities, Inc.', 'totalVal': 194.481}, {'manager': 'Avalon Global Asset Management LLC', 'totalVal': 194.107}, {'manager': 'King Wealth', 'totalVal': 192.66299999999998}, {'manager': 'TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA', 'totalVal': 190.523}, {'manager': 'Crestview Partners IV GP, L.P.', 'totalVal': 190.332}, {'manager': 'Virtu Financial LLC', 'totalVal': 188.94899999999998}, {'manager': 'Morningstar Investment Management LLC', 'totalVal': 187.658}, {'manager': 'Torray LLC', 'totalVal': 185.56699999999998}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Final answer: I don't know the answer to that question.\n"
     ]
    }
   ],
   "source": [
    "r4 = chain(\"\"\"Which fund manager under 200 million has similar investment strategy as Blackrock\"\"\")\n",
    "print(f\"Final answer: {r4['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (c1:Company) <-[:OWNS]- (m:Manager) -[:OWNS]-> (c2:Company) WHERE toLower(c1.companyName) CONTAINS \"tesla\" AND toLower(c2.companyName) CONTAINS \"microsoft\" RETURN DISTINCT m.managerName LIMIT 10\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'m.managerName': 'Bristlecone Advisors, LLC'}, {'m.managerName': 'BI Asset Management Fondsmaeglerselskab A/S'}, {'m.managerName': 'Cornell Pochily Investment Advisors, Inc.'}, {'m.managerName': 'Smith Anglin Financial, LLC'}, {'m.managerName': 'Costello Asset Management, INC'}, {'m.managerName': 'Decatur Capital Management, Inc.'}, {'m.managerName': 'FIRST FOUNDATION ADVISORS'}, {'m.managerName': 'BANK OF NOVA SCOTIA'}, {'m.managerName': 'Sturgeon Ventures LLP'}, {'m.managerName': 'WAFRA INC.'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Final answer: I'm sorry, I don't have access to that information.\n"
     ]
    }
   ],
   "source": [
    "r5 = chain(\"\"\"Please get me 10 common investors between Tesla and Microsoft\"\"\")\n",
    "print(f\"Final answer: {r5['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(c.companyName) IN [\"facebook\", \"apple\", \"amazon\", \"netflix\", \"google\"] RETURN m.managerName AS manager\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'manager': 'Beacon Wealthcare LLC'}, {'manager': 'Pinnacle Holdings, LLC'}, {'manager': 'Pinnacle Holdings, LLC'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Final answer: Beacon Wealthcare LLC and Pinnacle Holdings, LLC own FAANG stocks.\n"
     ]
    }
   ],
   "source": [
    "r6 = chain(\"\"\"Which managers own FAANG stocks?\"\"\")\n",
    "print(f\"Final answer: {r6['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "Now we are going to use Gradio to deploy a chat interface that will have our chain behind it.\n",
    "\n",
    "When we run the code below, a Gradio application will be deployed and can be accessed at a local URL.  We also get a public URL that can be shared for 3 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://96b239f8ba748381b4.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://96b239f8ba748381b4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (c1:Company) <-[:OWNS]- (m1:Manager) -[o:OWNS]-> (c2:Company) WHERE toLower(c1.companyName) contains \"tesla\" AND date() > o.reportCalendarOrQuarter > o.reportCalendarOrQuarter - duration({months:3}) RETURN DISTINCT m1.managerName as investor, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'investor': 'VANGUARD GROUP INC', 'totalInvested': 1.8341382085352e+16, 'totalShares': 351100044799}, {'investor': 'BlackRock Inc.', 'totalInvested': 1.5378500757708e+16, 'totalShares': 299559253664}, {'investor': 'STATE STREET CORP', 'totalInvested': 1.2696875459397e+16, 'totalShares': 203468560836}, {'investor': 'NORTHERN TRUST CORP', 'totalInvested': 5807043816540000.0, 'totalShares': 108219189345}, {'investor': 'FMR LLC', 'totalInvested': 4454628669549000.0, 'totalShares': 81905219404}, {'investor': 'JANE STREET GROUP, LLC', 'totalInvested': 3409349733651000.0, 'totalShares': 55456033736}, {'investor': 'Sumitomo Mitsui Trust Holdings, Inc.', 'totalInvested': 3106405615804000.0, 'totalShares': 54472672435}, {'investor': 'MORGAN STANLEY', 'totalInvested': 3097104871303000.0, 'totalShares': 53361795184}, {'investor': 'BANK OF AMERICA CORP /DE/', 'totalInvested': 2405126259532000.0, 'totalShares': 38317522637}, {'investor': 'WELLINGTON MANAGEMENT GROUP LLP', 'totalInvested': 2323036388482000.0, 'totalShares': 38420827894}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import typing_extensions\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages = True)\n",
    "llm = VertexAI(model_name='code-bison',\n",
    "            max_output_tokens=2048,\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            top_k=0.40)\n",
    "agent_chain = chain\n",
    "\n",
    "def chat_response(input_text,history):\n",
    "    try:\n",
    "        entry_response = entrypoint_agent(input_text)\n",
    "        if entry_response['is_relevant']:\n",
    "            return agent_chain.run(input_text)\n",
    "        else:\n",
    "            return entry_response[\"response\"]\n",
    "    except:\n",
    "        # a bit of protection against exposed error messages\n",
    "        # we could log these situations in the backend to revisit later in development\n",
    "        return \"I'm sorry, there was an error retrieving the information you requested.\"\n",
    "\n",
    "interface = gr.ChatInterface(fn = chat_response,\n",
    "                             title = \"Investment Chatbot\",\n",
    "                             description = \"powered by Neo4j\",\n",
    "                             theme = \"soft\",\n",
    "                             chatbot = gr.Chatbot(height=500),\n",
    "                             undo_btn = None,\n",
    "                             clear_btn = \"\\U0001F5D1 Clear chat\",\n",
    "                             examples = [\"Who are Tesla's top investors in last 3 months?\",\n",
    "                                         \"What are the top 10 investments for Blackrock?\",\n",
    "                                         \"Which manager owns FAANG stocks?\",\n",
    "                                         \"What are other top investments for fund managers investing in Exxon?\",\n",
    "                                         \"What are Vanguard's top investments by value for 2023?\",\n",
    "                                         \"Who are the common investors between Tesla and Microsoft?\"])\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we went through the steps of connecting a LangChain agent to a Neo4j database and using it to generate Cypher queries in response to user requests via LLMs on VertexAI.\n",
    "\n",
    "We used the `code-bison` model, but this approach can be generalized to any of the VertexAI LLMs and it can also be augmented with additional procedural steps around the generation chain to customize the user experience further for specific use cases.  The critical takeaway is the importance of Neo4j as a grounding database to anchor your chatbot to reality as it generates responses and to enable it to provide responses enriched with relevant enterprise data.  Knowledge graph is the best type of data structure to use for this type of grounding.  We also added an entrypoint agent to help provide a more acceptable user experience in cases where the input queries from users don't match the expected subject matter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38 (Local)",
   "language": "python",
   "name": "local-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
